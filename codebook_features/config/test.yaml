model_args:
  model_name_or_path: taufeeque/tiny-gpt2
data_args:
  dataset_name: wikitext
  dataset_config_name: wikitext-2-raw-v1
  max_train_samples: 4
  max_eval_samples: 4
  streaming: False
training_args:
  run_name: test
  output_dir: output
  overwrite_output_dir: True
  do_train: True
  do_eval: True
  max_steps: 4
  save_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  eval_steps: 1
  logging_steps: 1
  # report_to: wandb

pretrained_path: null
get_baseline: False
codebook_size: 100
layers_to_snap: all
similarity_metric: euclidean
codebook_at: attention_and_mlp
num_compositional_codebooks: 1
train_model_params: True
model_lr_factor: 0.1
vqvae_loss: False
k_codebook: 3
tag_keys: ["codebook_at", "per_device_train_batch_size", "model_name_or_path"]
tags: ["test", "TGPT"]
project: gpt2-codebook
