model_args:
  model_name_or_path: EleutherAI/pythia-70m-deduped
  cache_dir: /data/.cache/huggingface/
data_args:
  dataset_name: wikitext
  dataset_config_name: wikitext-103-v1
  streaming: False
training_args:
  run_name: train_gpt2_on_wiki
  output_dir: output_main
  overwrite_output_dir: True
  do_train: True
  do_eval: True
  per_device_train_batch_size: 24
  per_device_eval_batch_size: 8
  warmup_ratio: 0.0
  learning_rate: 5e-4
  max_steps: 15000
  logging_first_step: True
  logging_steps: 100
  evaluation_strategy: steps
  eval_steps: 100
  save_steps: 10000
  ddp_find_unused_parameters: False

  train_model_params: True
  model_lr_factor: 0.1
  loss: aeloss
  codebook_reg_p: null
  codebook_weight_decay: 0.01

pretrained_path: null
get_baseline: False
codebook_type: vanilla
codebook_size: 10000
layers_to_snap: all
similarity_metric: inner_product
codebook_at: attention
num_codebooks: 1
k_codebook: 100
kmeans_init: False
kmeans_path: path: /data/.cache/huggingface/kmeans_embeddings.pt
kmeans_kwargs:
  n_init: auto
  batch_size: 24576
tag_keys: []
tags: []
project: gpt2-codebook
