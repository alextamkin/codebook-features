model_args:
  model_name_or_path: gpt2
data_args:
  dataset_name: wikitext
  dataset_config_name: wikitext-103-v1
  streaming: False
training_args:
  run_name: train_gpt2_on_wiki
  output_dir: output_main
  overwrite_output_dir: True
  do_train: True
  do_eval: True
  logging_first_step: True
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  learning_rate: 5e-5
  max_steps: 20000
  evaluation_strategy: "steps"
  eval_steps: 200
  save_steps: 1000

get_baseline: False
codebook_size: 10000
layers_to_snap: [0, 5]
similarity_metric: euclidean
tag_keys: []
project: gpt2-codebook
