{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Code Intervention Tutorial\n",
                "\n",
                "This is a tutorial notebook on how to perform code interventions with the Codebook Features library. The goal of this tutorial is to steer a language model to generate text that follows a topic (by activating specific topic codes) and quantitatively evaluate how well the model was steered. We use the TinyStories 21M parameter model trained on synthetic children's stories (https://arxiv.org/abs/2305.07759). This small model typically produces grammatical but incoherent stories; nevertheless we can use it to see how different topics are woven into the network. For example, activating 'baby' codes causes the model to introduce topics such as babies, bathtubs, and baby birds, rather than simply outputting 'baby baby baby' repeatedly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa5bc733280>"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import transformers\n",
                "from tqdm import tqdm\n",
                "from codebook_features import models, run_clm, trainer as cb_trainer\n",
                "from codebook_features import utils as cb_utils\n",
                "import torch\n",
                "import re\n",
                "\n",
                "# We turn automatic differentiation off, to save GPU memory,\n",
                "# as this tutorial focuses only on model inference\n",
                "torch.set_grad_enabled(False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name_or_path = \"roneneldan/TinyStories-1Layer-21M\"\n",
                "pretrained_path = \"taufeeque/TinyStories-1Layer-21M-Codebook\"\n",
                "\n",
                "device = \"cuda\"\n",
                "orig_cb_model = models.wrap_codebook(\n",
                "    model_or_path=model_name_or_path, pretrained_path=pretrained_path\n",
                ")\n",
                "orig_cb_model = orig_cb_model.to(device).eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Convert the model into a hooked transformer model (from transformer_lens) that allows us to do code interventions easily"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using pad_token, but it is not set yet.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model roneneldan/TinyStories-1Layer-21M into HookedTransformer\n"
                    ]
                }
            ],
            "source": [
                "hooked_kwargs = dict(\n",
                "    center_unembed=False,\n",
                "    fold_value_biases=False,\n",
                "    center_writing_weights=False,\n",
                "    fold_ln=False,\n",
                "    refactor_factored_attn_matrices=False,\n",
                "    device=device,\n",
                ")\n",
                "cb_model = models.convert_to_hooked_model(\n",
                "    model_name_or_path, orig_cb_model, hooked_kwargs=hooked_kwargs\n",
                ")\n",
                "cb_model = cb_model.to(device).eval()\n",
                "tokenizer = cb_model.tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Topic Codes\n",
                "Below, we have provided a subset of topic codes we have found in this model. Many more such topic codes can be found through the Codebook Features webapp: https://huggingface.co/spaces/taufeeque/codebook-features\n",
                "\n",
                "Note that multiple codes can be patched in at the same component codebook (in this case, a given attention head at a given layer) since the codebook activates multiple codes. Since there are multiple codes that can represent a topic, we patch in multiple codes for each topic, possibly from different attention heads. You can play around by removing some codes for a topic and seeing how the generated text changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "topic_codes_str = {\n",
                "    \"\": \"\"\n",
                "}  # blank one is used for default generations (no topic steering)\n",
                "\n",
                "topic = \"dragon\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 4670, Layer: 0, Head: 13\n",
                "Code: 17640, Layer: 0, Head: 13\n",
                "Code: 19845, Layer: 0, Head: 13\n",
                "Code: 23958, Layer: 0, Head: 13\n",
                "Code: 3410, Layer: 0, Head: 13\n",
                "Code: 19523, Layer: 0, Head: 13\n",
                "Code: 2262, Layer: 0, Head: 13\n",
                "Code: 16060, Layer: 0, Head: 13\n",
                "\"\"\"\n",
                "\n",
                "topic = \"slide\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 1331, Layer: 0, Head: 14\n",
                "Code: 22178, Layer: 0, Head: 14\n",
                "Code: 15885, Layer: 0, Head: 14\n",
                "Code: 9524, Layer: 0, Head: 14\n",
                "Code: 15549, Layer: 0, Head: 14\n",
                "Code: 7802, Layer: 0, Head: 14\n",
                "Code: 11942, Layer: 0, Head: 14\n",
                "Code: 4095, Layer: 0, Head: 1\n",
                "Code: 2179, Layer: 0, Head: 1\n",
                "Code: 22425, Layer: 0, Head: 1\n",
                "Code: 10661, Layer: 0, Head: 1\n",
                "Code: 8598, Layer: 0, Head: 1\n",
                "\"\"\"\n",
                "\n",
                "topic = \"friend\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 20506, Layer: 0, Head: 11\n",
                "Code: 6103, Layer: 0, Head: 11\n",
                "Code: 15764, Layer: 0, Head: 11\n",
                "Code: 14060, Layer: 0, Head: 11\n",
                "Code: 21005, Layer: 0, Head: 11\n",
                "Code: 16006, Layer: 0, Head: 11\n",
                "Code: 12290, Layer: 0, Head: 11\n",
                "Code: 7404, Layer: 0, Head: 11\n",
                "Code: 2471, Layer: 0, Head: 13\n",
                "\"\"\"\n",
                "\n",
                "topic = \"flower\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 23967, Layer: 0, Head: 13\n",
                "Code: 13533, Layer: 0, Head: 13\n",
                "Code: 4175, Layer: 0, Head: 13\n",
                "Code: 6390, Layer: 0, Head: 13\n",
                "Code: 18765, Layer: 0, Head: 13\n",
                "Code: 1775, Layer: 0, Head: 13\n",
                "Code: 7430, Layer: 0, Head: 13\n",
                "Code: 9269, Layer: 0, Head: 13\n",
                "\"\"\"\n",
                "\n",
                "topic = \"fire\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 9151, Layer: 0, Head: 13\n",
                "Code: 6389, Layer: 0, Head: 13\n",
                "Code: 16473, Layer: 0, Head: 13\n",
                "Code: 24184, Layer: 0, Head: 13\n",
                "Code: 11224, Layer: 0, Head: 13\n",
                "Code: 16757, Layer: 0, Head: 13\n",
                "Code: 16684, Layer: 0, Head: 13\n",
                "Code: 22825, Layer: 0, Head: 13\n",
                "Code: 22980, Layer: 0, Head: 14\n",
                "Code: 6544, Layer: 0, Head: 14\n",
                "Code: 2672, Layer: 0, Head: 14\n",
                "Code: 5791, Layer: 0, Head: 14\n",
                "Code: 22544, Layer: 0, Head: 14\n",
                "Code: 6971, Layer: 0, Head: 14\n",
                "Code: 23452, Layer: 0, Head: 14\n",
                "Code: 708, Layer: 0, Head: 14\n",
                "\"\"\"\n",
                "\n",
                "topic = \"prince|crown|king|castle\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 28, Layer: 0, Head: 13\n",
                "Code: 19802, Layer: 0, Head: 13\n",
                "Code: 22851, Layer: 0, Head: 13\n",
                "Code: 8907, Layer: 0, Head: 13\n",
                "Code: 18042, Layer: 0, Head: 13\n",
                "Code: 9619, Layer: 0, Head: 13\n",
                "Code: 15278, Layer: 0, Head: 13\n",
                "Code: 9649, Layer: 0, Head: 13\n",
                "Code: 13055, Layer: 0, Head: 14\n",
                "Code: 13575, Layer: 0, Head: 14\n",
                "Code: 9784, Layer: 0, Head: 14\n",
                "Code: 19023, Layer: 0, Head: 14\n",
                "Code: 7704, Layer: 0, Head: 14\n",
                "Code: 6056, Layer: 0, Head: 14\n",
                "\"\"\"\n",
                "\n",
                "topic = \"baby\"\n",
                "topic_codes_str[\n",
                "    topic\n",
                "] = \"\"\"\n",
                "Code: 66, Layer: 0, Head: 13\n",
                "Code: 657, Layer: 0, Head: 13\n",
                "Code: 9965, Layer: 0, Head: 13\n",
                "Code: 13724, Layer: 0, Head: 13\n",
                "Code: 5276, Layer: 0, Head: 13\n",
                "Code: 11101, Layer: 0, Head: 13\n",
                "Code: 10272, Layer: 0, Head: 13\n",
                "Code: 3067, Layer: 0, Head: 3\n",
                "Code: 18686, Layer: 0, Head: 3\n",
                "Code: 430, Layer: 0, Head: 3\n",
                "Code: 12364, Layer: 0, Head: 3\n",
                "Code: 1209, Layer: 0, Head: 3\n",
                "Code: 13863, Layer: 0, Head: 3\n",
                "Code: 15111, Layer: 0, Head: 3\n",
                "Code: 1185, Layer: 0, Head: 3\n",
                "\"\"\"\n",
                "\n",
                "# this converts the strings to lists of topic codes of the type `CodeInfo` that the library uses.\n",
                "cb_at = cb_model.config.codebook_at[0]\n",
                "topic_codes = {\n",
                "    k: cb_utils.parse_topic_codes_string(v, pos=None, code_append=False, cb_at=cb_at)\n",
                "    for k, v in topic_codes_str.items()\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Code Intervention\n",
                "\n",
                "Now we perform the code intervention for a specific topic using the `generate_with_codes` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# specify the topic you want the generations to steer towards\n",
                "topic = \"baby\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "argument of type 'NoneType' is not iterable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m inp_tensor \u001b[39m=\u001b[39m cb_model\u001b[39m.\u001b[39mto_tokens(text_input, prepend_bos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m inp_tensor \u001b[39m=\u001b[39m inp_tensor\u001b[39m.\u001b[39mrepeat(\u001b[39m10\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m gen \u001b[39m=\u001b[39m cb_utils\u001b[39m.\u001b[39;49mgenerate_with_codes(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     inp_tensor,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     cb_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     list_of_code_infos\u001b[39m=\u001b[39;49mlist_of_code_infos,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     generate_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m200\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdo_sample\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m gen \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(g[\u001b[39m1\u001b[39m:]) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taufeeque/Desktop/codebook-features/tutorials/code_intervention.ipynb#Y144sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, g \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(gen):\n",
                        "File \u001b[0;32m/data/codebook-features/codebook_features/utils.py:404\u001b[0m, in \u001b[0;36mgenerate_with_codes\u001b[0;34m(input, cb_model, list_of_code_infos, tokfsm, generate_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_with_codes\u001b[39m(\n\u001b[1;32m    397\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    398\u001b[0m     cb_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m     generate_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    402\u001b[0m ):\n\u001b[1;32m    403\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sample from the language model while activating the codes in `list_of_code_infos`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     gen \u001b[39m=\u001b[39m run_model_fn_with_codes(\n\u001b[1;32m    405\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    406\u001b[0m         cb_model,\n\u001b[1;32m    407\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgenerate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    408\u001b[0m         generate_kwargs,\n\u001b[1;32m    409\u001b[0m         list_of_code_infos,\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m tokfsm\u001b[39m.\u001b[39mseq_to_traj(gen)[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m tokfsm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gen\n",
                        "File \u001b[0;32m/data/codebook-features/codebook_features/utils.py:386\u001b[0m, in \u001b[0;36mrun_model_fn_with_codes\u001b[0;34m(input, cb_model, fn_name, fn_kwargs, list_of_code_infos)\u001b[0m\n\u001b[1;32m    381\u001b[0m     fn_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    382\u001b[0m hook_fns \u001b[39m=\u001b[39m [\n\u001b[1;32m    383\u001b[0m     partial(patch_in_codes, pos\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mpos, code\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mcode, code_pos\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mcode_pos)\n\u001b[1;32m    384\u001b[0m     \u001b[39mfor\u001b[39;00m tupl \u001b[39min\u001b[39;00m list_of_code_infos\n\u001b[1;32m    385\u001b[0m ]\n\u001b[0;32m--> 386\u001b[0m fwd_hooks \u001b[39m=\u001b[39m [\n\u001b[1;32m    387\u001b[0m     (get_cb_hook_key(tupl\u001b[39m.\u001b[39mcb_at, tupl\u001b[39m.\u001b[39mlayer, tupl\u001b[39m.\u001b[39mhead), hook_fns[i])\n\u001b[1;32m    388\u001b[0m     \u001b[39mfor\u001b[39;00m i, tupl \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(list_of_code_infos)\n\u001b[1;32m    389\u001b[0m ]\n\u001b[1;32m    390\u001b[0m cb_model\u001b[39m.\u001b[39mreset_hook_kwargs()\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m cb_model\u001b[39m.\u001b[39mhooks(fwd_hooks, [], \u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m hooked_model:\n",
                        "File \u001b[0;32m/data/codebook-features/codebook_features/utils.py:387\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    381\u001b[0m     fn_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    382\u001b[0m hook_fns \u001b[39m=\u001b[39m [\n\u001b[1;32m    383\u001b[0m     partial(patch_in_codes, pos\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mpos, code\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mcode, code_pos\u001b[39m=\u001b[39mtupl\u001b[39m.\u001b[39mcode_pos)\n\u001b[1;32m    384\u001b[0m     \u001b[39mfor\u001b[39;00m tupl \u001b[39min\u001b[39;00m list_of_code_infos\n\u001b[1;32m    385\u001b[0m ]\n\u001b[1;32m    386\u001b[0m fwd_hooks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 387\u001b[0m     (get_cb_hook_key(tupl\u001b[39m.\u001b[39;49mcb_at, tupl\u001b[39m.\u001b[39;49mlayer, tupl\u001b[39m.\u001b[39;49mhead), hook_fns[i])\n\u001b[1;32m    388\u001b[0m     \u001b[39mfor\u001b[39;00m i, tupl \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(list_of_code_infos)\n\u001b[1;32m    389\u001b[0m ]\n\u001b[1;32m    390\u001b[0m cb_model\u001b[39m.\u001b[39mreset_hook_kwargs()\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m cb_model\u001b[39m.\u001b[39mhooks(fwd_hooks, [], \u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m hooked_model:\n",
                        "File \u001b[0;32m/data/codebook-features/codebook_features/utils.py:361\u001b[0m, in \u001b[0;36mget_cb_hook_key\u001b[0;34m(cb_at, layer_idx, gcb_idx)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cb_hook_key\u001b[39m(cb_at: \u001b[39mstr\u001b[39m, layer_idx: \u001b[39mint\u001b[39m, gcb_idx: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the layer name used to store hooks/cache.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     comp_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mattn\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39mattn\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m cb_at \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m gcb_idx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00mlayer_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mcomp_name\u001b[39m}\u001b[39;00m\u001b[39m.codebook_layer.hook_codebook_ids\u001b[39m\u001b[39m\"\u001b[39m\n",
                        "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
                    ]
                }
            ],
            "source": [
                "# CodeInfo objects hold a code's associated metadata (e.g. position in the network)\n",
                "list_of_code_infos = topic_codes[topic]\n",
                "\n",
                "text_input = \"Once upon a time,\"\n",
                "inp_tensor = cb_model.to_tokens(text_input, prepend_bos=True).to(device)\n",
                "inp_tensor = inp_tensor.repeat(10, 1)\n",
                "gen = cb_utils.generate_with_codes(\n",
                "    inp_tensor,\n",
                "    cb_model,\n",
                "    list_of_code_infos=list_of_code_infos,\n",
                "    generate_kwargs={\"max_new_tokens\": 200, \"do_sample\": True, \"temperature\": 1},\n",
                ")\n",
                "gen = [tokenizer.decode(g[1:]) for g in gen]\n",
                "for i, g in enumerate(gen):\n",
                "    print(f\"Story {i}:\")\n",
                "    print(g)\n",
                "    print(\"************************************\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Quantitative Evaluation of Topic Steering\n",
                "\n",
                "Here we do a quantitative evaluation of topic steering by measuring the fraction of generated texts that contain the topic string in the generated text."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For each of the topic that we have for steering, we generate 10 samples with the topic code patched in for each of our prompt. We then measure the fraction of generated texts that contain the topic string in the generated text. Note that this is an imperfect evaluation, as the model may generate strings related to the topic but not include the topic word itself (e.g. 'babies' vs 'baby')."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompts = [\n",
                "    \"\",\n",
                "    \"Once upon a time,\",\n",
                "    \"Once there was a\",\n",
                "    \"A long time ago,\",\n",
                "]\n",
                "\n",
                "prompt_completions = {}\n",
                "for topic in tqdm(topic_codes_str):\n",
                "    list_of_arg_tuples = topic_codes[topic]\n",
                "    prompt_completions[topic] = {}\n",
                "    for prompt in prompts:\n",
                "        prompt_token = cb_model.to_tokens(prompt, prepend_bos=True).to(device)\n",
                "        prompt_token = prompt_token.repeat(10, 1)\n",
                "        gen = cb_utils.generate_with_codes(\n",
                "            prompt_token,\n",
                "            cb_model,\n",
                "            list_of_code_infos=list_of_arg_tuples,\n",
                "            generate_kwargs={\n",
                "                \"max_new_tokens\": 200,\n",
                "                \"do_sample\": True,\n",
                "                \"temperature\": 1,\n",
                "            },\n",
                "        )\n",
                "        gen = [tokenizer.decode(gen[i][1:]) for i in range(len(gen))]\n",
                "        prompt_completions[topic][prompt] = gen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "topic_in_prompt_completion = {}\n",
                "for topic in topic_codes_str:\n",
                "    if not topic:\n",
                "        continue\n",
                "    topic_in_prompt_completion[topic] = {}\n",
                "    for prompt in prompts:\n",
                "        topic_in_prompt_completion[topic][prompt] = 0\n",
                "        for completion in prompt_completions[topic][prompt]:\n",
                "            if re.search(topic.lower(), completion.lower()):\n",
                "                topic_in_prompt_completion[topic][prompt] += 1\n",
                "        topic_in_prompt_completion[topic][prompt] /= len(\n",
                "            prompt_completions[topic][prompt]\n",
                "        )\n",
                "\n",
                "topic_in_prompt_completion_avg = {}\n",
                "for topic in topic_codes_str:\n",
                "    if not topic:\n",
                "        continue\n",
                "    topic_in_prompt_completion_avg[topic] = 0\n",
                "    for prompt in prompts:\n",
                "        topic_in_prompt_completion_avg[topic] += topic_in_prompt_completion[topic][\n",
                "            prompt\n",
                "        ]\n",
                "    topic_in_prompt_completion_avg[topic] /= len(prompts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We also get the baseline fraction of generated texts that contain the topic string in the generated text with 10 samples that don't have any topic code patched in for each of our prompt. This gives us a baseline number for each topic being mentioned by default."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "orig_prompts = prompt_completions[\"\"]\n",
                "topic_in_orig_prompt_completion = {}\n",
                "\n",
                "for topic in topic_codes_str:\n",
                "    if not topic:\n",
                "        continue\n",
                "    topic_in_orig_prompt_completion[topic] = {}\n",
                "    for prompt in prompts:\n",
                "        topic_in_orig_prompt_completion[topic][prompt] = 0\n",
                "        for completion in orig_prompts[prompt]:\n",
                "            if re.search(topic.lower(), completion.lower()):\n",
                "                topic_in_orig_prompt_completion[topic][prompt] += 1\n",
                "        topic_in_orig_prompt_completion[topic][prompt] /= len(\n",
                "            prompt_completions[topic][prompt]\n",
                "        )\n",
                "\n",
                "topic_in_orig_prompt_completion_avg = {}\n",
                "for topic in topic_codes_str:\n",
                "    if not topic:\n",
                "        continue\n",
                "    topic_in_orig_prompt_completion_avg[topic] = 0\n",
                "    for prompt in prompts:\n",
                "        topic_in_orig_prompt_completion_avg[topic] += topic_in_orig_prompt_completion[\n",
                "            topic\n",
                "        ][prompt]\n",
                "    topic_in_orig_prompt_completion_avg[topic] /= len(prompts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we can see, the fraction of generated texts that contain the topic string in the generated text is much higher when we patch in the topic code compared to the baseline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Baseline (no topic steering):\n",
                        "Topic\t\t\t\tAvg Steering (%)\n",
                        "dragon\t\t\t\t2.5\n",
                        "slide\t\t\t\t2.5\n",
                        "friend\t\t\t\t42.5\n",
                        "flower\t\t\t\t0.0\n",
                        "fire\t\t\t\t2.5\n",
                        "prince|crown|king|castle\t\t\t\t40.0\n",
                        "baby\t\t\t\t0.0\n",
                        "\n",
                        "\n",
                        "Topic steering with code interventions:\n",
                        "Topic\t\t\t\tAvg Steering (%)\n",
                        "dragon\t\t\t\t65.0\n",
                        "slide\t\t\t\t95.0\n",
                        "friend\t\t\t\t75.0\n",
                        "flower\t\t\t\t90.0\n",
                        "fire\t\t\t\t100.0\n",
                        "prince|crown|king|castle\t\t\t\t87.5\n",
                        "baby\t\t\t\t90.0\n"
                    ]
                }
            ],
            "source": [
                "print(\"Baseline (no topic steering):\")\n",
                "print(f\"Topic\\t\\t\\t\\tAvg Steering (%)\")\n",
                "for topic, frac in topic_in_orig_prompt_completion_avg.items():\n",
                "    if not topic:\n",
                "        continue\n",
                "    print(f\"{topic}\\t\\t\\t\\t{frac*100:.1f}\")\n",
                "\n",
                "print()\n",
                "print()\n",
                "\n",
                "print(\"Topic steering with code interventions:\")\n",
                "print(f\"Topic\\t\\t\\t\\tAvg Steering (%)\")\n",
                "for topic, frac in topic_in_prompt_completion_avg.items():\n",
                "    if not topic:\n",
                "        continue\n",
                "    print(f\"{topic}\\t\\t\\t\\t{frac*100:.1f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
