{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall -y transformer_lens\n",
    "! pip install -U transformers\n",
    "! pip install git+https://github.com/taufeeque9/TransformerLens/\n",
    "! pip install git+https://github.com/minyoungg/vqtorch/\n",
    "! pip install termcolor\n",
    "! pip install -U accelerate\n",
    "! pip install -U kaleido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/codebook-features/codebook_features/train_toy_model.py:400: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"config\", config_name=\"toy_main\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fd6d10aac20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "import transformers\n",
    "import codebook_features\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import copy\n",
    "import wandb\n",
    "import json\n",
    "import transformer_lens.utils as utils\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    GPTNeoXConfig,\n",
    "    GPTNeoXForCausalLM,\n",
    "    GPT2TokenizerFast,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from torch.utils.data import IterableDataset\n",
    "from codebook_features import models, run_clm, train_toy_model, trainer as cb_trainer   \n",
    "from codebook_features.utils import *\n",
    "from codebook_features.toy_utils import *\n",
    "import os\n",
    "import math\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf0d3213434e97bb13b2f61e7fc0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"EleutherAI/pythia-410m-deduped\"\n",
    "# pretrained_path = \"/data/outputs/2023-04-25/00-10-18/output_main/\" # mlp 70m\n",
    "# pretrained_path = \"/data/outputs/2023-05-08/01-42-10/output_main/\" # mlp 410m ccb\n",
    "pretrained_path = \"/shared/outputs/2023-05-04/07-19-37/output_main/\" # mlp 410m vcb\n",
    "device = \"cuda\"\n",
    "orig_cb_model = models.wrap_codebook(model_or_path=model_name_or_path, pretrained_path=pretrained_path)\n",
    "orig_cb_model = orig_cb_model.to(device).eval()\n",
    "orig_cb_model.disable_logging()\n",
    "hooked_kwargs = dict(center_unembed=False,fold_value_biases=False,center_writing_weights=True,fold_ln=True,refactor_factored_attn_matrices=False,device=device)\n",
    "cb_model = models.convert_to_hooked_model(model_name_or_path, orig_cb_model, hooked_kwargs=hooked_kwargs)\n",
    "# # cb_model = cb_model.model\n",
    "cb_model.disable_logging()\n",
    "cb_model = cb_model.to(device).eval()\n",
    "model = orig_cb_model\n",
    "tokenizer = cb_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/17/2023 04:11:29 - WARNING - codebook_features.run_clm - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/17/2023 04:11:29 - INFO - codebook_features.run_clm - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "codebook_reg_p=None,\n",
      "codebook_weight_decay=0.0,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=temp_mlp/runs/Jul17_04-11-29_tf-db1-u047w2yq48j-launch-0-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_lr_factor=1.0,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=temp_mlp/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=temp_mlp/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_model_params=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "07/17/2023 04:11:30 - INFO - datasets.info - Loading Dataset Infos from /homedir/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "07/17/2023 04:11:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "07/17/2023 04:11:30 - INFO - datasets.info - Loading Dataset info from /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
      "07/17/2023 04:11:30 - WARNING - datasets.builder - Found cached dataset wikitext (/shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "07/17/2023 04:11:30 - INFO - datasets.info - Loading Dataset info from /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c751cea1e723405f93160b33018ebd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/17/2023 04:11:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-12eba99cec1e5a8c.arrow\n",
      "07/17/2023 04:11:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2f9b0caef800bd1c.arrow\n",
      "07/17/2023 04:11:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8f89a48d258ee8.arrow\n",
      "07/17/2023 04:11:31 - WARNING - codebook_features.run_clm - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.\n",
      "07/17/2023 04:11:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b43aabd3c97f8130.arrow\n",
      "07/17/2023 04:11:33 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-019394b2f859ffdf.arrow\n",
      "07/17/2023 04:11:33 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-60d51b0539cf49d2.arrow\n",
      "07/17/2023 04:11:33 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /shared/.cache/huggingface/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cedc35ecac3e31e0.arrow\n"
     ]
    }
   ],
   "source": [
    "report_to = \"none\"\n",
    "# report_to = \"all\"\n",
    "training_args = run_clm.TrainingArguments(\n",
    "    #     no_cuda=True,\n",
    "    output_dir=\"temp_mlp/\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    ")\n",
    "\n",
    "model_args = run_clm.ModelArguments(model_name_or_path=pretrained_path, cache_dir=\"/shared/.cache/huggingface/\")\n",
    "data_args = run_clm.DataTrainingArguments(\n",
    "    dataset_name=\"wikitext\", dataset_config_name=\"wikitext-103-v1\", streaming=False,\n",
    ")\n",
    "\n",
    "trainer, lm_datasets, last_checkpoint = run_clm.get_trainer_and_dataset(\n",
    "    model_args,\n",
    "    data_args,\n",
    "    training_args,\n",
    "    model,\n",
    "    optimizers=(None, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:07, 281.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading activations from /shared/cb_eval_acts/pythia410_mlp_2023-07-13_03-31-50\n",
      "{'eval_loss': 3.046041965484619, 'eval_accuracy': 0.4174623655913979, 'eval_runtime': 1042.2949, 'eval_samples_per_second': 1.919, 'eval_steps_per_second': 0.06, 'eval_multicode_k': 1, 'eval_dead_code_fraction/layer0': 0.0088, 'eval_MSE/layer0': 207.70325073242185, 'eval_input_norm/layer0': 20.950143371582026, 'eval_output_norm/layer0': 23.073701156616206, 'eval_dead_code_fraction/layer1': 0.0, 'eval_MSE/layer1': 245.72139245605462, 'eval_input_norm/layer1': 6.6752554817199705, 'eval_output_norm/layer1': 19.145910171508795, 'eval_dead_code_fraction/layer2': 0.0, 'eval_MSE/layer2': 213.10583129882806, 'eval_input_norm/layer2': 7.069615131378175, 'eval_output_norm/layer2': 18.375904968261718, 'eval_dead_code_fraction/layer3': 0.0, 'eval_MSE/layer3': 194.4261348876953, 'eval_input_norm/layer3': 7.286291618347167, 'eval_output_norm/layer3': 17.63788438415527, 'eval_dead_code_fraction/layer4': 0.0, 'eval_MSE/layer4': 190.74791235351555, 'eval_input_norm/layer4': 7.8596038818359375, 'eval_output_norm/layer4': 17.630446685791014, 'eval_dead_code_fraction/layer5': 0.0, 'eval_MSE/layer5': 265.12472900390634, 'eval_input_norm/layer5': 10.836742431640626, 'eval_output_norm/layer5': 16.9481455078125, 'eval_dead_code_fraction/layer6': 0.0, 'eval_MSE/layer6': 175.26022631835943, 'eval_input_norm/layer6': 8.504824562072756, 'eval_output_norm/layer6': 17.139855056762695, 'eval_dead_code_fraction/layer7': 0.0, 'eval_MSE/layer7': 168.7509388427734, 'eval_input_norm/layer7': 8.94763405609131, 'eval_output_norm/layer7': 16.70788580322266, 'eval_dead_code_fraction/layer8': 0.0, 'eval_MSE/layer8': 188.8214145507812, 'eval_input_norm/layer8': 10.056889450073248, 'eval_output_norm/layer8': 16.44906762695312, 'eval_dead_code_fraction/layer9': 0.0, 'eval_MSE/layer9': 157.97772253417975, 'eval_input_norm/layer9': 9.252333221435551, 'eval_output_norm/layer9': 16.45955023193359, 'eval_dead_code_fraction/layer10': 0.0, 'eval_MSE/layer10': 151.92339111328124, 'eval_input_norm/layer10': 9.037760467529296, 'eval_output_norm/layer10': 15.914534568786625, 'eval_dead_code_fraction/layer11': 0.0, 'eval_MSE/layer11': 150.86885156249994, 'eval_input_norm/layer11': 9.158286293029787, 'eval_output_norm/layer11': 15.839693054199216, 'eval_dead_code_fraction/layer12': 0.0, 'eval_MSE/layer12': 148.14237475585935, 'eval_input_norm/layer12': 8.801263320922851, 'eval_output_norm/layer12': 15.651890258789065, 'eval_dead_code_fraction/layer13': 0.0, 'eval_MSE/layer13': 152.38228417968753, 'eval_input_norm/layer13': 8.62443383026123, 'eval_output_norm/layer13': 15.967729751586912, 'eval_dead_code_fraction/layer14': 0.0, 'eval_MSE/layer14': 156.65222802734377, 'eval_input_norm/layer14': 8.800631408691407, 'eval_output_norm/layer14': 16.22552816772462, 'eval_dead_code_fraction/layer15': 0.0, 'eval_MSE/layer15': 153.98280590820315, 'eval_input_norm/layer15': 8.931493087768553, 'eval_output_norm/layer15': 16.08847515869141, 'eval_dead_code_fraction/layer16': 0.0, 'eval_MSE/layer16': 154.62259252929687, 'eval_input_norm/layer16': 8.749254928588867, 'eval_output_norm/layer16': 16.07568086242675, 'eval_dead_code_fraction/layer17': 0.0, 'eval_MSE/layer17': 152.3637449951171, 'eval_input_norm/layer17': 9.86432670593262, 'eval_output_norm/layer17': 16.0137205734253, 'eval_dead_code_fraction/layer18': 0.0, 'eval_MSE/layer18': 156.05150415039057, 'eval_input_norm/layer18': 10.19028242492676, 'eval_output_norm/layer18': 16.225067687988286, 'eval_dead_code_fraction/layer19': 0.0, 'eval_MSE/layer19': 157.43143627929683, 'eval_input_norm/layer19': 11.230754791259765, 'eval_output_norm/layer19': 16.091097000122073, 'eval_dead_code_fraction/layer20': 0.0, 'eval_MSE/layer20': 153.2299001464843, 'eval_input_norm/layer20': 10.892868927001953, 'eval_output_norm/layer20': 15.862607376098634, 'eval_dead_code_fraction/layer21': 0.0, 'eval_MSE/layer21': 148.32544067382815, 'eval_input_norm/layer21': 11.126961715698243, 'eval_output_norm/layer21': 15.704189201354978, 'eval_dead_code_fraction/layer22': 0.0, 'eval_MSE/layer22': 151.69713635253908, 'eval_input_norm/layer22': 10.830856895446779, 'eval_output_norm/layer22': 15.942548217773437, 'eval_dead_code_fraction/layer23': 0.0, 'eval_MSE/layer23': 160.47150463867183, 'eval_input_norm/layer23': 8.779298347473146, 'eval_output_norm/layer23': 16.338713745117186}\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "max_samples = 2000\n",
    "dataset = lm_datasets[\"train\"].select(rng.choice(len(lm_datasets[\"train\"]), max_samples, replace=False))\n",
    "tokens = dataset[\"input_ids\"]\n",
    "tokens_str = []\n",
    "for i, token in tqdm(enumerate(tokens)):\n",
    "    tokens_str.append([tokenizer.decode(t) for t in token])\n",
    "tokens_str = np.array(tokens_str)\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "trainer.args.report_on = \"none\"\n",
    "cb_acts = {}\n",
    "ft_tkns = {}\n",
    "\n",
    "def store_cb_activations(key, codebook_ids, codebook_acts=cb_acts):\n",
    "    assert len(codebook_ids.shape) == 3  # (bs, seq_len, k_codebook)\n",
    "    if key not in codebook_acts:\n",
    "        codebook_acts[key] = []\n",
    "    codebook_acts[key].append(codebook_ids)\n",
    "\n",
    "\n",
    "output_dir = \"/shared/cb_eval_acts/pythia410_mlp*\"\n",
    "dirs = glob.glob(output_dir)\n",
    "dirs.sort(key=os.path.getmtime)\n",
    "\n",
    "if len(dirs) > 0:\n",
    "    output_dir = dirs[-1]\n",
    "    print(\"Loading activations from\", output_dir)\n",
    "    with open(f\"{output_dir}/cb_acts.pkl\", \"rb\") as f:\n",
    "        cb_acts = pickle.load(f)\n",
    "    tokens = np.load(f\"{output_dir}/tokens.npy\")\n",
    "    metrics = np.load(f\"{output_dir}/metrics.npy\", allow_pickle=True)\n",
    "else:\n",
    "    orig_cb_model.set_hook_fn(store_cb_activations)\n",
    "    orig_cb_model.reset_codebook_metrics(), orig_cb_model.reset_hook_kwargs()\n",
    "    orig_cb_model.enable_logging()\n",
    "    metrics = trainer.evaluate(dataset)\n",
    "    for k, v in cb_acts.items():\n",
    "        cb_acts[k] = np.concatenate(v, axis=0)\n",
    "    output_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = \"/shared/cb_eval_acts/pythia410_mlp_\" + output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(f\"{output_dir}/tokens.npy\", tokens)\n",
    "    np.save(f\"{output_dir}/metrics.npy\", metrics)\n",
    "    with open(f\"{output_dir}/cb_acts.pkl\", \"wb\") as f:\n",
    "        pickle.dump(cb_acts, f)\n",
    "    print(\"Saved activations to\", output_dir)\n",
    "\n",
    "print(metrics)\n",
    "num_codes = 10_000\n",
    "cb_at = \"mlp\"\n",
    "ccb = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/shared/cb_eval_acts/pythia410_mlp_2023-07-13_03-31-50'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:58<00:00, 34.12it/s]\n",
      "100%|██████████| 2000/2000 [00:51<00:00, 39.12it/s]\n",
      "100%|██████████| 2000/2000 [01:10<00:00, 28.52it/s]\n",
      "100%|██████████| 2000/2000 [00:53<00:00, 37.46it/s]\n",
      "100%|██████████| 2000/2000 [00:55<00:00, 36.21it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 37.81it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 37.99it/s]\n",
      "100%|██████████| 2000/2000 [01:08<00:00, 29.19it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 38.43it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 38.05it/s]\n",
      " 40%|████      | 805/2000 [00:21<00:31, 38.16it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 2000/2000 [01:07<00:00, 29.58it/s]\n",
      "100%|██████████| 2000/2000 [00:51<00:00, 38.87it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 38.33it/s]\n",
      "100%|██████████| 2000/2000 [00:52<00:00, 37.95it/s]\n",
      "100%|██████████| 2000/2000 [01:14<00:00, 26.68it/s]\n",
      "100%|██████████| 2000/2000 [00:51<00:00, 38.72it/s]\n",
      "100%|██████████| 2000/2000 [00:57<00:00, 34.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "freq_filter = 5\n",
    "\n",
    "def process_component(head=None, layer=None):\n",
    "    layer_dir = f'{output_dir}/layer{layer}'\n",
    "    os.makedirs(layer_dir, exist_ok=True)\n",
    "    comp_name = f\"layer{layer}_{cb_at}{ccb}{head if ccb else ''}\"\n",
    "    ft_tkns = features_to_tokens(comp_name, cb_acts, num_codes=num_codes)\n",
    "    with open(f\"{layer_dir}/ft_tkns_layer{layer}{'_head'+head if head else ''}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ft_tkns, f)\n",
    "    codes, freqs, acts = print_ft_tkns(ft_tkns,n=3,start=0,stop=10000,tokens=tokens_str,max_examples=100,freq_filter=freq_filter)\n",
    "    df = pd.DataFrame({\"freqs\": freqs, \"acts\": acts}, index=codes)\n",
    "    df.loc[:, \"acts\"] = df[\"acts\"].apply(lambda x : x.replace('\\n', '\\\\n'))\n",
    "    df.to_csv(f\"{layer_dir}/code_token_acts_layer{layer}{'_head'+head if head else ''}_filt{freq_filter}.csv\", index_label=\"code\")\n",
    "\n",
    "for layer in tqdm(range(1, cb_model.cfg.n_layers)):\n",
    "    process_component(layer=layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = orig_cb_model.generate(do_sample=True, max_length=100, temperature=0.7)\n",
    "tokenizer.decode(seq[0].tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model.generate(\"Resident Evil\", max_new_tokens=10, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \" American\"\n",
    "example_answer = \" States\"\n",
    "\n",
    "utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    cb_model,\n",
    "    prepend_bos=False,\n",
    "    prepend_space_to_answer=False,\n",
    "    top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Paris is the capital city of\"\n",
    "example_answer = \" France\"\n",
    "\n",
    "utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    cb_model,\n",
    "    prepend_bos=False,\n",
    "    prepend_space_to_answer=False,\n",
    "    top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"The Italian dish\"\n",
    "input_b = \"The football club Manchester\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"Washington is the capital of The United\"\n",
    "input_b = \"London is the capital of The United\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "print(input_a)\n",
    "print(input_b)\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"Paris is\"\n",
    "input_b = \"The football club Manchester United is at the top of the league. Manchester\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "print(input_a)\n",
    "print(input_b)\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,17,19,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = -1\n",
    "# for layer_idx in range(cb_model.cfg.n_layers):\n",
    "#     n_layers = 1\n",
    "for layer_idx in range(1):\n",
    "    n_layers = cb_model.cfg.n_layers\n",
    "    # n_layers = 1\n",
    "    cb_at = ( [\"mlp\"] * cb_model.cfg.n_heads ) * n_layers\n",
    "    head = (list(range(cb_model.cfg.n_heads))) * n_layers\n",
    "    # layer = [layer_idx] * len(cb_at)\n",
    "    layer = range_over_repeat(n_layers, repeat=cb_model.cfg.n_heads)\n",
    "    # layer = range_over_repeat([0,17,19,20], repeat=cb_model.cfg.n_heads)\n",
    "    code = [cache_b[get_cb_layer_name(cb_at[i], layer[i], head[i])][0, pos, :] for i in range(len(cb_at))]\n",
    "    # ind = [0,1,2,3,4,5]\n",
    "    ind = range(len(cb_at))\n",
    "    mod_logits, mod_cache = run_with_codes(\n",
    "        input_a,\n",
    "        cb_model,\n",
    "        [code[i] for i in ind],\n",
    "        [cb_at[i] for i in ind],\n",
    "        [layer[i] for i in ind],\n",
    "        [head[i] for i in ind],\n",
    "        pos=[pos],\n",
    "    )\n",
    "    print(\"Layer:\", layer_idx)\n",
    "    print(logits_to_pred(mod_logits, tokenizer, k=5))\n",
    "    print(f\"JSD(a <- b, b) = {JSD(mod_logits, logits_b, pos=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_in_codes(run_cb_ids, hook, pos, code):\n",
    "    \"\"\"Patch in the `code` at `run_cb_ids`.\"\"\"\n",
    "    if pos:\n",
    "        run_cb_ids[:, pos] = code\n",
    "    else:\n",
    "        run_cb_ids[:, :] = code\n",
    "    return run_cb_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_code_changes(mod_cache, cache_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_divs = torch.zeros((cb_model.cfg.n_layers, cb_model.cfg.n_heads))\n",
    "# for layer_idx in tqdm(range(cb_model.cfg.n_layers)):\n",
    "for layer_idx in tqdm(range(2)):\n",
    "    for head_idx in range(cb_model.cfg.n_heads):\n",
    "        # n_layers = cb_model.cfg.n_layers\n",
    "        cb_at = [\"mlp\"]\n",
    "        head = [head_idx]\n",
    "        layer = [layer_idx] * len(cb_at)\n",
    "        # layer = range_over_repeat(n_layers, repeat=cb_model.cfg.n_heads)\n",
    "        # layer = range_over_repeat([0,17,19,20], repeat=cb_model.cfg.n_heads)\n",
    "        code = [cache_b[get_cb_layer_name(cb_at[i], layer[i], head[i])][0, -1, :] for i in range(len(cb_at))]\n",
    "        # ind = [0,1,2,3,4,5]\n",
    "        ind = range(len(cb_at))\n",
    "        mod_logits, mod_cache = run_with_codes(\n",
    "            input_a,\n",
    "            cb_model,\n",
    "            [code[i] for i in ind],\n",
    "            [cb_at[i] for i in ind],\n",
    "            [layer[i] for i in ind],\n",
    "            [head[i] for i in ind],\n",
    "            pos=[-1],\n",
    "        )\n",
    "        # print(\"Layer, Head:\", layer_idx, head_idx)\n",
    "        # print(logits_to_pred(mod_logits, tokenizer, k=5))\n",
    "        # print(f\"JSD(a <- b, b) = {JSD(mod_logits, logits_b, pos=-1)}\")\n",
    "        js_divs[layer_idx, head_idx] = JSD(mod_logits, logits_b, pos=-1)\n",
    "\n",
    "# make the color map range start from 0\n",
    "imshow(js_divs, xaxis=\"head\", yaxis=\"layer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_over_repeat(end_or_list, repeat=1):\n",
    "    \n",
    "    if isinstance(end_or_list, int):\n",
    "        end_or_list = range(end_or_list)\n",
    "    l = []\n",
    "    for i in end_or_list:\n",
    "        l += [i] * repeat\n",
    "    return l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
