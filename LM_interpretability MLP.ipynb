{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall -y transformer_lens\n",
    "! pip install git+https://github.com/taufeeque9/TransformerLens/\n",
    "! pip install git+https://github.com/minyoungg/vqtorch/\n",
    "! pip install termcolor\n",
    "! pip install -U accelerate\n",
    "! pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "import transformers\n",
    "import codebook_features\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import copy\n",
    "import wandb\n",
    "import json\n",
    "import transformer_lens.utils as utils\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    GPTNeoXConfig,\n",
    "    GPTNeoXForCausalLM,\n",
    "    GPT2TokenizerFast,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from torch.utils.data import IterableDataset\n",
    "from codebook_features import models, run_clm, train_toy_model, trainer as cb_trainer   \n",
    "from codebook_features.utils import *\n",
    "from codebook_features.toy_utils import *\n",
    "import os\n",
    "import math\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"EleutherAI/pythia-410m-deduped\"\n",
    "# pretrained_path = \"/data/outputs/2023-04-25/00-10-18/output_main/\" # mlp 70m\n",
    "pretrained_path = \"/data/outputs/2023-05-08/01-42-10/output_main/\" # mlp 410m\n",
    "device = \"cuda\"\n",
    "orig_cb_model = models.wrap_codebook(model_or_path=model_name_or_path, pretrained_path=pretrained_path)\n",
    "from time import time\n",
    "t0 = time()\n",
    "orig_cb_model = orig_cb_model.to(device).eval()\n",
    "orig_cb_model.disable_logging()\n",
    "t1 = time()\n",
    "print(\"Loaded original cb model. Post time:\", t1-t0)\n",
    "hooked_kwargs = dict(center_unembed=False,fold_value_biases=False,center_writing_weights=True,fold_ln=True,refactor_factored_attn_matrices=False,device=device)\n",
    "cb_model = models.convert_to_hooked_model(model_name_or_path, orig_cb_model, hooked_kwargs=hooked_kwargs)\n",
    "# # cb_model = cb_model.model\n",
    "cb_model.disable_logging()\n",
    "cb_model = cb_model.to(device).eval()\n",
    "model = orig_cb_model\n",
    "tokenizer = cb_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_to = \"none\"\n",
    "# report_to = \"all\"\n",
    "training_args = run_clm.TrainingArguments(\n",
    "    #     no_cuda=True,\n",
    "    output_dir=\"temp_mlp/\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    ")\n",
    "\n",
    "model_args = run_clm.ModelArguments(model_name_or_path=pretrained_path, cache_dir=\"/data/.cache/huggingface/\")\n",
    "data_args = run_clm.DataTrainingArguments(\n",
    "    dataset_name=\"wikitext\", dataset_config_name=\"wikitext-103-v1\", streaming=False,\n",
    ")\n",
    "\n",
    "trainer, lm_datasets, last_checkpoint = run_clm.get_trainer_and_dataset(\n",
    "    model_args,\n",
    "    data_args,\n",
    "    training_args,\n",
    "    model,\n",
    "    optimizers=(None, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 2000\n",
    "dataset = lm_datasets[\"train\"].select(np.random.choice(len(lm_datasets[\"train\"]), max_samples, replace=False))\n",
    "tokens = dataset[\"input_ids\"]\n",
    "\n",
    "trainer.args.report_on = \"none\"\n",
    "codebook_acts = {}\n",
    "\n",
    "def store_cb_activations(key, codebook_ids, codebook_acts=codebook_acts):\n",
    "    assert len(codebook_ids.shape) == 3  # (bs, seq_len, k_codebook)\n",
    "    if key not in codebook_acts:\n",
    "        codebook_acts[key] = []\n",
    "    codebook_acts[key].append(codebook_ids)\n",
    "\n",
    "orig_cb_model.set_hook_fn(store_cb_activations)\n",
    "\n",
    "metrics = trainer.evaluate(dataset)\n",
    "print(metrics)\n",
    "\n",
    "cb_acts = codebook_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "output_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = \"/homedir/cb_eval_acts/mlp_\" + output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "np.save(f\"{output_dir}/tokens.npy\", tokens)\n",
    "np.save(f\"{output_dir}/metrics.npy\", metrics)\n",
    "with open(f\"{output_dir}/cb_acts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(codebook_acts, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = orig_cb_model.generate(do_sample=True, max_length=100, temperature=0.7)\n",
    "tokenizer.decode(seq[0].tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model.generate(\"Resident Evil\", max_new_tokens=10, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \" American\"\n",
    "example_answer = \" States\"\n",
    "\n",
    "utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    cb_model,\n",
    "    prepend_bos=False,\n",
    "    prepend_space_to_answer=False,\n",
    "    top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Paris is the capital city of\"\n",
    "example_answer = \" France\"\n",
    "\n",
    "utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    cb_model,\n",
    "    prepend_bos=False,\n",
    "    prepend_space_to_answer=False,\n",
    "    top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"The Italian dish\"\n",
    "input_b = \"The football club Manchester\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"Washington is the capital of The United\"\n",
    "input_b = \"London is the capital of The United\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "print(input_a)\n",
    "print(input_b)\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = \"Paris is\"\n",
    "input_b = \"The football club Manchester United is at the top of the league. Manchester\"\n",
    "\n",
    "input_a = cb_model.to_tokens(input_a, prepend_bos=False).to(\"cuda\")\n",
    "input_b = cb_model.to_tokens(input_b, prepend_bos=False).to(\"cuda\")\n",
    "print(input_a)\n",
    "print(input_b)\n",
    "logits_a, cache_a = cb_model.run_with_cache(input_a)\n",
    "logits_b, cache_b = cb_model.run_with_cache(input_b)\n",
    "\n",
    "print(f\"JSD(a, b) = {JSD(logits_a, logits_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,17,19,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = -1\n",
    "# for layer_idx in range(cb_model.cfg.n_layers):\n",
    "#     n_layers = 1\n",
    "for layer_idx in range(1):\n",
    "    n_layers = cb_model.cfg.n_layers\n",
    "    # n_layers = 1\n",
    "    cb_at = ( [\"mlp\"] * cb_model.cfg.n_heads ) * n_layers\n",
    "    head = (list(range(cb_model.cfg.n_heads))) * n_layers\n",
    "    # layer = [layer_idx] * len(cb_at)\n",
    "    layer = range_over_repeat(n_layers, repeat=cb_model.cfg.n_heads)\n",
    "    # layer = range_over_repeat([0,17,19,20], repeat=cb_model.cfg.n_heads)\n",
    "    code = [cache_b[get_cb_layer_name(cb_at[i], layer[i], head[i])][0, pos, :] for i in range(len(cb_at))]\n",
    "    # ind = [0,1,2,3,4,5]\n",
    "    ind = range(len(cb_at))\n",
    "    mod_logits, mod_cache = run_with_codes(\n",
    "        input_a,\n",
    "        cb_model,\n",
    "        [code[i] for i in ind],\n",
    "        [cb_at[i] for i in ind],\n",
    "        [layer[i] for i in ind],\n",
    "        [head[i] for i in ind],\n",
    "        pos=[pos],\n",
    "    )\n",
    "    print(\"Layer:\", layer_idx)\n",
    "    print(logits_to_pred(mod_logits, tokenizer, k=5))\n",
    "    print(f\"JSD(a <- b, b) = {JSD(mod_logits, logits_b, pos=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_in_codes(run_cb_ids, hook, pos, code):\n",
    "    \"\"\"Patch in the `code` at `run_cb_ids`.\"\"\"\n",
    "    if pos:\n",
    "        run_cb_ids[:, pos] = code\n",
    "    else:\n",
    "        run_cb_ids[:, :] = code\n",
    "    return run_cb_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_code_changes(mod_cache, cache_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_divs = torch.zeros((cb_model.cfg.n_layers, cb_model.cfg.n_heads))\n",
    "# for layer_idx in tqdm(range(cb_model.cfg.n_layers)):\n",
    "for layer_idx in tqdm(range(2)):\n",
    "    for head_idx in range(cb_model.cfg.n_heads):\n",
    "        # n_layers = cb_model.cfg.n_layers\n",
    "        cb_at = [\"mlp\"]\n",
    "        head = [head_idx]\n",
    "        layer = [layer_idx] * len(cb_at)\n",
    "        # layer = range_over_repeat(n_layers, repeat=cb_model.cfg.n_heads)\n",
    "        # layer = range_over_repeat([0,17,19,20], repeat=cb_model.cfg.n_heads)\n",
    "        code = [cache_b[get_cb_layer_name(cb_at[i], layer[i], head[i])][0, -1, :] for i in range(len(cb_at))]\n",
    "        # ind = [0,1,2,3,4,5]\n",
    "        ind = range(len(cb_at))\n",
    "        mod_logits, mod_cache = run_with_codes(\n",
    "            input_a,\n",
    "            cb_model,\n",
    "            [code[i] for i in ind],\n",
    "            [cb_at[i] for i in ind],\n",
    "            [layer[i] for i in ind],\n",
    "            [head[i] for i in ind],\n",
    "            pos=[-1],\n",
    "        )\n",
    "        # print(\"Layer, Head:\", layer_idx, head_idx)\n",
    "        # print(logits_to_pred(mod_logits, tokenizer, k=5))\n",
    "        # print(f\"JSD(a <- b, b) = {JSD(mod_logits, logits_b, pos=-1)}\")\n",
    "        js_divs[layer_idx, head_idx] = JSD(mod_logits, logits_b, pos=-1)\n",
    "\n",
    "# make the color map range start from 0\n",
    "imshow(js_divs, xaxis=\"head\", yaxis=\"layer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_over_repeat(end_or_list, repeat=1):\n",
    "    \n",
    "    if isinstance(end_or_list, int):\n",
    "        end_or_list = range(end_or_list)\n",
    "    l = []\n",
    "    for i in end_or_list:\n",
    "        l += [i] * repeat\n",
    "    return l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
