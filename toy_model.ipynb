{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import codebook_features\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from transformers import GPTNeoXConfig, GPTNeoXForCausalLM, GPT2TokenizerFast\n",
    "from torch.utils.data import IterableDataset\n",
    "from codebook_features import models, run_clm, trainer as cb_trainer\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyGraph():\n",
    "    def __init__(self, N=100, transition_matrix=None, seed=None, edges=10):\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        if transition_matrix is None:\n",
    "            self.transition_matrix = np.zeros((N, N))\n",
    "            for i in range(N):\n",
    "                self.transition_matrix[i, self.rng.choice(N, size=edges, replace=False)] = 1\n",
    "            self.transition_matrix = self.transition_matrix / self.transition_matrix.sum(axis=1, keepdims=True)\n",
    "            self.N = N\n",
    "        else:\n",
    "            self.transition_matrix = transition_matrix\n",
    "            self.N = self.transition_matrix.shape[0]\n",
    "        assert self.transition_matrix.shape == (N, N)\n",
    "\n",
    "        self.state = 0\n",
    "        self.digits = int(np.ceil(np.log10(N)))\n",
    "\n",
    "    def step(self):\n",
    "        self.state = self.rng.choice(self.N, p=self.transition_matrix[self.state])\n",
    "        return self.state\n",
    "    \n",
    "    def step_with(self, state):\n",
    "        return self.rng.choice(self.N, p=self.transition_matrix[state])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def save(self, path):\n",
    "        np.save(path, self.transition_matrix)\n",
    "\n",
    "    def generate_trajectory(self, length):\n",
    "        trajectory = [self.rng.choice(self.N)]\n",
    "        for _ in range(length-1):\n",
    "            trajectory.append(self.step_with(trajectory[-1]))\n",
    "        return trajectory\n",
    "    \n",
    "    def generate_trajectories(self, length, start_states=None):\n",
    "        if start_states is None:\n",
    "            curr_states = np.array(self.state * length)\n",
    "        else:\n",
    "            curr_states = copy.deepcopy(start_states)\n",
    "        trajectories = np.zeros((len(start_states), length))\n",
    "        for i in range(length):\n",
    "            for j in range(len(start_states)):\n",
    "                curr_states[j] = self.step_with(curr_states[j])\n",
    "                trajectories[j, i] = curr_states[j]\n",
    "        return trajectories\n",
    "\n",
    "    def verify_trajectory(self, traj):\n",
    "        for i in range(len(traj)-1):\n",
    "            if self.transition_matrix[traj[i], traj[i+1]] == 0:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(IterableDataset):\n",
    "    def __init__(self, graph, seq_len):\n",
    "        self.graph = graph\n",
    "        self.seq_len = seq_len\n",
    "        assert self.seq_len % self.graph.digits == 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            self.graph.set_seed(worker_info.seed)\n",
    "        while True:\n",
    "            yield self.tokenize(self.graph.generate_trajectory(self.seq_len//self.graph.digits))\n",
    "\n",
    "    def tokenize(self, traj):\n",
    "        inp_str = \"\".join([str(x) if x > 9 else '0'*(self.graph.digits-1)+str(x) for x in traj])\n",
    "        inp_dict = {k: v.reshape(-1) for k, v in tokenizer(inp_str, return_tensors=\"pt\").items()}\n",
    "        inp_dict[\"labels\"] = inp_dict[\"input_ids\"].clone()\n",
    "        return inp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"toy\"):\n",
    "    os.makedirs(\"toy\")\n",
    "\n",
    "vocab = '{\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"<|endoftext|>\": 10}'\n",
    "with open(\"toy/vocab.json\", \"w\") as f:\n",
    "    f.write(vocab)\n",
    "\n",
    "with open(\"toy/merges.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast(vocab_file=\"toy/vocab.json\", merges_file=\"toy/merges.txt\", pad_token=\"<|endoftext|>\")\n",
    "tokenizer.save_pretrained(\"toy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "automata = ToyGraph(N=100, edges=2)\n",
    "train_dataset = ToyDataset(automata, seq_len=512)\n",
    "eval_dataset = ToyDataset(automata, seq_len=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTNeoXConfig(vocab_size=11, hidden_size=8, num_hidden_layers=2, num_attention_heads=2, intermediate_size=32, rotary_emb_base=10000, bos_token_id=10, eos_token_id=10, max_position_embeddings=512)\n",
    "model = GPTNeoXForCausalLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = run_clm.TrainingArguments(\n",
    "    output_dir=\"toy/output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=0.001,\n",
    "    max_steps=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.0,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=10,\n",
    "    overwrite_output_dir=True,\n",
    "    seed=42,\n",
    "    train_model_params=True,\n",
    "    model_lr_factor=1.0,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "model_args = run_clm.ModelArguments(model_name_or_path=\"toy/model\")\n",
    "data_args = run_clm.DataTrainingArguments(dataset_name=\"toy_graph\", max_eval_samples=10)\n",
    "\n",
    "optimizers = (None, None)\n",
    "if isinstance(model, models.CodebookModel):\n",
    "    if training_args.train_model_params:\n",
    "        params = [\n",
    "            {\n",
    "                \"params\": model.get_codebook_params(),\n",
    "                \"lr\": training_args.learning_rate,\n",
    "                # weight decay for codebook params is used through\n",
    "                # `codebook_weight_decay` param that is used directly\n",
    "                # to compute regularized loss.\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": model.get_model_params(),\n",
    "                \"lr\": training_args.model_lr_factor * training_args.learning_rate,\n",
    "                \"weight_decay\": training_args.weight_decay,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        params = model.get_codebook_params()\n",
    "    if len(params) > 0:\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            params,\n",
    "            training_args.learning_rate,\n",
    "        )\n",
    "        optimizers = (optimizer, None)\n",
    "\n",
    "\n",
    "callbacks = [cb_trainer.WandbCallback()]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "trainer = cb_trainer.CodebookTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    optimizers=optimizers,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "lm_datasets = {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "metrics = run_clm.run_trainer(\n",
    "    model_args, data_args, training_args, trainer, lm_datasets, last_checkpoint=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
