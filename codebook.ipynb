{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea1b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebook_features import models\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, pipeline, set_seed\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    ")\n",
    "orig_model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "model = models.wrap_codebook(model_or_path=\"EleutherAI/pythia-70m-deduped\", pretrained_path=\"taufeeque/best-cb-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325c5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = \"2023-02-17_09-27-39/\"\n",
    "base_dir = \"2023-03-02_21-17-36/\"\n",
    "\n",
    "tokens = np.load(base_dir + \"tokens.npy\", allow_pickle=True)\n",
    "# cb_acts_complete = np.load(\"2023-02-17_09-27-39/cb_acts.npy\", allow_pickle=True).item()\n",
    "with open(base_dir + \"cb_acts.pkl\",\"rb\") as f:\n",
    "    cb_acts_complete = pickle.load(f)\n",
    "metrics = np.load(base_dir + \"metrics.npy\", allow_pickle=True)\n",
    "num_codes = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cebb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = np.load(\"2023-02-09_21-37-58/tokens.npy\", allow_pickle=True)\n",
    "# cb_acts_complete = np.load(\"2023-02-09_21-37-58/cb_acts.npy\", allow_pickle=True).item()\n",
    "# metrics = np.load(\"2023-02-09_21-37-58/metrics.npy\", allow_pickle=True)\n",
    "# num_codes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f4a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10_000\n",
    "tokens_trunc = tokens[:samples]\n",
    "cb_acts = {}\n",
    "for k, v in cb_acts_complete.items():\n",
    "    cb_acts[k] = v[:samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d44fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4699559211730957, 'eval_accuracy': 0.3778763440860215, 'eval_runtime': 289.5455, 'eval_samples_per_second': 34.537, 'eval_steps_per_second': 1.081, 'eval_dead_code_fraction/layer0': 0.0131, 'eval_MSE/layer0': 10.428322392272948, 'eval_mean_norm/layer0': 22.481287002563477, 'eval_max_norm/layer0': 27.701583862304688, 'eval_dead_code_fraction/layer1': 0.0859, 'eval_MSE/layer1': 29.021728021240225, 'eval_mean_norm/layer1': 27.37179946899414, 'eval_max_norm/layer1': 38.93690872192383, 'eval_dead_code_fraction/layer2': 0.4581, 'eval_MSE/layer2': 213.85468557128917, 'eval_mean_norm/layer2': 36.73466873168945, 'eval_max_norm/layer2': 65.44490814208984, 'eval_dead_code_fraction/layer3': 0.3518, 'eval_MSE/layer3': 81.8039717895508, 'eval_mean_norm/layer3': 33.063480377197266, 'eval_max_norm/layer3': 59.71989822387695, 'eval_dead_code_fraction/layer4': 0.2956, 'eval_MSE/layer4': 11810.415718750002, 'eval_mean_norm/layer4': 48.984153747558594, 'eval_max_norm/layer4': 74.98965454101562, 'eval_dead_code_fraction/layer5': 0.7027, 'eval_MSE/layer5': 13455.929962500004, 'eval_mean_norm/layer5': 35.9307746887207, 'eval_max_norm/layer5': 85.04580688476562, 'eval_dead_code_fraction': 0.3178666666666667, 'eval_MSE': 4266.909064837393, 'eval_mean_norm': 34.09436066945394, 'eval_max_norm': 85.04580688476562}\n",
      "['layer0_attn_ccb0', 'layer1_attn_ccb0', 'layer2_attn_ccb0', 'layer3_attn_ccb0', 'layer4_attn_ccb0', 'layer5_attn_ccb0']\n",
      "(10000, 1024, 100)\n"
     ]
    }
   ],
   "source": [
    "print(metrics)\n",
    "print(list(cb_acts.keys()))\n",
    "print(cb_acts[\"layer0_attn_ccb0\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = np.unique(cb_acts[\"layer0_attn_ccb1\"], return_counts=True)\n",
    "plt.bar(k, v)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "v_sortind = np.argsort(-v)\n",
    "k, v = k[v_sortind], v[v_sortind]\n",
    "plt.bar(range(len(v)), v)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "862f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features_to_tokens(cb_key,n=10):\n",
    "    codebook_ids = cb_acts[cb_key]\n",
    "    features_tokens = [[] for _ in range(num_codes)]\n",
    "    for i in tqdm(range(codebook_ids.shape[0])):\n",
    "        for j in range(codebook_ids.shape[1]):\n",
    "            for k in range(codebook_ids.shape[2]):\n",
    "                past_future_tuple = (tokens[i, max(0, j-n):j+1], tokens[i, j+1:j+n])\n",
    "                features_tokens[codebook_ids[i,j,k]].append(past_future_tuple)\n",
    "\n",
    "    return features_tokens\n",
    "\n",
    "def features_to_tokens_fast(cb_key,n=10):\n",
    "    codebook_ids = cb_acts[cb_key]\n",
    "    features_tokens = [[] for _ in range(num_codes)]\n",
    "    for i in tqdm(range(codebook_ids.shape[0])):\n",
    "        for j in range(codebook_ids.shape[1]):\n",
    "            for k in range(codebook_ids.shape[2]):\n",
    "                features_tokens[codebook_ids[i,j,k]].append((i,j))\n",
    "\n",
    "    return features_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "373c0b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5234, 5971,  407, 6890])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0, 1020:1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c0a5035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_key = \"layer0_attn_ccb0\"\n",
    "codebook_ids = cb_acts[cb_key]\n",
    "# idxs = codebook_ids[:, :, :, None] == np.arange(1000)[None, None, None, :]\n",
    "codebook_ids.shape\n",
    "codebook_ids.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5f3fe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:27<00:00, 25.80it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_tkns = features_to_tokens_fast(\"layer0_attn_ccb0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19434572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [07:50<00:00, 21.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_tkns2 = features_to_tokens_fast(\"layer2_attn_ccb0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd7c1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:57<00:00, 18.59it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_tkns5 = features_to_tokens_fast(\"layer5_attn_ccb0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c71d5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tkn_print(ll, n=3):\n",
    "    for idx, (i, j) in enumerate(ll):\n",
    "        print(f\"{idx}: example - {i}, token - {j}\")\n",
    "        print(tokenizer.decode(tokens[i,max(0,j-n):j]), end='')\n",
    "        print(colored(tokenizer.decode(tokens[i,j]), 'red'), end='')\n",
    "        print(tokenizer.decode(tokens[i,j+1:j+n]))\n",
    "        if idx > 100:\n",
    "            break\n",
    "    print(\"************************************************************\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9feab12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ft_tkns(ft_tkns, n=3):\n",
    "    for i in range(len(ft_tkns)):\n",
    "#         unique_tokens = set(ft_tkns[i])\n",
    "#         if 0 < len(unique_tokens) < 100:\n",
    "#             print(i, len(unique_tokens), len(ft_tkns[i]))\n",
    "#             tkn_print(unique_tokens)\n",
    "#             print(\"**********************************\")\n",
    "        tkns = ft_tkns[i]\n",
    "        if len(tkns) > 0 and len(tkns) > 1000:\n",
    "            print(f\"code - {i}, mapped to {len(tkns)} tokens\")\n",
    "            tkn_print(tkns, n)\n",
    "        if i > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22729877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ft_tkns(ft_tkns,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ft_tkns(ft_tkns2,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0373c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ft_tkns(ft_tkns5,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def token_to_dist_of_fts(token, layer_key):\n",
    "    token_idx = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(token_idx)\n",
    "    layer_cb_acts = cb_acts[layer_key]\n",
    "    token_present = tokens == token_idx\n",
    "    fts_of_token = layer_cb_acts[token_present]\n",
    "    print(fts_of_token.shape)\n",
    "    plt.hist(fts_of_token[:, :3], bins=100, label=[\"1\", \"2\", \"3\"])\n",
    "    plt.legend()\n",
    "    plt.title(f'Features Queried | Layer = {layer_key}')\n",
    "    plt.show()\n",
    "\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer0_attn_ccb0')\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer1_attn_ccb0')\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer2_attn_ccb0')\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer3_attn_ccb0')\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer4_attn_ccb0')\n",
    "token_to_dist_of_fts('ĠAfrica', 'layer5_attn_ccb0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a4a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.wrap_codebook(model_or_path=\"EleutherAI/pythia-70m-deduped\", pretrained_path=\"taufeeque/pythia-attn-cb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7e908b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"Hello I am\", \"What is the\", \"The largest\", \"One of the most\", \"Your time is limited\", \"Keep your friends close, but\", \"Once upon a time,\"]\n",
    "# prefixes = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3e702346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am a non-member of a team in the USA.\n",
      "\n",
      "My question is: How do I make a roster in one team?\n",
      "\n",
      "I am not a member of the Team in the USA. I am in the USA and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am a newbie with a background in UI.\n",
      "<xmpp> i'm using a simple text editor but i don't use the web page when it's a text editor\n",
      "<xmpp> and i've already found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am getting this strange behavior. I am using the below code:\n",
      "    <script src=\"https://code.jquery.com/jquery-2.1.1.min.js\"></script>\n",
      "\n",
      "<script>\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the first half hour of the second hour? What is the third hour of the fourth hour? A second half hour of the fifth hour is the sixth hour of the fifth hour.\n",
      "\n",
      "With the last hour of the seventh, the first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the remainder when (4 - 2)*-5*2 is divided by 2?\n",
      "4\n",
      "Let u = -11 + 17. Let w = u - -10. Calculate the remainder when 10 is divided by w.\n",
      "2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the common denominator of 101/14 and (3/7)/((-8)/6)?\n",
      "5\n",
      "Let p = -7 + 12. Suppose 0*o = -p*o + 12. What is the least common multiple of o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest company of the U.S. is a US firm based in Beijing, also known as Beijing’s Tiananmen Square, which was built in the 1950s.\n",
      "\n",
      "China has also been the world’s leading supplier of Chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest in the country, with around $7 billion at the time, was one of the largest in its history.\n",
      "\n",
      "The biggest is the first of the two major cities on the island, which include Soho, the city of Gis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest group of people in the world is the United States of America, and it's the largest group of American people in the world.\n",
      "\n",
      "A few years ago, I had the opportunity to write a book about this story and I was struck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most important questions to ask when deciding a particular product will be to determine whether a particular product is a particular product, or whether it is a particular product.\n",
      "\n",
      "Once a product is a particular product, it should be determined how the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most popular\n",
      "-\n",
      "\n",
      "Troy, I have been thinking of a new product called \"Baked Soy\". It is called \"Soy\" because it is the most popular, and most popular, brand of, and one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most exciting and the most innovative of the new generation of online retailers, the CME has been a tremendous success.\n",
      "\n",
      "The CME has been a huge success, and I have been proud to be a part of its new and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited to just one hour before the event.\"\n",
      "\n",
      "\"Yeah, it's just one hour before the event, right?\"\n",
      "\n",
      "\"I'll have it covered.\"\n",
      "\n",
      "\"Yeah, I guess so. You were right.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited to $7.10. If the time is limited to $7.10, the person with the most recent payment will be in no position to take any kind of payment. The person with the most recent payment will be in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited.\n",
      "\n",
      "The reason I am doing the post is because I am not sure why this is happening. I am just explaining my position, but I am confused.\n",
      "\n",
      "I have a comment and something to do is this,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but you are welcome to share yours with others.\n",
      "\n",
      "We want to make sure that you are on the right track when you’re coming, and that your friends are staying safe as far as possible. You can also\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but they will always be there for you.\"\n",
      "\n",
      "You do not have to lie. Remember to share with others.\n",
      "\n",
      "When you are not alone, you will be with them.\n",
      "\n",
      "I am an adult.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but that's what you make of him.\"\n",
      "\n",
      "\"How do you know that?\"\n",
      "\n",
      "\"I never knew you'd be here,\" she said. \"I never expected you to tell me what you do, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, my daughter used to be a very active participant in social distancing.\n",
      "\n",
      "I had a friend who was in a group together who knew his wife, and he was on the run for a bit for it. She was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the same is true of the former, at the time of the loss of the former and the present, of the latter.\n",
      "\n",
      "The problem is with the loss of the former. For example, the loss of the former\n",
      "Once upon a time, I just found that the\n",
      "washers were not so simple as to be able to handle the entire\n",
      "work, and it was not the time for me to simply do the same. I\n",
      "had a few different questions about\n"
     ]
    }
   ],
   "source": [
    "orig_generator = pipeline('text-generation', model=orig_model, tokenizer=tokenizer)\n",
    "for prefix in prefixes:\n",
    "    for i in range(3):\n",
    "        print(orig_generator(prefix, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7e213410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am a child to be a person of the family. \" \n",
      " = = = Reception = = = \n",
      " \" The episode of the episode \" is a guest @-@ cast @-@ action thriller, which includes guest @-@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I amai's wife \". The first female of the American writer of the American columnist, and was the authorial, having been inspired by his earlier and @-@ fiction that his \" character was \" more popular in the United States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I amassi, who are involved in his father's family. He also made a post @-@ year in his law of the law of Parliament of the United States. \n",
      " = = = Aftermath = = = \n",
      " In the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the only \" an example of the <unk> \" of the same one of the main theme \". \n",
      " The \" the \" most remarkable of the original work in the poem of an ancient church and one of the most important examples of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the \" God gods @-@ the \" [ <unk> ] ]. \" \n",
      " = = = = Formation = = = = \n",
      " = = = = = = = \n",
      " = = = = = = = = = \n",
      " \" I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the three @-@-@ four @-@ game game game, and their best game games. \n",
      " = = = Gameplay = = = \n",
      " = = = Gameplay = = = \n",
      " Gameplay is a first game for the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest of the United States. The organization of the university is a private student in the 1980s, and the school has also the most successful school in the area. The first school on both the National University of State University of State University was the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest of the United States of the World of the United States. The state center of the United States, U.S. and other regions, the area of U.S. lies the state of the United States, <unk>, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest modern world. The name of the Rata is probably more than a form of an unconnected world with the city's \" s \". \n",
      " = = = = Background = = = = \n",
      " The first name in the \" Kingdom of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most of the worst @-@ ranking year @-@ ranking and for his final campaign in the same time. \n",
      " At the beginning of the events of the season of the summer, the race is an \" a \" one @-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most American. \n",
      " = = = Reception = = = \n",
      " The song was written in a live video in 1999, and has been produced in the UK, The Independent of the Year, as well as the third song for the second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most notable figures of the works of science fiction, and the novel, the two of the novel, was the first novel, written by the <unk>'s use of the characters of his work in the series of the film that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited and replaced with a large number of seats to the individual @-@ half all of the same. \n",
      " = = = World War I = = = \n",
      " The First World War I was the first time and first day. Despite the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited to the remaining. However of which the majority of the way to be in a point of the nature of the region are an area of the center of the area, while the population has a number of the most important areas of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited to a large number of the population. \n",
      " = = = = = \n",
      " The population is a very low population @-@ 5 in the north of the Great Britain and the South Australian border in the region. The population is located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but he had had been the same and asked about to be a woman. \n",
      " In fact, the actor was also a fan of the character and the director of the character. In the series of the series, a series of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but, he, he and his father's relationship. \n",
      " = = = = = = \n",
      " = = = = = = = = = = \n",
      " In his tenure, he was described as the first @-@ year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your friends close, but to which their families were to have the house. \n",
      " = = = Other things that are used for the Lord of Lord of the Earl of England and his father of William H. HMS, they had been at the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the player is having a career ; \" the game is a good and the guy. \" \n",
      " The game was released in the United States in the World War I, who gave his first role as the first @-@ game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time,, the fate of this was so many of the world's efforts. \n",
      " = = = Geography of the world = = = = \n",
      " The Great East, the first of the Northern United Nations and the World War (\n",
      "Once upon a time,, these were not seen in \" the episode \", but the episode was originally aired on The MHS and the Fox Show. The film premiered on the Fox Show at the Fox Show. The film was written by the film\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model=model.model, tokenizer=tokenizer)\n",
    "for prefix in prefixes:\n",
    "    for i in range(3):\n",
    "        print(generator(prefix, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400c4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at taufeeque/pythia-100mc were not used when initializing GPTNeoXCodebookModel: ['model.gpt_neox.layers.2.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.1.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.5.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.0.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.3.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.4.attention.codebook_layer.codebook.0.counts']\n",
      "- This IS expected if you are initializing GPTNeoXCodebookModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXCodebookModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "best_cb_model = models.wrap_codebook(model_or_path=\"EleutherAI/pythia-70m-deduped\", pretrained_path=\"taufeeque/pythia-100mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a5fdcfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prefixes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[1;32m      3\u001b[0m best_cb_generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mbest_cb_model\u001b[38;5;241m.\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprefixes\u001b[49m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(best_cb_generator(prefix, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prefixes' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "best_cb_generator = pipeline('text-generation', model=best_cb_model.model, tokenizer=tokenizer)\n",
    "for prefix in prefixes:\n",
    "    for i in range(3):\n",
    "        print(best_cb_generator(prefix, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c997ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GPTNeoXTokenizerFast in module transformers.models.gpt_neox.tokenization_gpt_neox_fast object:\n",
      "\n",
      "class GPTNeoXTokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
      " |  GPTNeoXTokenizerFast(vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |  \n",
      " |  Construct a \"fast\" GPT-NeoX-20B tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n",
      " |  Byte-Pair-Encoding.\n",
      " |  \n",
      " |  This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
      " |  be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
      " |  \n",
      " |  ```\n",
      " |  >>> from transformers import GPTNeoXTokenizerFast\n",
      " |  >>> tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"gpt2\")\n",
      " |  >>> tokenizer(\"Hello world\")['input_ids']\n",
      " |  [15496, 995]\n",
      " |  >>> tokenizer(\" Hello world\")['input_ids']\n",
      " |  [18435, 995]\n",
      " |  ```\n",
      " |  \n",
      " |  You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n",
      " |  the model was not pretrained this way, it might yield a decrease in performance.\n",
      " |  \n",
      " |  <Tip>\n",
      " |  \n",
      " |  When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n",
      " |  \n",
      " |  </Tip>\n",
      " |  \n",
      " |  This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      " |  refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (`str`):\n",
      " |          Path to the vocabulary file.\n",
      " |      merges_file (`str`):\n",
      " |          Path to the merges file.\n",
      " |      errors (`str`, *optional*, defaults to `\"replace\"`):\n",
      " |          Paradigm to follow when decoding bytes to UTF-8. See\n",
      " |          [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
      " |      unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
      " |          The beginning of sequence token.\n",
      " |      eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
      " |          The end of sequence token.\n",
      " |      add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
      " |          other word. (GPTNeoX tokenizer detect beginning of words by the preceding space).\n",
      " |      trim_offsets (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not the post-processing step should trim offsets to avoid including whitespaces.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GPTNeoXTokenizerFast\n",
      " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Union[str, NoneType] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'gpt-neox-20b': 2048}\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'attention_mask']\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'tokenizer_file': {'EleutherAI/gpt-neox-...\n",
      " |  \n",
      " |  vocab_files_names = {'merges_file': 'merges.txt', 'tokenizer_file': 't...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`int` or `List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or `List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int` or `List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
      " |      often want to remove sub-word tokenization artifacts at the same time.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`List[str]`): The token to join in a string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
      " |      vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
      " |      this inside your training loop.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Union[int, NoneType])\n",
      " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
      " |      library) and restore the tokenizer settings afterwards.\n",
      " |      \n",
      " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
      " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
      " |      section.\n",
      " |      \n",
      " |      Args:\n",
      " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
      " |              The kind of padding that will be applied to the input\n",
      " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
      " |              The kind of truncation that will be applied to the input\n",
      " |          max_length (`int`):\n",
      " |              The maximum size of a sequence.\n",
      " |          stride (`int`):\n",
      " |              The stride to use when handling overflow.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |  \n",
      " |  tokenize(self, text: str, pair: Union[str, NoneType] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          pair (`str`, *optional*):\n",
      " |              A second sequence to be encoded with the first.\n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add the special tokens associated with the corresponding model.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific encode method. See details in\n",
      " |              [`~PreTrainedTokenizerBase.__call__`]\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of tokens.\n",
      " |  \n",
      " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
      " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
      " |      as the current one.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_iterator (generator of `List[str]`):\n",
      " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
      " |              if you have everything in memory.\n",
      " |          vocab_size (`int`):\n",
      " |              The size of the vocabulary you want for your tokenizer.\n",
      " |          length (`int`, *optional*):\n",
      " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
      " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
      " |              A list of new special tokens to add to the tokenizer you are training.\n",
      " |          special_tokens_map (`Dict[str, str]`, *optional*):\n",
      " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
      " |              token name to new special token name in this argument.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
      " |          `text_iterator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  backend_tokenizer\n",
      " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
      " |  \n",
      " |  decoder\n",
      " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  vocab_size\n",
      " |      `int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __annotations__ = {'can_save_slow_tokenizer': <class 'bool'>, 'slow_to...\n",
      " |  \n",
      " |  can_save_slow_tokenizer = True\n",
      " |  \n",
      " |  slow_tokenizer_class = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in `encode_plus`).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens.\n",
      " |      \n",
      " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The model input with special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed. [What are token type\n",
      " |      IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |      Should be overridden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The token type ids.\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
      " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
      " |      \n",
      " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
      " |      text followed by a call to the `pad` method to get a padded encoding.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
      " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
      " |              tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
      " |              List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
      " |              collate function.\n",
      " |      \n",
      " |              Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
      " |              the note above for the return type.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
      " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (`list`, *optional*):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
      " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
      " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (`int`, *optional*):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to `None`, this will use the max_length value.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to `self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Union[bool, NoneType] = None, commit_message: Union[str, NoneType] = None, private: Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '10GB', create_pr: bool = False, **deprecated_kwargs) -> str\n",
      " |      Upload the tokenizer files to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
      " |      `repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          use_auth_token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoTokenizer\n",
      " |      \n",
      " |      tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Union[bool, NoneType] = None, filename_prefix: Union[str, NoneType] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
      " |      \n",
      " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
      " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (`bool`, *optional*):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
      " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
      " |              loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
      " |              error is raised.\n",
      " |          filename_prefix: (`str`, *optional*):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of `str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
      " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
      " |                batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
      " |                than the model maximum admissible input size).\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
      " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
      " |          of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
      " |      tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      " |                user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
      " |                `./my_model_directory/`.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                `./my_model_directory/vocab.txt`.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__` method.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
      " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
      " |      # Download vocabulary from huggingface.co and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
      " |      \n",
      " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
      " |      \n",
      " |      # You can link tokens to special vocabulary when instantiating\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n",
      " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |      assert tokenizer.unk_token == \"<unk>\"\n",
      " |      ```\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoTokenizer') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
      " |      library are already mapped with `AutoTokenizer`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
      " |              The auto class to register this new tokenizer with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  pretrained_init_configuration = {}\n",
      " |  \n",
      " |  truncation_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]], replace_additional_special_tokens=True) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      Note,None When adding new tokens to the vocabulary, you should make sure to also resize the token embedding\n",
      " |      matrix of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
      " |      `'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
      " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the `unk_token` to them).\n",
      " |          replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
      " |              If `True`, the existing list of additional special tokens will be replaced by the one specified in\n",
      " |              `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is updated. In the former case, the\n",
      " |              tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged as\n",
      " |              non-special tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to add a new classification token to GPT-2\n",
      " |      tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      " |      model = GPT2Model.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |      assert tokenizer.cls_token == \"<CLS>\"\n",
      " |      ```\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
      " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
      " |      not treated in the same way.\n",
      " |      \n",
      " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
      " |      of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
      " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
      " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
      " |              strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |      tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
      " |      model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      ```\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (`tokenizer.mask_token`,\n",
      " |      `tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          `int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      `int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
      " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
      " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been\n",
      " |      set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      `str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not\n",
      " |      been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full\n",
      " |      depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence\n",
      " |      leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      `str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      `str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      `str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0705fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at taufeeque/pythia-100mc were not used when initializing GPTNeoXCodebookModel: ['model.gpt_neox.layers.3.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.5.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.2.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.1.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.4.attention.codebook_layer.codebook.0.counts', 'model.gpt_neox.layers.0.attention.codebook_layer.codebook.0.counts']\n",
      "- This IS expected if you are initializing GPTNeoXCodebookModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXCodebookModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from codebook_features import models\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    ")\n",
    "orig_model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "model = models.wrap_codebook(model_or_path=\"EleutherAI/pythia-70m-deduped\", pretrained_path=\"taufeeque/pythia-100mc\")\n",
    "\n",
    "vis = {}\n",
    "\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "    vis[m] = (i, o)\n",
    "\n",
    "vis_attn = {}\n",
    "    \n",
    "def hook_fn_attn(m, i, o):\n",
    "    vis_attn[m] = (i, o)\n",
    "\n",
    "    \n",
    "for name, layer in model.model.gpt_neox.layers._modules.items():\n",
    "    print(name)\n",
    "    layer.attention.codebook_layer.register_forward_hook(hook_fn)\n",
    "    layer.attention.register_forward_hook(hook_fn_attn)\n",
    "\n",
    "\n",
    "s = \"This tokenizer has been trained to treat spaces like parts of the tokens random tokens added here\"\n",
    "inp = tokenizer(s, return_tensors=\"pt\")\n",
    "out = model(**inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d2a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.75636291503906\n",
      "cb_norm 22.606342315673828\n",
      "150.0145721435547\n",
      "cb_norm 22.624954223632812\n",
      "953.9581298828125\n",
      "cb_norm 22.625259399414062\n",
      "567.1958618164062\n",
      "cb_norm 22.624855041503906\n",
      "977.8589477539062\n",
      "cb_norm 22.627485275268555\n",
      "3250.580322265625\n",
      "cb_norm 22.615549087524414\n"
     ]
    }
   ],
   "source": [
    "for i, layers in model.all_codebooks.items():\n",
    "    for cb in layers:\n",
    "        print(cb.reconstruction_mse)\n",
    "        print('cb_norm',cb.codebook[0].avg_norm())\n",
    "#         print(cb.codebook[0].input.norm())\n",
    "#         print(cb.codebook[0].output.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc9ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_codebook_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dcc1946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'codebook_features.models' from '/home/taufeeque/codebook-features/codebook_features/models.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "24.59918189048767\n",
    "58.6542329788208\n",
    "1281.2109985351562\n",
    "1219.846923828125\n",
    "2715.999969482422\n",
    "8037.982604980469\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b333f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_cb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57002c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "dict_keys([CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      "), CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      "), CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      "), CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      "), CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      "), CompositionalCodebookLayer(\n",
      "  (codebook): ModuleList(\n",
      "    (0): CodebookLayer(\n",
      "      (codebook): Embedding(10000, 512)\n",
      "    )\n",
      "  )\n",
      ")])\n"
     ]
    }
   ],
   "source": [
    "print(len(vis))\n",
    "print(vis.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fda2d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.211728572845459 2.2341723442077637\n",
      "44.75636291503906\n",
      "13.70687484741211 2.742445230484009\n",
      "150.0145721435547\n",
      "32.99772262573242 3.2736246585845947\n",
      "953.9581298828125\n",
      "25.82971954345703 3.1037824153900146\n",
      "567.1959228515625\n",
      "33.30326843261719 3.3043413162231445\n",
      "977.8590698242188\n",
      "58.66970443725586 3.357837438583374\n",
      "3250.580322265625\n"
     ]
    }
   ],
   "source": [
    "for module, (i, o) in vis.items():\n",
    "    i, o = i[0], o\n",
    "    print(i.norm(p=2,dim=-1).mean().item(), o.norm(p=2,dim=-1).mean().item())\n",
    "    print((((i-o)).norm(p=2,dim=-1)**2).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f65fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.211728572845459\n",
      "11.842400550842285\n",
      "29.040924072265625\n",
      "22.245229721069336\n",
      "13.874805450439453\n",
      "21.93659019470215\n"
     ]
    }
   ],
   "source": [
    "# model.disable_codebooks()\n",
    "model.reset_codebook_metrics()\n",
    "vis_attn = {}\n",
    "out = model(**inp)\n",
    "\n",
    "for module, (i, o) in vis_attn.items():\n",
    "    i, o = i[0], o[0]\n",
    "    print(o.norm(p=2,dim=-1).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da3b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.all_codebooks\n",
    "# wrapper = list(vis_attn.keys())[0]\n",
    "# print(wrapper.snap)\n",
    "for layer in vis_attn:\n",
    "    layer.snap = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f83c2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(t):\n",
    "    return t.norm(p=2,dim=-1).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "add8ab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.357837438583374\n"
     ]
    }
   ],
   "source": [
    "last_cb = model.all_codebooks[5][0].codebook[0]\n",
    "\n",
    "last_o = last_cb(i)\n",
    "print(get_norm(last_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39aacacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "logits = -torch.cdist(i, last_cb.codebook.weight, p=2)\n",
    "_, codebook_ids = logits.topk(models.BaseSnapFunction.k, dim=-1)\n",
    "outputs = torch.nn.functional.embedding(codebook_ids, last_cb.codebook.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d4c116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.306095123291016\n",
      "3.357837438583374\n"
     ]
    }
   ],
   "source": [
    "print(get_norm(outputs))\n",
    "outputs_avg = outputs.sum(dim=-2) / models.BaseSnapFunction.k\n",
    "print(get_norm(outputs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eda71a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 100, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7644e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7574ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "orig_vis = {}\n",
    "\n",
    "\n",
    "def orig_hook_fn(m, i, o):\n",
    "    orig_vis[m] = (i, o)\n",
    "\n",
    "for name, layer in orig_model.gpt_neox.layers._modules.items():\n",
    "    print(name)\n",
    "    layer.attention.register_forward_hook(orig_hook_fn)\n",
    "\n",
    "orig_out = orig_model(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68d6cd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2817928791046143\n",
      "3.281010866165161\n",
      "4.416781425476074\n",
      "3.543278455734253\n",
      "2.703380584716797\n",
      "14.310317993164062\n"
     ]
    }
   ],
   "source": [
    "for module, (i, o) in orig_vis.items():\n",
    "    i, o = i[0], o[0]\n",
    "    print(o.norm(p=2,dim=-1).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13a12326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_vis.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7113cf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[104.0447,  84.4274, 139.5462,  ...,  84.4247,  84.4243,  84.4238],\n",
       "         [104.6174,  84.4767, 139.2081,  ...,  84.4752,  84.4737,  84.4747],\n",
       "         [108.2611,  88.2957, 144.4648,  ...,  88.2942,  88.2946,  88.2932],\n",
       "         ...,\n",
       "         [109.2437,  89.6794, 144.8795,  ...,  89.6781,  89.6777,  89.6771],\n",
       "         [106.6640,  87.3825, 146.6189,  ...,  87.3811,  87.3813,  87.3805],\n",
       "         [107.4439,  87.2109, 147.9247,  ...,  87.2099,  87.2096,  87.2092]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-1.5175e-01, -2.1268e-01,  5.9498e-01,  ..., -1.4740e+00,\n",
       "           -2.6675e+00,  8.3408e-01],\n",
       "          [-6.1415e-01, -1.0304e+00,  5.5834e-01,  ..., -2.5103e+00,\n",
       "           -2.3254e+00,  2.7814e+00],\n",
       "          [-3.0116e-01, -7.6867e-01,  2.0433e-01,  ..., -1.4666e+00,\n",
       "           -1.4205e+00,  1.8040e+00],\n",
       "          ...,\n",
       "          [ 6.0656e-01,  5.9117e-01, -3.4602e-01,  ..., -3.5330e+00,\n",
       "           -2.3229e+00,  3.0498e+00],\n",
       "          [ 7.1132e-01, -1.4541e-01, -1.2642e+00,  ..., -3.1769e+00,\n",
       "           -2.5721e+00,  1.9821e+00],\n",
       "          [ 4.1548e-01,  1.1575e-01, -2.8445e-01,  ..., -2.3559e+00,\n",
       "           -1.1094e+00,  2.4356e-01]],\n",
       "\n",
       "         [[-3.9294e+00,  3.1752e+00,  4.1115e+00,  ..., -1.3114e+00,\n",
       "           -1.1307e+00,  1.1907e+01],\n",
       "          [-3.1231e+00,  1.8147e+00,  3.9871e+00,  ..., -1.4745e+00,\n",
       "           -1.3609e+00,  1.2744e+01],\n",
       "          [ 1.1506e+00,  1.4911e-01,  4.0621e+00,  ...,  2.5014e-01,\n",
       "           -2.0715e+00,  1.2245e+01],\n",
       "          ...,\n",
       "          [ 1.7551e+00,  2.8123e+00,  3.9844e+00,  ..., -2.3877e+00,\n",
       "           -2.2423e+00,  1.2856e+01],\n",
       "          [ 3.8895e+00,  3.4745e+00,  3.0699e+00,  ..., -2.1324e+00,\n",
       "           -2.9422e+00,  1.2602e+01],\n",
       "          [ 2.1456e+00,  3.9589e+00,  1.6891e+00,  ..., -2.1966e+00,\n",
       "           -2.4111e+00,  1.3038e+01]],\n",
       "\n",
       "         [[-9.6345e-02, -1.1535e+00, -1.6575e+00,  ...,  5.0537e+00,\n",
       "            3.6064e+00,  5.0870e-01],\n",
       "          [-8.6538e-01,  1.6685e+00, -4.0299e+00,  ...,  5.2668e+00,\n",
       "            4.6566e+00,  2.4337e-01],\n",
       "          [ 1.0757e+00,  2.0163e+00, -2.4895e+00,  ...,  5.3361e+00,\n",
       "            1.5790e-01, -2.0488e-01],\n",
       "          ...,\n",
       "          [ 1.4096e+00, -2.0454e+00, -7.2585e-01,  ...,  4.9778e+00,\n",
       "            5.1216e+00,  3.1299e-01],\n",
       "          [ 9.3014e-01, -5.5316e-01,  5.9426e-01,  ...,  5.4425e+00,\n",
       "            3.5487e+00, -6.3454e-01],\n",
       "          [ 5.5454e-01, -2.0208e+00, -1.5951e+00,  ...,  5.8770e+00,\n",
       "            4.1847e+00, -2.8057e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.6639e+00, -4.0324e+00, -4.7812e+00,  ..., -2.0317e+00,\n",
       "           -7.9595e-01, -1.8857e+00],\n",
       "          [ 2.3928e-02, -3.9796e+00, -2.4636e+00,  ...,  8.4612e-02,\n",
       "           -3.4960e+00, -3.6631e+00],\n",
       "          [-4.9848e+00, -4.6437e+00, -3.1196e+00,  ..., -2.0522e+00,\n",
       "           -5.4541e-01, -4.3359e+00],\n",
       "          ...,\n",
       "          [-5.9771e+00,  2.1009e+00,  1.7783e+00,  ..., -2.5797e-01,\n",
       "           -4.2216e+00, -3.8158e+00],\n",
       "          [-3.8250e+00,  9.4445e-01,  3.3144e+00,  ..., -4.1200e+00,\n",
       "           -3.7045e+00, -2.6181e+00],\n",
       "          [ 3.4873e+00, -1.8129e+00,  3.2874e+00,  ..., -6.8884e+00,\n",
       "           -3.5950e+00, -4.2359e+00]],\n",
       "\n",
       "         [[ 2.9483e-01, -5.0458e-03,  5.4227e-01,  ...,  1.7523e+00,\n",
       "           -4.4028e+00, -6.1745e+00],\n",
       "          [-3.4544e-02, -2.6302e-02,  2.3138e-01,  ...,  5.5833e-01,\n",
       "           -5.3378e+00, -5.8064e+00],\n",
       "          [-1.0268e+00, -1.6616e+00,  1.5623e+00,  ...,  2.1029e+00,\n",
       "           -4.5282e+00, -5.0408e+00],\n",
       "          ...,\n",
       "          [ 6.0791e-02, -4.9554e-01,  4.4614e-01,  ...,  6.3530e-01,\n",
       "           -4.6476e+00, -6.4100e+00],\n",
       "          [-6.5342e-01, -4.7363e-01,  2.3086e-01,  ...,  4.8253e-01,\n",
       "           -4.4427e+00, -6.3630e+00],\n",
       "          [-1.8485e-01, -3.8459e-01,  8.7513e-01,  ...,  1.6818e+00,\n",
       "           -5.1553e+00, -5.9782e+00]],\n",
       "\n",
       "         [[-3.6342e+00, -1.4592e+00, -9.4988e-01,  ..., -2.7573e+00,\n",
       "           -2.3406e+00, -2.7333e+00],\n",
       "          [-1.1698e+00, -3.5349e-02, -3.3619e-01,  ...,  4.0523e-01,\n",
       "            1.2584e+00, -5.3609e+00],\n",
       "          [ 3.5581e+00,  1.1230e+00, -3.1319e-01,  ..., -1.9832e+00,\n",
       "           -8.5528e-01, -3.3800e+00],\n",
       "          ...,\n",
       "          [ 2.2108e+00, -2.5310e+00, -2.9948e+00,  ..., -2.5151e-02,\n",
       "            3.7186e-01, -5.6767e+00],\n",
       "          [ 2.5729e+00, -2.7507e+00, -3.0805e+00,  ..., -1.0754e+00,\n",
       "           -1.1787e+00, -5.1350e+00],\n",
       "          [-5.6255e-01, -2.2852e+00, -1.2242e+00,  ..., -1.8054e+00,\n",
       "           -1.3774e+00, -3.9447e+00]]]], grad_fn=<CatBackward0>), tensor([[[[-1.1928e-01, -3.7145e-01,  5.6151e-01,  ..., -2.6889e-01,\n",
       "           -1.9734e-03, -3.2769e-01],\n",
       "          [ 4.5593e-01, -2.4819e-01, -3.1337e-01,  ...,  1.1892e-01,\n",
       "           -4.9758e-01, -5.6669e-01],\n",
       "          [ 2.7181e-01,  8.0559e-01,  5.4273e-01,  ..., -1.5228e-01,\n",
       "           -8.2212e-01,  3.2146e-01],\n",
       "          ...,\n",
       "          [ 4.6900e-01,  3.6671e-01, -4.6923e-01,  ...,  1.8869e-01,\n",
       "           -8.0648e-01, -3.8638e-01],\n",
       "          [ 3.9288e-01,  1.3792e+00, -7.6941e-01,  ..., -6.5918e-01,\n",
       "           -1.1437e+00,  7.9692e-01],\n",
       "          [ 4.8564e-02,  4.9032e-01,  5.8164e-01,  ..., -2.1569e-02,\n",
       "            3.0239e-01, -5.9341e-01]],\n",
       "\n",
       "         [[-2.6328e-02, -9.5277e-01, -1.2798e-02,  ..., -1.4991e-02,\n",
       "            1.7861e-01,  2.5927e-01],\n",
       "          [ 1.2379e-01,  4.6329e-01, -6.8263e-01,  ..., -6.4356e-01,\n",
       "            1.6540e-01,  5.3304e-01],\n",
       "          [-4.9333e-01,  5.1965e-01,  1.4183e-01,  ..., -3.4547e-01,\n",
       "            1.5462e-01, -7.2818e-01],\n",
       "          ...,\n",
       "          [-2.0714e-01,  2.6546e-01,  2.3207e-01,  ...,  2.0800e-01,\n",
       "            3.1137e-01,  6.9873e-04],\n",
       "          [ 9.4143e-01,  5.6002e-01, -5.3951e-01,  ..., -3.8676e-01,\n",
       "           -3.4960e-01, -1.1802e-01],\n",
       "          [ 1.1717e+00, -5.9885e-01,  4.5155e-01,  ..., -3.3528e-01,\n",
       "            6.6268e-01,  1.4355e-01]],\n",
       "\n",
       "         [[-3.4058e-01,  1.4409e-01,  7.1314e-01,  ..., -9.1670e-02,\n",
       "           -8.8771e-01,  4.9955e-01],\n",
       "          [ 4.3702e-01,  8.7119e-01, -1.8136e-01,  ...,  6.2989e-02,\n",
       "           -2.0616e+00,  2.4852e+00],\n",
       "          [-2.0493e-01,  7.0069e-01, -4.9856e-01,  ...,  7.2728e-01,\n",
       "           -2.2098e+00,  3.0277e+00],\n",
       "          ...,\n",
       "          [ 3.6662e-01,  9.1586e-01, -6.5209e-02,  ..., -3.4162e-01,\n",
       "           -1.4309e+00,  2.0353e+00],\n",
       "          [-4.8293e-01,  2.5793e-01,  1.9550e-01,  ...,  4.4224e-01,\n",
       "           -1.9559e+00,  1.8584e+00],\n",
       "          [-4.8998e-01,  1.4894e-01, -2.3159e-01,  ..., -1.6680e-01,\n",
       "           -1.2999e+00,  1.0016e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.1935e-01, -2.1068e-02,  1.0871e+00,  ..., -2.3348e-01,\n",
       "            8.1403e-01, -2.1167e+00],\n",
       "          [-1.0665e-01, -1.0121e-01,  1.7492e-02,  ..., -7.7050e-02,\n",
       "            2.0180e-01, -3.1494e-02],\n",
       "          [-4.3145e-01,  9.4959e-01, -7.9563e-01,  ..., -7.0615e-01,\n",
       "           -1.0596e-02,  6.2361e-01],\n",
       "          ...,\n",
       "          [-4.1426e-01,  6.1266e-02,  6.3860e-01,  ..., -5.7245e-01,\n",
       "            3.5665e-01, -2.4674e-01],\n",
       "          [ 1.2396e-01, -1.2535e+00,  3.9056e-01,  ...,  1.1839e-01,\n",
       "           -4.8510e-01,  8.4219e-02],\n",
       "          [ 3.8380e-01, -1.2422e-01, -4.5048e-02,  ..., -8.9946e-02,\n",
       "           -1.6553e-03, -5.8133e-01]],\n",
       "\n",
       "         [[-4.1814e-01, -6.7664e-01,  4.2014e-01,  ..., -2.7517e-01,\n",
       "           -4.7881e-01, -2.5990e-01],\n",
       "          [-2.9612e-01, -1.9951e-02, -1.5883e-01,  ...,  6.8256e-01,\n",
       "            7.2470e-02,  1.0342e+00],\n",
       "          [-9.2043e-01, -4.7192e-01,  5.0513e-01,  ...,  1.9085e+00,\n",
       "           -9.0264e-01,  6.3394e-01],\n",
       "          ...,\n",
       "          [-3.1685e-01, -2.1034e-01, -1.4362e-01,  ...,  1.9724e-01,\n",
       "            3.3879e-01,  1.0118e+00],\n",
       "          [-2.0190e-03, -1.3693e+00, -2.2683e-02,  ...,  4.2085e-01,\n",
       "           -3.9194e-02, -1.6434e-02],\n",
       "          [-1.0364e-01, -8.2272e-01, -2.0222e-02,  ...,  4.6957e-01,\n",
       "            4.2222e-01,  2.3425e-01]],\n",
       "\n",
       "         [[ 8.9083e-01,  3.6590e-01, -6.4968e-01,  ..., -5.9272e-01,\n",
       "            4.5632e-01,  5.2349e-01],\n",
       "          [ 4.7518e-02,  1.4735e-01,  7.8351e-01,  ...,  7.5343e-02,\n",
       "           -1.6151e-01,  8.1548e-02],\n",
       "          [-2.0355e-01,  3.5526e-01,  8.1803e-01,  ..., -4.3219e-01,\n",
       "            1.6261e-01,  6.9342e-02],\n",
       "          ...,\n",
       "          [ 5.9612e-01, -8.9432e-02,  1.0012e+00,  ...,  2.2836e-01,\n",
       "           -9.5930e-01, -3.6507e-01],\n",
       "          [ 7.9082e-01,  4.6743e-01,  3.8169e-01,  ..., -3.8045e-01,\n",
       "            7.4942e-01, -5.2219e-02],\n",
       "          [ 9.8410e-01,  2.2803e-01, -4.4252e-01,  ..., -3.6530e-01,\n",
       "            7.1835e-01,  5.9426e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ -0.5820,   2.7731,   2.1560,  ...,  -3.1902,  -0.3042,  -6.7676],\n",
       "          [  0.0445,   2.1904,  -0.9057,  ...,  -5.3673,  -0.7731,  -9.3144],\n",
       "          [  1.8174,   2.6204,   0.4204,  ...,  -2.2814,  -1.8307,  -6.4018],\n",
       "          ...,\n",
       "          [  1.0605,   1.4574,  -0.4429,  ...,  -4.6049,  -1.9598, -10.6542],\n",
       "          [  1.9393,   0.3914,  -0.1384,  ...,  -4.9616,   0.1996,  -7.4803],\n",
       "          [ -1.4210,   1.5175,  -2.5092,  ...,  -4.2177,  -0.1770,  -8.0362]],\n",
       "\n",
       "         [[ -4.2090,   2.4606,  -3.0029,  ...,  -0.9210,  -0.8173,  -0.0473],\n",
       "          [ -3.0802,   6.3758,  -3.9407,  ...,  -0.1295,  -2.5523,   0.0266],\n",
       "          [  1.8610,   7.0746,  -4.1541,  ...,   1.5571,  -2.7389,   0.9549],\n",
       "          ...,\n",
       "          [  3.1022,  -5.7188,  -1.5736,  ...,   0.2542,  -2.4546,   0.6648],\n",
       "          [  2.9822,  -5.2570,   1.2309,  ...,   0.6838,  -0.1826,   2.1735],\n",
       "          [  1.6316,  -1.6477,  -0.9737,  ...,   0.2122,   1.3249,  -0.6021]],\n",
       "\n",
       "         [[ -1.2538,  -0.5489,  -2.5092,  ...,   0.5524,  -2.0164,   4.8403],\n",
       "          [  3.1314,  -1.0801,  -4.1054,  ...,  -0.7971,  -1.5640,   6.5363],\n",
       "          [  1.5652,  -1.2470,  -3.3393,  ...,   1.5826,  -1.3942,   6.3957],\n",
       "          ...,\n",
       "          [  0.0434,  -3.0214,   3.0134,  ...,   1.2415,  -1.5284,   5.9473],\n",
       "          [ -1.1368,  -4.7647,   2.9855,  ...,   1.6673,   0.1903,   7.0039],\n",
       "          [ -1.7362,  -4.1795,   2.3845,  ...,   1.2379,  -1.2108,   4.7475]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -2.5658,  -2.3919,  -3.9581,  ...,   0.8540,  -3.1736,   7.6799],\n",
       "          [  0.0913,  -0.4333,  -1.3049,  ...,   0.6873,  -2.2763,   5.7861],\n",
       "          [  0.3657,  -0.1887,  -0.9285,  ...,   1.7062,  -3.0291,   8.0230],\n",
       "          ...,\n",
       "          [  0.0903,   0.2804,   0.7260,  ...,   1.9899,  -1.7552,   6.2227],\n",
       "          [  0.5865,  -2.4746,   2.8663,  ...,   3.3479,  -2.4673,   8.1583],\n",
       "          [ -0.7837,  -2.5039,   1.9699,  ...,   0.7989,  -3.0369,   6.7838]],\n",
       "\n",
       "         [[  7.5946,  -6.5868,   7.2445,  ...,  -3.8487,   9.3240,  -2.4280],\n",
       "          [ -0.7694,  -1.5667,  -0.0556,  ...,  -3.1236,   8.7021,  -3.6081],\n",
       "          [ -2.2342,  -0.8358,  -0.1275,  ...,  -1.8297,   7.1045,  -2.1094],\n",
       "          ...,\n",
       "          [ -1.5300,  -1.0876,  -1.2039,  ...,  -2.2447,   9.0585,  -3.1360],\n",
       "          [ -1.1062,  -0.3062,  -1.5467,  ...,  -2.5934,   9.5273,  -1.0785],\n",
       "          [ -2.1012,  -3.9920,  -2.7493,  ...,  -2.4204,  10.1785,  -0.9793]],\n",
       "\n",
       "         [[  2.2896,  -0.8297,  -3.2534,  ...,   2.4506,  -4.2068,   0.4970],\n",
       "          [  1.2366,  -1.2327,  -1.1662,  ...,   4.6482,  -4.0981,   1.0570],\n",
       "          [ -2.5748,  -1.0756,  -2.0338,  ...,   4.0309,  -3.6610,  -0.9798],\n",
       "          ...,\n",
       "          [ -3.4683,  -2.2953,   2.3908,  ...,   5.1049,  -3.3248,  -0.8591],\n",
       "          [ -3.6476,  -4.5222,   2.1959,  ...,   4.1885,  -3.0634,  -0.3256],\n",
       "          [ -0.6979,  -4.6476,   3.1156,  ...,   3.9853,  -2.7934,   2.1638]]]],\n",
       "       grad_fn=<CatBackward0>), tensor([[[[-0.4589,  0.2973, -0.0270,  ..., -1.2603, -1.8995, -0.3726],\n",
       "          [-1.2836, -1.2670, -0.4590,  ...,  0.1119, -1.2061,  1.3530],\n",
       "          [-0.4901,  0.6216,  0.2129,  ...,  0.5069,  0.5289, -0.5749],\n",
       "          ...,\n",
       "          [-0.4228, -1.2236, -0.9121,  ..., -0.0795, -0.3901,  1.6079],\n",
       "          [ 0.0417,  0.1697, -0.1955,  ...,  0.3996, -0.3984, -1.1162],\n",
       "          [-0.5351, -0.6917, -0.7539,  ..., -0.4060, -1.8410,  0.1345]],\n",
       "\n",
       "         [[-0.9529, -0.6461, -0.0490,  ..., -0.6801, -0.8822, -1.1290],\n",
       "          [-0.3187, -0.3852, -0.5090,  ..., -1.4737, -1.2161,  0.5302],\n",
       "          [ 0.5419, -1.5173,  0.9250,  ..., -1.1115,  0.5047, -0.9159],\n",
       "          ...,\n",
       "          [-0.7263, -0.1127, -0.1685,  ..., -0.7873, -1.0224, -1.1487],\n",
       "          [-0.6702, -0.8871,  0.8001,  ..., -0.1002, -0.1024, -1.3545],\n",
       "          [ 0.2512, -0.6528, -0.4523,  ..., -0.1494, -1.0726, -0.6222]],\n",
       "\n",
       "         [[ 0.2493, -0.1529, -0.3354,  ..., -0.3681, -1.0955,  0.7751],\n",
       "          [ 0.8398, -0.1488, -0.1882,  ...,  0.7387,  0.0994,  0.9790],\n",
       "          [ 0.8140, -0.0625,  0.2169,  ...,  0.5133,  0.6648,  0.4122],\n",
       "          ...,\n",
       "          [ 0.3295,  0.4915,  0.8206,  ...,  0.7212,  0.5055,  0.3436],\n",
       "          [-1.0079,  1.6759,  0.5638,  ...,  0.5452, -0.3959, -0.9550],\n",
       "          [-0.0504,  0.5172, -0.3159,  ..., -0.7155, -0.1606,  0.5203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9365,  0.4328, -0.9480,  ..., -0.4272, -0.5121, -0.8737],\n",
       "          [ 0.6221,  1.1857, -0.0140,  ...,  1.4290,  0.7046, -0.9846],\n",
       "          [ 0.4149,  0.9377,  0.2253,  ...,  0.8597,  1.2289, -0.5031],\n",
       "          ...,\n",
       "          [ 0.3177,  0.4425,  0.1218,  ...,  1.0654,  0.6043, -1.2766],\n",
       "          [ 1.7541, -1.5217,  1.0928,  ...,  0.3364,  1.7037, -0.6846],\n",
       "          [-0.0273, -0.8816,  0.6848,  ...,  0.5476,  0.0788, -0.0314]],\n",
       "\n",
       "         [[-0.2229,  0.0387, -0.5804,  ..., -0.1806,  0.5097, -0.2117],\n",
       "          [ 0.5699,  0.0861,  0.1591,  ..., -0.1445, -0.9924,  0.7409],\n",
       "          [ 1.0283, -0.5623,  0.4654,  ..., -0.0944,  0.0992,  0.8376],\n",
       "          ...,\n",
       "          [-0.2833,  0.3152,  0.4304,  ..., -0.0344, -1.4708,  1.2642],\n",
       "          [ 0.2758, -0.0578,  0.8082,  ...,  0.5153, -0.7366,  0.4153],\n",
       "          [ 0.0289, -0.1841, -0.1444,  ...,  0.2775, -0.8394, -0.0374]],\n",
       "\n",
       "         [[ 0.1986,  0.8529,  0.7224,  ..., -1.2539,  0.5617,  0.4346],\n",
       "          [-1.1368,  1.3038,  0.1968,  ..., -0.2890,  0.3585,  1.1385],\n",
       "          [-1.0671, -0.1753,  0.4630,  ...,  0.6825,  0.0285, -0.2589],\n",
       "          ...,\n",
       "          [-0.5560,  0.9345, -0.2097,  ...,  0.3886,  0.3403,  0.0796],\n",
       "          [-0.4900, -0.5070,  0.2085,  ...,  0.3012,  0.6561, -0.2470],\n",
       "          [-0.1001,  0.9166, -0.2636,  ..., -1.2829,  0.2980,  0.9032]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.5259e+00, -4.0988e+00,  3.8645e+00,  ...,  1.2462e+01,\n",
       "           -6.7518e+00,  1.8532e+01],\n",
       "          [ 4.0225e+00, -2.2537e+00,  1.3388e+00,  ...,  1.1012e+01,\n",
       "           -4.1644e+00,  1.8599e+01],\n",
       "          [ 2.1162e+00, -3.2253e+00,  9.5723e-01,  ...,  1.1727e+01,\n",
       "           -6.3102e+00,  1.8784e+01],\n",
       "          ...,\n",
       "          [ 8.1032e-01, -1.2496e-01,  1.9607e+00,  ...,  9.7453e+00,\n",
       "           -4.6800e+00,  1.8191e+01],\n",
       "          [-1.4466e+00,  1.2566e-01,  1.0047e+00,  ...,  1.0977e+01,\n",
       "           -5.3220e+00,  1.9256e+01],\n",
       "          [-1.9122e+00, -6.7935e-02,  2.4392e+00,  ...,  1.2021e+01,\n",
       "           -6.7776e+00,  1.8765e+01]],\n",
       "\n",
       "         [[ 3.2397e+00,  3.6258e+00, -4.7746e+00,  ...,  2.0472e+00,\n",
       "           -2.8568e+00, -2.7174e+00],\n",
       "          [ 6.0326e+00,  4.2655e+00, -4.8018e+00,  ...,  1.2209e+00,\n",
       "           -1.6290e+00, -3.5259e+00],\n",
       "          [ 2.7783e+00,  3.5706e+00, -4.4758e+00,  ...,  1.6285e+00,\n",
       "           -2.0754e+00, -2.5827e+00],\n",
       "          ...,\n",
       "          [-6.6444e-02, -1.1432e+00, -1.9516e+00,  ...,  1.3702e+00,\n",
       "           -1.1141e+00, -4.0284e+00],\n",
       "          [-3.5078e+00,  9.7564e-03, -2.9012e+00,  ...,  2.2955e+00,\n",
       "            9.1508e-01, -2.1894e+00],\n",
       "          [-4.9355e+00,  9.8755e-01, -2.4718e+00,  ...,  1.4104e+00,\n",
       "            1.0470e+00, -3.6378e+00]],\n",
       "\n",
       "         [[-3.1073e+00,  1.6231e+00,  1.8596e+00,  ..., -5.5833e-01,\n",
       "            5.7751e+00,  3.6243e+00],\n",
       "          [-1.7394e+00,  1.6222e+00,  1.3704e+00,  ...,  5.7783e-01,\n",
       "            4.8492e+00,  2.1741e+00],\n",
       "          [ 1.4741e+00,  1.6880e+00,  2.2187e+00,  ...,  3.0848e-02,\n",
       "            3.9412e+00,  3.8219e+00],\n",
       "          ...,\n",
       "          [ 6.6474e-01, -8.1300e-02, -1.6428e+00,  ...,  7.1708e-01,\n",
       "            4.6142e+00,  1.3187e+00],\n",
       "          [ 1.8801e+00,  9.7526e-01, -1.2777e+00,  ...,  1.5948e+00,\n",
       "            5.5086e+00,  4.0642e+00],\n",
       "          [ 9.2813e-01,  1.4349e+00, -2.4346e+00,  ..., -4.6679e-01,\n",
       "            5.3832e+00,  3.6658e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2633e-01,  1.1276e+00,  1.5753e+00,  ..., -1.3047e+00,\n",
       "           -1.4376e+00, -8.6269e+00],\n",
       "          [ 1.8159e+00, -1.5426e+00,  1.6111e+00,  ..., -1.5897e+00,\n",
       "           -2.4245e+00, -1.0054e+01],\n",
       "          [ 5.8836e-01, -1.1231e+00,  1.1708e+00,  ..., -1.1143e+00,\n",
       "           -2.4634e+00, -1.0103e+01],\n",
       "          ...,\n",
       "          [ 3.6621e-01, -4.2916e-01, -6.5067e-01,  ..., -1.2664e+00,\n",
       "           -2.6423e+00, -1.0366e+01],\n",
       "          [-6.1006e-01,  4.8629e-01,  2.8463e-01,  ..., -1.6039e+00,\n",
       "           -2.5640e+00, -9.7291e+00],\n",
       "          [-5.7838e-01, -2.5130e-01,  1.1026e+00,  ..., -3.7918e+00,\n",
       "           -1.7945e+00, -8.0351e+00]],\n",
       "\n",
       "         [[-2.3888e-01,  2.6839e+00, -2.0028e+00,  ..., -5.0939e+00,\n",
       "           -1.3754e+01,  7.5374e+00],\n",
       "          [ 1.1924e+00,  2.3863e+00, -2.1132e+00,  ..., -2.7936e+00,\n",
       "           -1.3111e+01,  9.4355e+00],\n",
       "          [ 2.0158e+00,  2.0425e+00, -1.1345e+00,  ..., -4.2068e+00,\n",
       "           -1.0947e+01,  7.7533e+00],\n",
       "          ...,\n",
       "          [ 1.4260e+00,  4.3342e-02,  2.2756e+00,  ..., -3.8011e+00,\n",
       "           -1.2116e+01,  8.6812e+00],\n",
       "          [-5.0103e-01,  1.2142e-01,  2.3606e+00,  ..., -6.5808e+00,\n",
       "           -1.2208e+01,  7.1839e+00],\n",
       "          [-2.3576e+00, -3.8727e-01,  2.1732e+00,  ..., -6.5483e+00,\n",
       "           -1.3128e+01,  6.8234e+00]],\n",
       "\n",
       "         [[-7.0909e+00, -5.2113e+00,  4.8547e+00,  ..., -4.6048e+00,\n",
       "            6.2397e+00, -4.6193e+00],\n",
       "          [-1.1559e+00, -5.2244e+00,  4.9453e+00,  ..., -3.6843e+00,\n",
       "            4.6064e+00, -6.0654e+00],\n",
       "          [ 5.1706e+00, -4.9662e+00,  4.7463e+00,  ..., -4.5458e+00,\n",
       "            6.3703e+00, -6.0169e+00],\n",
       "          ...,\n",
       "          [ 6.3217e+00,  5.9593e-01,  3.0154e+00,  ..., -4.2367e+00,\n",
       "            4.6016e+00, -6.2694e+00],\n",
       "          [ 5.0704e+00, -1.0013e+00,  3.0846e+00,  ..., -4.8277e+00,\n",
       "            5.6181e+00, -6.6863e+00],\n",
       "          [-7.8026e-02, -1.9583e+00,  3.1169e+00,  ..., -4.4432e+00,\n",
       "            6.1186e+00, -6.6502e+00]]]], grad_fn=<CatBackward0>), tensor([[[[-0.6725, -1.4011,  1.0255,  ...,  0.4941,  1.5712, -0.9120],\n",
       "          [ 0.0442,  0.3744, -0.6944,  ..., -1.7031, -0.0182, -0.3582],\n",
       "          [-0.2446, -0.3241,  0.1294,  ..., -0.2156, -0.5475, -0.4768],\n",
       "          ...,\n",
       "          [ 1.1743,  2.3226, -0.3893,  ..., -2.4223, -0.3149, -1.2380],\n",
       "          [ 0.3236,  0.4730, -0.2003,  ..., -1.2699, -1.3415,  0.6820],\n",
       "          [ 0.5757,  0.3106, -0.0194,  ..., -0.7061, -1.4125,  0.8065]],\n",
       "\n",
       "         [[-0.7551, -0.7967, -1.5454,  ..., -0.8153, -1.5139, -2.3523],\n",
       "          [ 0.2874,  0.4510, -0.3448,  ..., -0.4038, -1.9486, -2.6896],\n",
       "          [ 1.3705,  0.6849, -0.4515,  ..., -1.5974, -1.8570, -1.6219],\n",
       "          ...,\n",
       "          [ 1.4526,  0.2635,  0.9189,  ..., -0.4440, -3.5584, -1.4909],\n",
       "          [-1.0508,  0.9634,  1.0893,  ..., -0.5901, -0.7879, -2.1901],\n",
       "          [-0.1796, -1.2327,  1.8748,  ..., -0.3318, -0.0957, -1.7365]],\n",
       "\n",
       "         [[-0.9640, -0.2208,  0.6613,  ...,  0.4389,  0.5965,  0.9668],\n",
       "          [ 1.6307,  1.5437,  1.9742,  ...,  1.8586,  0.6161,  2.6178],\n",
       "          [ 0.2065,  1.3303, -1.2894,  ..., -0.0391,  1.3701, -0.3891],\n",
       "          ...,\n",
       "          [ 2.6362,  0.6612,  1.1805,  ...,  0.9378,  1.6252,  2.1991],\n",
       "          [ 1.6185,  1.2242,  0.4375,  ...,  0.3486,  1.2233,  1.7755],\n",
       "          [ 2.0087,  1.3367,  1.1980,  ..., -0.1650, -0.3967,  2.1230]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0991,  0.6449, -1.0552,  ..., -0.3998,  1.2777, -0.0696],\n",
       "          [ 1.9806,  0.9587,  1.0854,  ...,  0.2916,  2.0773, -0.0615],\n",
       "          [ 1.4947, -1.2100, -0.7021,  ..., -0.7561,  0.6081,  0.1917],\n",
       "          ...,\n",
       "          [ 2.6795,  1.0788,  0.5684,  ...,  0.3325,  1.8064, -1.0806],\n",
       "          [ 2.4846, -0.8552, -0.9010,  ..., -0.0104,  0.1972, -0.3811],\n",
       "          [ 1.0051, -0.1216, -1.0311,  ...,  1.1904,  0.6824,  0.0873]],\n",
       "\n",
       "         [[-0.0992,  0.1660, -0.1596,  ..., -0.4977, -0.5783, -0.7475],\n",
       "          [ 0.0676, -0.3771, -0.8598,  ..., -0.4712, -0.0835, -1.5002],\n",
       "          [ 0.4417,  1.2722, -1.2788,  ..., -1.0957,  0.0114, -2.2420],\n",
       "          ...,\n",
       "          [-0.3660,  0.4248, -0.2936,  ..., -0.2815, -0.0626, -1.2036],\n",
       "          [ 0.9783, -0.0780, -0.5443,  ..., -0.3644,  0.0681,  0.3420],\n",
       "          [ 0.8043, -0.6331, -0.6668,  ...,  0.2672,  0.0481,  1.1605]],\n",
       "\n",
       "         [[-2.2983,  0.4081, -5.0322,  ...,  1.0939,  1.9307, -1.0486],\n",
       "          [ 1.6489, -1.7154,  1.6263,  ..., -2.4954,  0.4732,  3.2434],\n",
       "          [-4.0993,  3.4753, -1.9122,  ...,  4.2025, -1.6148,  0.0805],\n",
       "          ...,\n",
       "          [ 0.5333, -3.6257,  0.5416,  ..., -1.8915, -2.1081,  2.8798],\n",
       "          [-0.2002,  4.2675, -4.5858,  ...,  2.5161, -4.0384, -1.5390],\n",
       "          [-0.7053,  2.2906,  0.5807,  ...,  0.5581, -4.4480,  0.8265]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-7.2120e-01,  1.9141e-01, -4.1446e-01,  ..., -6.6114e+01,\n",
       "           -6.6605e+01,  4.1313e+01],\n",
       "          [ 8.6045e-01,  1.8509e+00,  1.4113e+00,  ..., -6.6359e+01,\n",
       "           -6.5939e+01,  4.1815e+01],\n",
       "          [-2.2050e-01,  1.8822e+00, -4.2584e-01,  ..., -6.6323e+01,\n",
       "           -6.6075e+01,  4.3637e+01],\n",
       "          ...,\n",
       "          [ 1.5024e+00, -1.3571e+00,  9.1998e-01,  ..., -6.5464e+01,\n",
       "           -6.6946e+01,  4.2233e+01],\n",
       "          [-1.2824e-01, -3.8238e-01,  6.2376e-01,  ..., -6.5169e+01,\n",
       "           -6.6843e+01,  4.2928e+01],\n",
       "          [ 5.0236e-01, -8.8545e-01, -3.9383e-01,  ..., -6.6639e+01,\n",
       "           -6.6281e+01,  4.3661e+01]],\n",
       "\n",
       "         [[-8.6918e-01, -7.1029e-02, -2.2883e+00,  ...,  2.9074e+01,\n",
       "           -5.5091e+01, -6.2036e+01],\n",
       "          [-7.4133e-01,  1.6592e-02, -2.2988e+00,  ...,  2.9405e+01,\n",
       "           -5.4707e+01, -6.1738e+01],\n",
       "          [-3.7008e-01, -2.9946e-01, -1.5207e+00,  ...,  3.1795e+01,\n",
       "           -5.3736e+01, -6.2260e+01],\n",
       "          ...,\n",
       "          [ 2.8052e-01, -4.6143e-01, -1.6391e+00,  ...,  3.0288e+01,\n",
       "           -5.4192e+01, -6.1620e+01],\n",
       "          [-6.8315e-03, -6.4058e-01, -2.2960e-01,  ...,  2.9128e+01,\n",
       "           -5.3222e+01, -6.2100e+01],\n",
       "          [ 6.8458e-01,  1.2852e+00, -1.2961e+00,  ...,  2.6490e+01,\n",
       "           -5.3645e+01, -6.1996e+01]],\n",
       "\n",
       "         [[-7.4209e-01, -2.0811e+00,  1.4904e+00,  ...,  2.7896e-01,\n",
       "            2.6364e+00,  6.9771e-01],\n",
       "          [-7.3401e-01, -2.6110e+00,  1.4766e+00,  ..., -7.3761e-01,\n",
       "            3.8661e+00,  8.5110e-01],\n",
       "          [-4.7340e-01, -1.6869e+00,  7.5856e-01,  ..., -5.8855e-01,\n",
       "            3.5835e+00,  1.9453e+00],\n",
       "          ...,\n",
       "          [ 3.8588e-02,  1.2242e+00, -1.0883e+00,  ..., -3.5253e-01,\n",
       "            3.7129e+00,  2.5041e+00],\n",
       "          [ 6.8639e-01,  2.2107e-01, -1.0581e+00,  ...,  8.5893e-01,\n",
       "            3.0893e+00,  2.2292e+00],\n",
       "          [ 9.9109e-01, -8.3166e-01, -1.7816e+00,  ...,  3.2795e-01,\n",
       "            2.6315e+00,  2.3819e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.4882e-02,  9.7006e-01, -1.3015e+00,  ..., -2.3637e+01,\n",
       "            3.5301e+01, -3.8129e+01],\n",
       "          [-2.7062e-01,  7.3350e-01, -7.9329e-01,  ..., -2.3137e+01,\n",
       "            3.5413e+01, -3.8299e+01],\n",
       "          [-1.2109e-01,  9.2907e-01,  2.6782e-01,  ..., -2.3261e+01,\n",
       "            3.5407e+01, -3.7908e+01],\n",
       "          ...,\n",
       "          [ 6.7591e-02, -1.1174e+00, -1.3148e+00,  ..., -2.4520e+01,\n",
       "            3.5600e+01, -3.7790e+01],\n",
       "          [ 3.2752e-01, -1.2139e+00, -1.0159e+00,  ..., -2.3607e+01,\n",
       "            3.6063e+01, -3.8071e+01],\n",
       "          [ 5.2609e-03, -9.8668e-02, -5.9969e-01,  ..., -2.2543e+01,\n",
       "            3.5982e+01, -3.7341e+01]],\n",
       "\n",
       "         [[-1.7191e+00, -1.3073e+00, -3.8694e-01,  ...,  6.5602e+01,\n",
       "           -6.6251e+01, -6.5037e+01],\n",
       "          [-1.8393e+00, -1.5488e+00, -4.5978e-01,  ...,  6.5575e+01,\n",
       "           -6.6066e+01, -6.4946e+01],\n",
       "          [ 3.5477e-01, -1.3974e+00, -4.9064e-01,  ...,  6.5879e+01,\n",
       "           -6.5845e+01, -6.4816e+01],\n",
       "          ...,\n",
       "          [ 4.8768e-01, -5.3508e-01,  3.0946e-01,  ...,  6.6270e+01,\n",
       "           -6.4745e+01, -6.3712e+01],\n",
       "          [ 7.7525e-01, -3.8590e-01, -7.6483e-01,  ...,  6.6218e+01,\n",
       "           -6.4080e+01, -6.3834e+01],\n",
       "          [ 2.6652e-01, -3.0259e-01, -2.3527e-01,  ...,  6.5139e+01,\n",
       "           -6.4686e+01, -6.2542e+01]],\n",
       "\n",
       "         [[ 2.5375e+00, -3.2672e+00, -1.0365e+00,  ...,  1.3717e+00,\n",
       "            6.3463e+00,  1.4373e+01],\n",
       "          [-6.8007e-01, -2.5275e+00, -2.1309e+00,  ...,  9.3257e-01,\n",
       "            7.8883e+00,  1.4963e+01],\n",
       "          [-2.1458e+00, -1.1140e+00, -9.9935e-01,  ...,  2.7980e-01,\n",
       "            6.9787e+00,  1.4422e+01],\n",
       "          ...,\n",
       "          [-3.1115e+00, -4.4916e-01,  2.1665e+00,  ..., -1.5197e+00,\n",
       "            9.0202e+00,  1.4677e+01],\n",
       "          [-2.3377e+00, -9.3694e-01,  5.1419e+00,  ..., -6.7720e-01,\n",
       "            7.9409e+00,  1.3944e+01],\n",
       "          [ 1.1847e+00, -1.9763e+00,  4.0924e+00,  ...,  3.6735e-01,\n",
       "            8.4523e+00,  1.4040e+01]]]], grad_fn=<CatBackward0>), tensor([[[[-0.3803, -0.9722, -2.5682,  ...,  1.4375,  1.5089, -1.1480],\n",
       "          [-0.8285, -0.6995, -0.8097,  ...,  0.9691,  1.0774,  0.2795],\n",
       "          [-1.5180, -2.3590, -0.4658,  ...,  1.0065, -1.0064,  0.8049],\n",
       "          ...,\n",
       "          [-1.1485, -1.3576,  1.6919,  ..., -1.3270, -1.3666,  2.5791],\n",
       "          [-0.7179, -0.7824, -1.4078,  ..., -0.7804,  0.4206,  1.8660],\n",
       "          [-0.9168,  0.4346, -0.2983,  ..., -0.1648,  0.9534, -1.2232]],\n",
       "\n",
       "         [[-0.1679,  1.9017, -0.3614,  ..., -0.8571, -0.3426, -1.3663],\n",
       "          [-1.4063, -0.1201, -1.0301,  ..., -0.8904,  1.4804, -1.3280],\n",
       "          [-0.8626,  1.2582, -1.1295,  ..., -0.3617,  1.8789,  0.9854],\n",
       "          ...,\n",
       "          [-0.1361, -0.0136, -0.6432,  ..., -0.3082,  1.7789,  0.3716],\n",
       "          [ 1.3242, -1.4677, -1.1909,  ..., -0.3307,  1.7150, -1.7225],\n",
       "          [ 1.1141, -2.7549, -0.3957,  ..., -0.3729,  0.9127, -2.2440]],\n",
       "\n",
       "         [[-0.1151, -0.2067,  1.1364,  ..., -0.2604, -0.1151, -0.4285],\n",
       "          [-0.3358,  0.1896,  2.7482,  ..., -1.2245, -0.6235, -1.4638],\n",
       "          [-0.1821,  0.9096,  0.7124,  ...,  0.8866, -0.0399, -0.7517],\n",
       "          ...,\n",
       "          [ 0.4683, -0.0096,  2.4698,  ..., -0.5643, -0.1243, -1.9971],\n",
       "          [-1.3431,  0.4256,  1.2786,  ...,  1.4476,  1.5586,  1.1849],\n",
       "          [-1.4803,  0.8387,  1.6439,  ...,  1.4821,  2.0091, -0.5786]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0136,  2.4727,  0.1438,  ..., -1.4963, -0.9233, -3.9607],\n",
       "          [-2.4042,  1.6710,  1.9895,  ...,  0.2503, -1.1104, -0.5369],\n",
       "          [ 0.3653,  0.9090,  0.2546,  ..., -0.1974, -0.0674, -1.8181],\n",
       "          ...,\n",
       "          [-1.8878,  0.7520,  0.4462,  ..., -0.0190,  1.0686,  0.3965],\n",
       "          [ 1.7335, -0.2536,  0.4355,  ..., -1.5394,  0.7225,  0.9851],\n",
       "          [-0.4402,  1.3169, -0.6273,  ...,  0.4359,  1.8009, -0.0645]],\n",
       "\n",
       "         [[-1.3552,  0.5766,  0.0116,  ..., -2.7619, -1.6232, -1.0557],\n",
       "          [-0.8703, -0.3280,  0.3924,  ..., -1.1942, -4.4642, -1.8627],\n",
       "          [-1.9900,  3.8021, -3.9446,  ..., -2.6330, -0.1878,  4.1109],\n",
       "          ...,\n",
       "          [-1.3166,  3.4511, -0.7179,  ...,  1.5349, -1.1574, -0.2496],\n",
       "          [-2.3787,  2.2165, -0.4648,  ..., -2.1880,  1.0451,  0.8034],\n",
       "          [-1.7127, -0.5483,  0.2656,  ...,  1.8520, -0.5252,  2.4196]],\n",
       "\n",
       "         [[ 1.5987, -1.3605, -0.1013,  ..., -1.4397,  0.2832, -0.7030],\n",
       "          [ 1.4151, -1.3160, -0.2599,  ..., -2.3856,  1.0881, -0.3019],\n",
       "          [ 0.5660, -1.3111, -0.0955,  ..., -1.6818, -0.1610, -0.2597],\n",
       "          ...,\n",
       "          [ 1.4290, -1.3316, -0.1450,  ..., -2.3469,  1.4483, -0.2675],\n",
       "          [-2.9601, -0.5023,  0.2538,  ..., -2.2423,  0.6884, -1.7294],\n",
       "          [ 0.6001, -0.1531, -0.0797,  ..., -0.2090, -0.9971, -0.1542]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-5.3890e-01, -1.7022e-01,  1.1970e+00,  ...,  3.8903e+01,\n",
       "           -3.4831e+01,  3.1929e+01],\n",
       "          [-3.8474e-01,  8.4714e-01, -5.9316e-01,  ...,  3.8585e+01,\n",
       "           -3.4161e+01,  3.2303e+01],\n",
       "          [-1.7109e+00, -1.8106e-01,  1.8191e+00,  ...,  3.9054e+01,\n",
       "           -3.4511e+01,  3.1706e+01],\n",
       "          ...,\n",
       "          [-8.2162e-01, -2.8650e-02, -6.6961e-01,  ...,  3.9360e+01,\n",
       "           -3.4092e+01,  3.1786e+01],\n",
       "          [-1.1403e+00, -1.1250e+00,  1.3596e+00,  ...,  3.9422e+01,\n",
       "           -3.3751e+01,  3.1797e+01],\n",
       "          [ 7.9106e-01, -6.9019e-01,  4.6818e-01,  ...,  3.9068e+01,\n",
       "           -3.3091e+01,  3.2471e+01]],\n",
       "\n",
       "         [[ 7.6073e-02,  4.3946e-01,  9.6258e-01,  ..., -3.6567e+01,\n",
       "           -2.2699e+01, -4.0933e+01],\n",
       "          [ 2.3935e+00, -4.3573e-01,  9.7493e-01,  ..., -3.6710e+01,\n",
       "           -2.4923e+01, -4.1889e+01],\n",
       "          [ 3.9849e-01,  5.9218e-01,  4.1918e+00,  ..., -3.7043e+01,\n",
       "           -2.5226e+01, -4.1191e+01],\n",
       "          ...,\n",
       "          [-8.7143e-01, -9.1242e-01, -1.0582e+00,  ..., -3.8126e+01,\n",
       "           -2.5759e+01, -4.1678e+01],\n",
       "          [-1.8465e+00, -3.2468e+00,  1.5911e+00,  ..., -3.7925e+01,\n",
       "           -2.6083e+01, -4.0888e+01],\n",
       "          [-1.9862e+00, -2.1914e+00,  4.9495e-01,  ..., -3.7333e+01,\n",
       "           -2.5482e+01, -4.0889e+01]],\n",
       "\n",
       "         [[ 1.1659e+00,  9.3355e-01,  2.2610e+00,  ..., -2.3886e+01,\n",
       "           -4.3432e+01,  4.3221e+01],\n",
       "          [-9.1933e-01, -8.8926e-01,  2.2740e+00,  ..., -2.4376e+01,\n",
       "           -4.4504e+01,  4.2365e+01],\n",
       "          [ 8.5958e-01, -1.6569e+00, -2.2037e-01,  ..., -2.3476e+01,\n",
       "           -4.5953e+01,  4.3500e+01],\n",
       "          ...,\n",
       "          [-6.6982e-01, -1.8540e+00,  2.4585e+00,  ..., -2.3041e+01,\n",
       "           -4.5624e+01,  4.2518e+01],\n",
       "          [-3.2128e+00, -3.6970e+00,  1.8552e+00,  ..., -2.5632e+01,\n",
       "           -4.6387e+01,  4.2938e+01],\n",
       "          [-3.3812e+00, -6.0102e-02, -2.3840e-01,  ..., -2.2663e+01,\n",
       "           -4.5991e+01,  4.2296e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.1313e-01, -1.0369e+00,  8.7848e-01,  ..., -3.3559e+01,\n",
       "            3.9619e+01,  4.7773e+01],\n",
       "          [-5.5993e-01, -4.8710e-01, -2.7489e-02,  ..., -3.5705e+01,\n",
       "            4.0345e+01,  4.8227e+01],\n",
       "          [ 5.9799e-01,  9.8466e-01, -2.3475e+00,  ..., -3.4958e+01,\n",
       "            3.9989e+01,  4.7752e+01],\n",
       "          ...,\n",
       "          [ 2.1248e-01,  1.2219e+00,  1.5399e+00,  ..., -3.4796e+01,\n",
       "            4.1418e+01,  4.9126e+01],\n",
       "          [ 7.8041e-01,  2.8705e+00,  4.1672e-01,  ..., -3.5377e+01,\n",
       "            4.1023e+01,  4.9261e+01],\n",
       "          [-4.8369e-01,  1.7471e+00, -6.7654e-02,  ..., -3.4646e+01,\n",
       "            4.0123e+01,  4.9040e+01]],\n",
       "\n",
       "         [[-2.2514e-01,  1.3080e+00,  3.1245e-01,  ...,  6.6638e+01,\n",
       "            5.5431e+01,  4.0213e+01],\n",
       "          [-6.0775e-01,  5.5575e-01,  4.6300e-01,  ...,  6.4645e+01,\n",
       "            5.5429e+01,  3.9805e+01],\n",
       "          [ 1.0160e+00, -9.4980e-01,  9.6901e-01,  ...,  6.5687e+01,\n",
       "            5.5119e+01,  4.0154e+01],\n",
       "          ...,\n",
       "          [ 1.2342e-01,  3.5952e-01,  1.2679e+00,  ...,  6.4383e+01,\n",
       "            5.4429e+01,  3.9970e+01],\n",
       "          [ 8.3030e-01, -1.5097e-01,  1.3860e+00,  ...,  6.6151e+01,\n",
       "            5.4914e+01,  4.1410e+01],\n",
       "          [ 1.9842e+00,  2.1131e-01,  9.3457e-01,  ...,  6.7101e+01,\n",
       "            5.4670e+01,  4.0905e+01]],\n",
       "\n",
       "         [[ 4.3338e-01,  4.5288e-01,  1.2120e-01,  ...,  4.7549e+01,\n",
       "           -3.4171e+01, -3.6378e+01],\n",
       "          [-5.6876e-01,  1.0177e+00, -2.9012e-01,  ...,  4.7748e+01,\n",
       "           -3.3376e+01, -3.6510e+01],\n",
       "          [ 1.6263e-01,  6.4584e-01, -3.4555e-01,  ...,  4.7701e+01,\n",
       "           -3.3669e+01, -3.6898e+01],\n",
       "          ...,\n",
       "          [ 2.6920e-02, -1.2976e+00,  2.2387e+00,  ...,  4.8483e+01,\n",
       "           -3.4254e+01, -3.5297e+01],\n",
       "          [ 1.2637e+00, -2.6430e+00,  1.7433e+00,  ...,  4.8797e+01,\n",
       "           -3.1768e+01, -3.5179e+01],\n",
       "          [-1.6579e-01, -2.6669e+00,  1.9642e+00,  ...,  4.8366e+01,\n",
       "           -3.3069e+01, -3.5544e+01]]]], grad_fn=<CatBackward0>), tensor([[[[-8.8305e-01, -1.4723e+00,  1.9239e+00,  ...,  2.1908e-01,\n",
       "            1.2751e+00,  1.9180e+00],\n",
       "          [-1.0687e+00, -5.7427e-01,  2.8651e-01,  ..., -3.0190e+00,\n",
       "            1.2420e+00, -4.5098e-01],\n",
       "          [ 2.5927e-01,  6.3529e-01,  2.3981e+00,  ..., -6.3759e-01,\n",
       "           -1.6723e+00,  1.7748e+00],\n",
       "          ...,\n",
       "          [-1.4888e+00,  6.0883e-02, -2.4617e-01,  ..., -3.7739e+00,\n",
       "           -1.5745e+00,  4.5785e-01],\n",
       "          [-1.6298e+00, -3.0594e+00,  7.9527e-01,  ...,  5.3838e-01,\n",
       "            2.1960e+00,  1.4642e+00],\n",
       "          [ 3.0389e-01, -2.3652e+00,  1.1133e+00,  ..., -4.4643e-01,\n",
       "           -7.0499e-01,  1.9980e+00]],\n",
       "\n",
       "         [[ 1.2989e+00,  1.0329e+00,  2.3491e+00,  ..., -2.6366e+00,\n",
       "           -1.2626e+00, -1.8875e+00],\n",
       "          [-2.3276e+00, -7.7133e-01, -5.2270e-01,  ...,  8.2022e-01,\n",
       "            5.4053e-01, -5.1708e-01],\n",
       "          [-8.3125e-01, -1.3691e+00, -3.2747e+00,  ...,  2.6297e+00,\n",
       "            2.0137e+00, -1.0390e+00],\n",
       "          ...,\n",
       "          [-3.0864e+00,  2.7530e-01, -2.3453e+00,  ...,  7.9416e-01,\n",
       "           -9.7336e-01, -1.1069e+00],\n",
       "          [-1.3310e+00, -7.7451e-01, -4.4785e+00,  ...,  2.7393e+00,\n",
       "           -2.5192e-02,  1.3474e+00],\n",
       "          [-3.6298e+00,  5.4066e-01, -3.9991e-01,  ..., -1.0641e+00,\n",
       "           -2.2126e-02, -1.0507e+00]],\n",
       "\n",
       "         [[-1.3263e+00,  1.1594e+00, -2.2033e-01,  ...,  3.9920e-01,\n",
       "            1.6931e-01,  5.0601e-01],\n",
       "          [-1.0958e+00,  1.8582e+00,  6.8629e-01,  ..., -8.2019e-01,\n",
       "            1.1725e+00, -4.5406e-01],\n",
       "          [ 4.0499e-01,  9.4799e-01,  4.0040e-02,  ..., -3.6437e-01,\n",
       "            1.4241e+00,  7.2865e-01],\n",
       "          ...,\n",
       "          [ 3.7143e-01,  1.7808e+00,  2.0257e-01,  ..., -6.9071e-01,\n",
       "            6.8555e-01,  2.4833e-01],\n",
       "          [-1.5011e+00,  1.9367e+00, -5.0866e-01,  ...,  1.5081e-01,\n",
       "            3.7636e-01, -2.3753e-01],\n",
       "          [-3.0353e-01,  3.7297e-01, -1.3431e+00,  ...,  7.9322e-02,\n",
       "           -3.2795e-01,  1.7642e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3847e+00, -4.6717e+00, -9.2251e-01,  ...,  1.3192e-01,\n",
       "           -1.0886e-01,  1.4827e+00],\n",
       "          [-1.2846e-01, -3.0240e+00, -2.4300e+00,  ..., -1.6278e+00,\n",
       "            4.3274e-01, -5.7390e-01],\n",
       "          [ 1.9501e+00, -1.0301e+00, -1.4501e+00,  ..., -8.6913e-01,\n",
       "            7.9411e-01,  9.8245e-01],\n",
       "          ...,\n",
       "          [-8.2543e-01, -3.1935e-02, -1.2017e+00,  ...,  7.5410e-01,\n",
       "            1.1418e+00,  1.7716e-01],\n",
       "          [-3.6668e-01,  1.9082e+00, -8.0889e-01,  ..., -1.3287e+00,\n",
       "            2.0160e+00, -4.3006e-02],\n",
       "          [ 1.6930e-01, -2.1851e+00, -2.2757e+00,  ..., -6.0468e-03,\n",
       "            1.3218e+00,  1.4412e+00]],\n",
       "\n",
       "         [[-6.1968e-01,  2.6593e+00, -2.3369e+00,  ..., -7.3890e-01,\n",
       "           -1.6927e+00,  1.0780e+00],\n",
       "          [ 3.3079e+00,  3.9468e-01,  1.0792e+00,  ...,  2.0492e+00,\n",
       "            1.9760e-01, -1.7954e+00],\n",
       "          [ 6.4671e-01, -2.3388e+00,  2.2263e+00,  ...,  3.9526e-01,\n",
       "           -1.5012e+00, -1.9298e+00],\n",
       "          ...,\n",
       "          [ 1.0896e+00,  3.0391e+00,  3.0958e-01,  ...,  2.0428e+00,\n",
       "            3.3510e+00, -3.1106e+00],\n",
       "          [ 3.0904e-01,  1.6631e+00, -7.8502e-01,  ...,  2.4294e+00,\n",
       "            9.9313e-02,  1.5003e-01],\n",
       "          [-6.1851e-01,  7.5586e-01,  1.5464e+00,  ..., -4.6876e-01,\n",
       "            7.7006e-01,  7.6807e-01]],\n",
       "\n",
       "         [[ 2.1580e-01,  1.3265e+00,  1.3218e+00,  ..., -1.1448e+00,\n",
       "            2.4530e-01, -9.9147e-02],\n",
       "          [-1.8925e+00, -8.5891e-01,  2.0072e-01,  ..., -1.6057e+00,\n",
       "            1.4852e+00, -9.4461e-01],\n",
       "          [ 2.8872e-01,  7.2655e-01, -7.0280e-01,  ..., -2.0415e+00,\n",
       "            2.7228e-01,  2.1171e+00],\n",
       "          ...,\n",
       "          [-4.3696e-01, -2.3285e-01,  4.3446e-03,  ..., -2.1877e+00,\n",
       "            1.9619e-01,  1.4315e+00],\n",
       "          [-1.1728e+00, -2.2772e-01,  1.3325e+00,  ...,  1.1415e+00,\n",
       "           -6.1158e-01,  3.0150e+00],\n",
       "          [ 1.1486e+00, -3.7030e-02, -4.9175e-01,  ...,  1.4172e+00,\n",
       "            2.5353e+00,  2.7394e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[  8.2568,   3.4873,   6.2076,  ..., -62.9081,  58.2124, -62.3302],\n",
       "          [ -2.9770,   5.1834,   4.6196,  ..., -62.8345,  57.4413, -63.3364],\n",
       "          [ -7.6879,   4.2021,   2.6037,  ..., -62.7634,  58.0958, -62.7005],\n",
       "          ...,\n",
       "          [ -5.5725,  -4.0849,  -2.5455,  ..., -61.1860,  58.9938, -62.8541],\n",
       "          [ -8.5279,  -9.0713,   0.9805,  ..., -62.7492,  58.3267, -62.1203],\n",
       "          [ -0.3683,  -1.9016,  -2.0066,  ..., -63.0349,  58.0058, -63.7449]],\n",
       "\n",
       "         [[ -4.2961,   3.4148,  -2.9213,  ...,  55.5580, -60.7188, -58.8543],\n",
       "          [ -6.0094,  -1.6999,  -2.0761,  ...,  56.4721, -60.8439, -57.6004],\n",
       "          [  2.0573,  -4.2247,  -4.8608,  ...,  56.2877, -60.7782, -58.0352],\n",
       "          ...,\n",
       "          [  5.6341,   7.0617,  -6.8920,  ...,  55.5246, -61.1706, -57.9986],\n",
       "          [  7.0912,   4.3613,  -6.4068,  ...,  54.7736, -62.1503, -58.8255],\n",
       "          [  4.6838,   4.9830,  -6.3068,  ...,  55.3601, -61.4445, -59.0893]],\n",
       "\n",
       "         [[-11.1088,   5.4228,  -0.2433,  ..., -61.9691, -64.7673, -36.4572],\n",
       "          [ -2.5350,   7.6329,  -2.6629,  ..., -61.6286, -65.4420, -34.4779],\n",
       "          [  3.9996,   9.4513,  -3.7148,  ..., -62.6607, -65.1530, -35.8052],\n",
       "          ...,\n",
       "          [  8.5316, -10.1223,  -1.8864,  ..., -61.8237, -63.8025, -38.5473],\n",
       "          [ 11.3152, -17.0107, -19.6209,  ..., -64.3695, -62.8131, -39.1980],\n",
       "          [  7.4866,  -4.5733,  -8.1433,  ..., -62.0815, -62.9110, -35.6815]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  1.9298,  -2.8886,  -4.5490,  ..., -55.3332,  54.6981, -44.9224],\n",
       "          [  3.1318,  -2.5849,  -3.7392,  ..., -54.8657,  54.5386, -44.1495],\n",
       "          [  4.1911,  -3.4800,  -7.3901,  ..., -55.3281,  55.0270, -44.5376],\n",
       "          ...,\n",
       "          [  0.3562,   4.4973,  -5.2235,  ..., -55.1232,  54.0880, -43.1913],\n",
       "          [ -1.6256,   4.6484,  -6.5408,  ..., -54.9945,  55.0371, -44.4929],\n",
       "          [ -6.4647,   1.2378,  -5.6020,  ..., -55.5794,  54.3777, -44.6351]],\n",
       "\n",
       "         [[ -0.1170,  -1.3626,   5.1741,  ..., -58.2785, -59.2015, -62.7747],\n",
       "          [ -0.6602,  -4.2627,   6.6158,  ..., -58.1228, -59.0821, -62.8945],\n",
       "          [ -3.0529,  -7.2539,   7.4753,  ..., -58.5299, -59.2002, -61.1317],\n",
       "          ...,\n",
       "          [ -0.1042,  11.4845,   9.9553,  ..., -58.2698, -59.8697, -62.3028],\n",
       "          [  0.7350,   4.6287,   4.0930,  ..., -58.9879, -56.6200, -62.3778],\n",
       "          [  1.0383,   2.3194,   6.6828,  ..., -58.8173, -56.9672, -61.4484]],\n",
       "\n",
       "         [[ 11.9748,  -2.1903,   5.2803,  ...,  42.6645,  59.5735, -47.1832],\n",
       "          [ -7.6539,   3.4477,  12.9267,  ...,  42.0045,  60.2532, -48.7853],\n",
       "          [-15.0951,   5.4768,  14.7221,  ...,  43.0847,  60.4477, -47.5006],\n",
       "          ...,\n",
       "          [-15.1915,  -8.5182,   5.3464,  ...,  44.6188,  59.7700, -49.0148],\n",
       "          [ -4.1292, -14.3261,   5.3102,  ...,  43.0939,  60.7585, -45.0888],\n",
       "          [ 16.3929, -10.0804,   3.7719,  ...,  43.5379,  59.9370, -46.7989]]]],\n",
       "       grad_fn=<CatBackward0>), tensor([[[[-0.1871,  0.5303, -1.4103,  ...,  3.7413, -0.9752,  0.8967],\n",
       "          [-1.6255,  0.8312,  0.6368,  ..., -0.4735, -1.2982, -0.1716],\n",
       "          [ 0.9699,  1.0628, -0.2235,  ...,  0.5137, -0.9751, -0.8694],\n",
       "          ...,\n",
       "          [-0.8656,  1.2573, -0.4996,  ...,  0.3196, -1.3226, -2.0979],\n",
       "          [-0.5473, -1.5190,  1.4198,  ..., -4.0434, -1.3655, -4.9709],\n",
       "          [-1.3319,  1.0870,  0.4904,  ...,  1.5883, -2.0803, -1.0122]],\n",
       "\n",
       "         [[ 2.2581, -2.6882,  0.6323,  ..., -2.1435, -1.5795,  0.9214],\n",
       "          [ 1.7509, -1.3784,  0.1773,  ..., -1.6487, -0.4331,  1.3144],\n",
       "          [ 1.7536, -0.6880, -1.0105,  ..., -0.9189, -1.9795,  0.2112],\n",
       "          ...,\n",
       "          [ 1.4464, -0.1267, -0.1250,  ..., -3.2223,  0.2144,  0.2476],\n",
       "          [ 0.5696, -2.0066, -1.0959,  ..., -1.7989, -0.5791, -0.4657],\n",
       "          [ 1.9032, -2.6804, -1.3453,  ..., -3.3416, -1.3807,  0.3075]],\n",
       "\n",
       "         [[-2.6351, -0.4495,  2.9073,  ...,  1.2996, -0.2650,  0.4704],\n",
       "          [-1.5940,  1.1463,  1.8073,  ...,  1.3603,  0.1726, -0.0444],\n",
       "          [-3.6936,  0.7007,  3.7729,  ..., -0.9210, -0.0336, -0.2488],\n",
       "          ...,\n",
       "          [-2.8046,  0.3536,  1.0148,  ...,  0.0234,  0.8868, -1.2948],\n",
       "          [ 1.4623,  0.1568, -2.0724,  ...,  2.9010, -0.0856, -3.5934],\n",
       "          [-2.8547, -1.9448,  1.1272,  ...,  1.2866,  0.6315, -0.9191]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8870, -1.8608, -2.5747,  ..., -3.7351, -0.2462,  6.7932],\n",
       "          [ 1.4142,  2.0429, -0.8031,  ..., -0.6570, -0.5319,  3.2448],\n",
       "          [ 0.6747,  0.1783, -5.0126,  ..., -2.8466,  0.3775,  5.9432],\n",
       "          ...,\n",
       "          [-2.8278,  1.3771, -2.6863,  ..., -1.8168,  0.4772, -0.0787],\n",
       "          [-2.7511,  2.3340, -2.3857,  ..., -2.2465, -0.6629, -1.2272],\n",
       "          [-2.3759,  0.6623, -2.5118,  ..., -2.8351, -0.6738,  2.6008]],\n",
       "\n",
       "         [[-2.5849, -1.9354,  0.4686,  ...,  1.1204,  0.9356, -1.8583],\n",
       "          [ 0.4979, -0.8624,  1.2134,  ..., -1.3329,  1.7247, -1.6281],\n",
       "          [-1.6374, -1.6456, -0.5835,  ...,  0.9689,  1.6787, -1.7659],\n",
       "          ...,\n",
       "          [-3.5577, -1.4614,  1.0416,  ...,  0.3501,  2.4632, -2.7256],\n",
       "          [-0.7513, -0.3892,  0.1510,  ...,  1.5483,  0.5099, -2.9897],\n",
       "          [-0.9253, -1.5270, -1.5019,  ...,  0.9133,  0.5780, -1.9001]],\n",
       "\n",
       "         [[ 1.3543, -1.5044, -1.4080,  ...,  1.0490,  1.2892, -0.0848],\n",
       "          [ 0.8460, -2.2119,  0.5533,  ..., -0.3221,  0.9675, -1.7548],\n",
       "          [ 0.9379, -2.9872,  3.1229,  ...,  0.2505, -0.5131, -1.2652],\n",
       "          ...,\n",
       "          [ 0.8185, -2.4546,  0.3969,  ..., -1.3219, -0.7343, -0.8691],\n",
       "          [ 2.1601, -0.6183,  1.2338,  ..., -3.1546, -1.7897, -1.5343],\n",
       "          [ 1.4587, -1.8322, -0.1781,  ..., -1.4218, -0.2181, -0.9016]]]],\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "163c0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['\\n = Tower Building of the Little Rock Arsenal = \\n']\n",
      "8 ['\\n = Cicely Mary Barker = \\n']\n",
      "12 [\"\\n = Gambia women's national football team = \\n\"]\n",
      "13 ['\\n = Plain maskray = \\n']\n",
      "14 ['\\n = 2011 – 12 Columbus Blue Jackets season = \\n']\n",
      "18 ['\\n = Position ; GP = \\n', '\\n = Goals ; A = \\n', '\\n = Points ; PIM = \\n', '\\n = Games Played ; TOI = \\n', '\\n = Wins ; L = \\n', '\\n = Overtime Losses ; GA = \\n', '\\n = Saves ; Sv % = \\n']\n",
      "19 ['\\n = Gregorian Tower = \\n']\n",
      "21 [\"\\n = There's Got to Be a Way = \\n\"]\n",
      "22 ['\\n = Nebraska Highway 88 = \\n', '\\n = USS Atlanta ( 1861 ) = \\n']\n",
      "26 ['\\n = Jacqueline Fernandez = \\n']\n",
      "29 ['\\n = John Cullen = \\n']\n",
      "32 ['\\n = SMS Erzherzog Ferdinand Max = \\n']\n",
      "33 ['\\n = Ancient Egyptian deities = \\n']\n",
      "44 ['\\n = South of Heaven = \\n']\n",
      "47 ['\\n = General aviation in the United Kingdom = \\n']\n",
      "55 ['\\n = SMS Zrínyi = \\n']\n",
      "57 ['\\n = Geopyxis carbonaria = \\n']\n",
      "59 ['\\n = Gold dollar = \\n']\n",
      "64 ['\\n = Johnson – Corey – Chaykovsky reaction = \\n']\n",
      "67 ['\\n = Treaty of Ciudad Juárez = \\n']\n",
      "70 ['\\n = The Feast of the Goat = \\n']\n",
      "76 ['\\n = Charles Eaton ( RAAF officer ) = \\n']\n",
      "80 ['\\n = Tina Fey = \\n']\n",
      "87 ['\\n = WASP @-@ 44 = \\n']\n",
      "88 ['\\n = Elephanta Caves = \\n']\n",
      "96 ['\\n = Devin Townsend = \\n']\n",
      "102 ['\\n = Zagreb Synagogue = \\n']\n",
      "105 ['\\n = 1806 Great Coastal hurricane = \\n']\n",
      "106 ['\\n = Forward Intelligence Team = \\n']\n",
      "109 ['\\n = Trinsey v. Pennsylvania = \\n']\n",
      "110 ['\\n = Michael Jordan = \\n']\n",
      "121 ['\\n = Polish culture during World War II = \\n']\n",
      "131 ['\\n = Arihant @-@ class submarine = \\n']\n",
      "132 ['\\n = SMS Markgraf = \\n']\n",
      "137 ['\\n = Coldrum Long Barrow = \\n']\n",
      "145 ['\\n = Soviet cruiser Krasnyi Kavkaz = \\n']\n",
      "147 ['\\n = Rhode Island Route 4 = \\n']\n",
      "149 ['\\n = West End Girls = \\n']\n",
      "152 ['\\n = Wrapped in Red = \\n']\n",
      "158 [\"\\n = Christmas 1994 nor 'easter = \\n\"]\n",
      "160 ['\\n = Sholay = \\n']\n",
      "168 ['\\n = Adam Stansfield = \\n']\n",
      "170 ['\\n = Saprang Kalayanamitr = \\n']\n",
      "176 ['\\n = Grammy Award for Best Concept Music Video = \\n']\n",
      "177 ['\\n = Hadji Ali = \\n']\n",
      "181 ['\\n = Loose ( Nelly Furtado album ) = \\n']\n",
      "187 ['\\n = 2013 – 14 York City F.C. season = \\n']\n",
      "191 ['\\n = Antimony = \\n']\n",
      "197 ['\\n = Mortimer Wheeler = \\n']\n",
      "211 ['\\n = Species of Allosaurus = \\n']\n",
      "214 ['\\n = Astraeus hygrometricus = \\n']\n",
      "218 ['\\n = Paul Thomas Anderson = \\n']\n",
      "222 ['\\n = The Fox, the Wolf and the Husbandman = \\n']\n",
      "224 ['\\n = Joe Nathan = \\n']\n",
      "228 ['\\n = Art Ross = \\n']\n",
      "233 ['\\n = Saint Leonard Catholic Church ( Madison, Nebraska ) = \\n']\n",
      "237 ['\\n = Portuguese ironclad Vasco da Gama = \\n']\n",
      "238 ['\\n = Nicole Franklin = \\n']\n",
      "244 [\"\\n = Livin'the Dream = \\n\"]\n",
      "246 ['\\n = Toniná = \\n']\n",
      "250 ['\\n = Central Area Command ( RAAF ) = \\n']\n",
      "251 ['\\n = Corn crake = \\n']\n",
      "256 ['\\n = Acute myeloid leukemia = \\n']\n",
      "261 ['\\n = Love Me Like You = \\n']\n",
      "264 ['\\n = Shaoguan incident = \\n']\n",
      "266 ['\\n = Galveston, Texas = \\n']\n",
      "276 ['\\n = Sarnia = \\n']\n",
      "284 ['\\n = French cruiser Sully = \\n']\n",
      "285 ['\\n = Norman Finkelstein = \\n']\n",
      "296 ['\\n = Mutinus elegans = \\n']\n",
      "298 ['\\n = The Boat Race 1900 = \\n']\n",
      "299 ['\\n = Ten Commandments in Catholic theology = \\n']\n",
      "309 ['\\n = Yamaha NS @-@ 10 = \\n']\n",
      "311 ['\\n = Utah State Route 61 = \\n']\n",
      "312 ['\\n = <unk> = \\n']\n",
      "315 ['\\n = Edward Creutz = \\n']\n",
      "318 ['\\n = Leanne Del Toso = \\n']\n",
      "319 ['\\n = No. 79 Wing RAAF = \\n']\n",
      "320 ['\\n = Vitamin D ( Glee ) = \\n']\n",
      "322 ['\\n = Fern Hobbs = \\n']\n",
      "324 ['\\n = Jessie Stephen = \\n']\n",
      "325 ['\\n = Of Human Feelings = \\n']\n",
      "328 ['\\n = Dangerously in Love Tour = \\n']\n",
      "330 ['\\n = Zhou Tong ( archer ) = \\n']\n",
      "339 ['\\n = Romanian Land Forces = \\n']\n",
      "344 ['\\n = Not Quite Hollywood : The Wild, Untold Story of Ozploitation! = \\n']\n",
      "345 ['\\n = Why Does It Hurt So Bad = \\n']\n",
      "347 ['\\n = Hurricane Omar ( 2008 ) = \\n']\n",
      "350 ['\\n = Papal conclave, 1769 = \\n']\n",
      "355 ['\\n = West Hendford Cricket Ground, Yeovil = \\n']\n",
      "356 [\"\\n = New Year's Eve ( Up All Night ) = \\n\"]\n",
      "357 ['\\n = World War Z = \\n']\n",
      "361 ['\\n = Sentence spacing = \\n']\n",
      "365 ['\\n = The Crab with the Golden Claws = \\n']\n",
      "369 ['\\n = L.A.M.B. = \\n']\n",
      "371 ['\\n = First @-@ move advantage in chess = \\n']\n",
      "381 ['\\n = Frederick Reines = \\n']\n",
      "384 ['\\n = Lock Haven, Pennsylvania = \\n']\n",
      "391 ['\\n = Rachel Green = \\n']\n",
      "401 ['\\n = Krak des Chevaliers = \\n']\n",
      "407 ['\\n = The Importance of Being Earnest = \\n']\n",
      "416 ['\\n = Lloyd Mathews = \\n']\n",
      "419 ['\\n = HMS Boreas ( <unk> ) = \\n']\n",
      "421 ['\\n = Kaimanawa horse = \\n']\n",
      "423 ['\\n = The Remix ( Lady Gaga album ) = \\n']\n",
      "425 ['\\n = Architecture of the Song dynasty = \\n']\n",
      "430 ['\\n = Lost Horizons ( Lemon Jelly album ) = \\n']\n",
      "432 ['\\n = Fastra II = \\n']\n",
      "433 ['\\n = USS Breese ( DD @-@ 122 ) = \\n']\n",
      "435 ['\\n = Sandwich Day = \\n']\n",
      "436 ['\\n = Tiber Oil Field = \\n']\n",
      "437 ['\\n = Glorious First of June = \\n']\n",
      "446 ['\\n = New York State Route 368 = \\n']\n",
      "447 ['\\n = M @-@ 122 ( Michigan highway ) = \\n', '\\n = Tupolev Tu @-@ 12 = \\n']\n",
      "453 ['\\n = Ireland = \\n']\n",
      "471 ['\\n = St Nazaire Raid = \\n']\n",
      "479 ['\\n = Hellblazer = \\n']\n",
      "487 ['\\n = Curtis Woodhouse = \\n']\n",
      "491 ['\\n = 2010 Haiti earthquake = \\n']\n",
      "504 ['\\n = De <unk> a <unk> : interviews with persons affected by the 2010 Haiti earthquake = \\n', '\\n = Thom Darden = \\n']\n",
      "505 ['\\n = Voyage : Inspired by Jules Verne = \\n']\n",
      "507 ['\\n = Old Baltimore Pike = \\n']\n",
      "508 ['\\n = Mega Man & Bass = \\n']\n",
      "511 ['\\n = Ohio State Route 319 = \\n', '\\n = Parliament Act 1911 = \\n']\n",
      "514 ['\\n = Hibiscus ( restaurant ) = \\n']\n",
      "516 ['\\n = Chris Turner ( American football ) = \\n']\n",
      "519 ['\\n = Jack and Jill ( nursery rhyme ) = \\n']\n",
      "520 ['\\n = Florida State Road 878 = \\n']\n",
      "522 ['\\n = James Nesbitt = \\n']\n",
      "531 ['\\n = Crazy in Love = \\n']\n",
      "539 ['\\n = Moro River Campaign = \\n']\n",
      "545 ['\\n = Berkley Bedell = \\n']\n",
      "548 ['\\n = Bart vs. Australia = \\n']\n",
      "550 ['\\n = Leslie Andrew = \\n']\n",
      "552 ['\\n = Rebbie Jackson = \\n']\n",
      "555 ['\\n = Hugh Foliot = \\n']\n",
      "556 ['\\n = AIL Storm = \\n']\n",
      "558 ['\\n = 1940 Atlantic hurricane season = \\n']\n",
      "563 ['\\n = Ode to a Nightingale = \\n']\n",
      "570 ['\\n = Weather buoy = \\n']\n",
      "572 ['\\n = HMS Marlborough ( 1912 ) = \\n']\n",
      "577 ['\\n = 766th Independent Infantry Regiment ( North Korea ) = \\n']\n",
      "581 ['\\n = Sister Wives = \\n']\n",
      "585 ['\\n = You Only Live Twice ( film ) = \\n']\n",
      "589 ['\\n = WASP @-@ 13b = \\n']\n",
      "590 ['\\n = U2 concert in Sarajevo = \\n']\n",
      "596 ['\\n = Frank Slide = \\n']\n",
      "600 ['\\n = Protein = \\n']\n",
      "607 ['\\n = LiSA ( Japanese musician, born 1987 ) = \\n']\n",
      "610 ['\\n = Aston Villa F.C. = \\n']\n",
      "616 ['\\n = Pattycake ( gorilla ) = \\n']\n",
      "618 ['\\n = Lactarius indigo = \\n']\n",
      "621 [\"\\n = You're Gonna Love Tomorrow = \\n\"]\n",
      "624 ['\\n = Fear of Flying ( The Simpsons ) = \\n']\n",
      "626 ['\\n = Harold Innis = \\n']\n",
      "633 ['\\n = Hurricane Lorenzo ( 2007 ) = \\n']\n",
      "635 ['\\n = Cadmium = \\n']\n",
      "636 ['\\n = 14 @.@ 1 years ), <unk> ( t1 / 2 = \\n']\n",
      "640 ['\\n = First Battle of Maryang San = \\n']\n",
      "648 ['\\n = Ulysses ( poem ) = \\n']\n",
      "653 ['\\n = The Food Album = \\n']\n",
      "654 ['\\n = Patriarchal Cathedral of the Holy Ascension of God = \\n']\n",
      "656 ['\\n = Daydream ( Mariah Carey album ) = \\n']\n",
      "663 ['\\n = Leg before wicket = \\n']\n",
      "668 ['\\n = The Family Jewels ( Marina and the Diamonds album ) = \\n']\n",
      "671 ['\\n = 1981 Peach Bowl ( January ) = \\n']\n",
      "676 ['\\n = The Magdalen Reading = \\n']\n",
      "680 [\"\\n = Rosemary's Baby ( 30 Rock ) = \\n\"]\n",
      "682 ['\\n = Polka Party! = \\n']\n",
      "685 ['\\n = Trees ( poem ) = \\n']\n",
      "690 ['\\n = Zygoballus sexpunctatus = \\n']\n",
      "691 ['\\n = 1986 Peach Bowl = \\n']\n",
      "697 ['\\n = Greens Ledge Light = \\n']\n",
      "699 ['\\n = Action of 13 September 1810 = \\n']\n",
      "702 ['\\n = M @-@ 114 ( Michigan highway ) = \\n']\n",
      "703 ['\\n = Jane Dudley, Duchess of Northumberland = \\n']\n",
      "705 ['\\n = Elgin Cathedral = \\n']\n",
      "713 [\"\\n = St Mary's Church, Nether Alderley = \\n\"]\n",
      "714 ['\\n = Tawny nurse shark = \\n']\n",
      "718 ['\\n = California State Route 243 = \\n']\n",
      "719 ['\\n = The Amps = \\n']\n",
      "720 ['\\n = Exploration of Jupiter = \\n']\n",
      "725 ['\\n = Fort Glanville Conservation Park = \\n']\n",
      "734 ['\\n = Royal prerogative in the United Kingdom = \\n']\n",
      "738 ['\\n = Mount Jackson ( Antarctica ) = \\n']\n",
      "739 ['\\n = Italian cruiser Aretusa = \\n']\n",
      "741 ['\\n = M @-@ 6 ( Michigan highway ) = \\n']\n",
      "745 ['\\n = Hi, Infidelity = \\n']\n",
      "747 ['\\n = Ceratopsia = \\n']\n",
      "752 ['\\n = Truth in Numbers? = \\n']\n",
      "754 ['\\n = Hope Highway = \\n']\n",
      "755 ['\\n = Super Mario Land = \\n']\n",
      "757 ['\\n = Stop!! Hibari @-@ kun! = \\n']\n",
      "761 ['\\n = Guitar Hero = \\n']\n",
      "774 ['\\n = <unk> = \\n']\n",
      "776 ['\\n = Type 94 Nambu pistol = \\n']\n",
      "778 ['\\n = Jifna = \\n']\n",
      "783 [\"\\n = Ha'K 'in Xook = \\n\"]\n",
      "785 ['\\n = Tommy Lawton = \\n']\n",
      "791 ['\\n = Trials and Tribble @-@ ations = \\n']\n",
      "795 ['\\n = Tintin in the Congo = \\n']\n",
      "801 ['\\n = Andrew Johnston ( singer ) = \\n']\n",
      "803 ['\\n = Illinois ( Sufjan Stevens album ) = \\n']\n",
      "808 ['\\n = Mycena galericulata = \\n']\n",
      "810 ['\\n = Crash Boom Bang! = \\n']\n",
      "812 ['\\n = Grade I listed buildings in Somerset = \\n']\n",
      "816 ['\\n = Gertrude Barrows Bennett = \\n']\n",
      "818 ['\\n = Man Down ( song ) = \\n']\n",
      "823 ['\\n = Marauders ( Star Trek : Enterprise ) = \\n']\n",
      "825 ['\\n = Johnny McNichol = \\n']\n",
      "828 ['\\n = Otra Nota = \\n']\n",
      "829 [\"\\n = St Peulan's Church, Llanbeulan = \\n\"]\n",
      "831 ['\\n = The Tramp Dentists = \\n', '\\n = Qedarite = \\n']\n",
      "837 ['\\n = Super Science Stories = \\n']\n",
      "842 ['\\n = Rocky Mountain Horse = \\n']\n",
      "843 ['\\n = Somerset County Cricket Club in 2009 = \\n']\n",
      "845 ['\\n = Played, W = \\n', '\\n = Losses, D = \\n', '\\n = Ties, A = \\n', '\\n = Batting points, <unk> = \\n', '\\n = Adjustments / Penalties, Pts = \\n']\n",
      "846 ['\\n = Played, W = \\n', '\\n = Ties, L = \\n', '\\n = No result, Pts = \\n']\n",
      "847 ['\\n = Played, W = \\n', '\\n = Losses, T = \\n', '\\n = No result, Pts = \\n', '\\n = Played, W = \\n', '\\n = Ties, L = \\n', '\\n = No result, Pts = \\n']\n",
      "848 ['\\n = Played, W = \\n', '\\n = Losses, T = \\n', '\\n = No result, Pts = \\n', '\\n = Texas A & M Singing Cadets = \\n']\n",
      "850 ['\\n = Ontario Highway 36 = \\n']\n",
      "851 ['\\n = Arizona State Route 67 = \\n']\n",
      "852 ['\\n = Josce de Dinan = \\n']\n",
      "853 ['\\n = World War I Memorial ( East Providence, Rhode Island ) = \\n']\n",
      "854 ['\\n = Oldham = \\n']\n",
      "865 ['\\n = 1981 European Cup Final = \\n']\n",
      "867 [\"\\n = Carre's Grammar School = \\n\"]\n",
      "873 [\"\\n = Don 't You Wanna Stay = \\n\"]\n",
      "876 ['\\n = Tropical Storm Domoina = \\n']\n",
      "879 ['\\n = Tales of Destiny 2 = \\n']\n",
      "884 ['\\n = In Bloom = \\n']\n",
      "886 ['\\n = Lady in the Lake trial = \\n']\n",
      "891 ['\\n = Dover Athletic F.C. = \\n']\n",
      "894 ['\\n = Plum cake = \\n']\n",
      "896 ['\\n = Kedok Ketawa = \\n']\n",
      "897 ['\\n = My Boo ( Usher and Alicia Keys song ) = \\n']\n",
      "899 ['\\n = Rio de Janeiro bid for the 2016 Summer Olympics = \\n']\n",
      "906 ['\\n = New Jersey Route 65 = \\n']\n",
      "907 ['\\n = Giacomo Meyerbeer = \\n']\n",
      "917 ['\\n = Washington State Route 221 = \\n', '\\n = Superman : Escape from Krypton = \\n']\n",
      "919 ['\\n = Battle of Hubbardton = \\n']\n",
      "921 ['\\n = Odaenathus = \\n']\n",
      "929 ['\\n = Banksia violacea = \\n']\n",
      "932 ['\\n = Rob Howard = \\n']\n",
      "934 ['\\n = Oribi = \\n']\n",
      "936 ['\\n = Rockstar 101 = \\n']\n",
      "939 ['\\n = First Light ( Rebecca Stead novel ) = \\n']\n",
      "941 ['\\n = Mexico City Metropolitan Cathedral = \\n']\n",
      "950 ['\\n = USS Illinois ( BB @-@ 7 ) = \\n']\n",
      "952 ['\\n = The Archaeology of Ritual and Magic = \\n']\n",
      "954 ['\\n = History of Braathens SAFE ( 1946 – 93 ) = \\n']\n",
      "966 ['\\n = Gerard ( archbishop of York ) = \\n']\n",
      "968 ['\\n = Something Borrowed ( Torchwood ) = \\n']\n",
      "973 ['\\n = Perfect Dark ( 2010 video game ) = \\n']\n",
      "975 ['\\n = First Ostend Raid = \\n']\n",
      "978 ['\\n = Joyful, Joyful = \\n']\n",
      "979 ['\\n = Hurricane Dot ( 1959 ) = \\n']\n",
      "980 ['\\n = Jacob deGrom = \\n']\n",
      "983 ['\\n = Battle of Merville Gun Battery = \\n']\n",
      "985 [\"\\n = St Caffo's Church, Llangaffo = \\n\"]\n",
      "987 ['\\n = George N. Briggs = \\n']\n",
      "989 ['\\n = Simon Bradstreet = \\n']\n",
      "992 ['\\n = Etymology of Wicca = \\n']\n",
      "996 ['\\n = Bob Dylan = \\n']\n",
      "1016 ['\\n = Stephanolepis cirrhifer = \\n', '\\n = Mogadishu = \\n']\n",
      "1029 ['\\n = The Moth ( Lost ) = \\n']\n",
      "1031 ['\\n = Charmbracelet = \\n']\n",
      "1037 ['\\n = Thomas Quiney = \\n']\n",
      "1040 ['\\n = Transit of Venus = \\n']\n",
      "1044 ['\\n = Ímar mac Arailt = \\n']\n",
      "1047 ['\\n = Tessa Noël = \\n']\n",
      "1054 ['\\n = M @-@ 5 ( Michigan highway ) = \\n']\n",
      "1057 ['\\n = McAllister Tower Apartments = \\n']\n",
      "1059 ['\\n = Karamokho Alfa = \\n']\n",
      "1061 ['\\n = Murder of Tom ap Rhys Pryce = \\n']\n",
      "1064 ['\\n = Lágrimas Cálidas = \\n']\n",
      "1065 ['\\n = Roger Federer = \\n']\n",
      "1078 [\"\\n = Jane's Attack Squadron = \\n\"]\n",
      "1082 ['\\n = 1955 Atlantic hurricane season = \\n']\n",
      "1088 ['\\n = <unk> = \\n']\n",
      "1091 ['\\n = Stuart McCall = \\n']\n",
      "1098 ['\\n = Black @-@ tailed jackrabbit = \\n']\n",
      "1102 ['\\n = Battle of Romani = \\n']\n",
      "1119 ['\\n = The Litigators = \\n']\n",
      "1123 ['\\n = Stanley Green = \\n']\n",
      "1124 ['\\n = Sclerodermatineae = \\n']\n",
      "1126 ['\\n = SM U @-@ 3 ( Austria @-@ Hungary ) = \\n']\n",
      "1127 ['\\n = The Son Also Draws = \\n']\n",
      "1129 ['\\n = Protomycena = \\n']\n",
      "1130 ['\\n = Art in Medieval Scotland = \\n']\n",
      "1134 ['\\n = Humpty Dumpty = \\n']\n",
      "1137 ['\\n = Welsh National Opera = \\n']\n",
      "1141 ['\\n = England national rugby union team = \\n']\n",
      "1149 ['\\n = Jeremi Wiśniowiecki = \\n']\n",
      "1154 ['\\n = Bassline ( Chris Brown song ) = \\n']\n",
      "1155 ['\\n = HMS Black Prince ( 1904 ) = \\n']\n",
      "1156 ['\\n = On the Pulse of Morning = \\n']\n",
      "1159 [\"\\n = God's Choice = \\n\"]\n",
      "1161 ['\\n = Baltimore mayoral election, 1999 = \\n']\n",
      "1164 ['\\n = Arikamedu = \\n']\n",
      "1167 ['\\n = Hurricane Ingrid = \\n']\n",
      "1170 ['\\n = Typhoon Imbudo = \\n']\n",
      "1173 ['\\n = Hurricane Felicia ( 2009 ) = \\n']\n",
      "1175 ['\\n = Santa @-@ Fe ( Bob Dylan song ) = \\n']\n",
      "1178 ['\\n = The Secret ( The Office ) = \\n']\n",
      "1180 ['\\n = Maggie Simpson = \\n']\n",
      "1182 ['\\n = Chasing Vermeer = \\n']\n",
      "1184 ['\\n = Caught Up ( Usher song ) = \\n']\n",
      "1185 ['\\n = If I Never See Your Face Again = \\n']\n",
      "1188 ['\\n = Yoko Shimomura = \\n']\n",
      "1190 ['\\n = Lisa the Simpson = \\n']\n",
      "1191 ['\\n = Stay @-@ at @-@ home dad = \\n']\n",
      "1196 ['\\n = Tristan ( horse ) = \\n']\n",
      "1199 ['\\n = Plunketts Creek ( Loyalsock Creek ) = \\n']\n",
      "1205 ['\\n = Richard Cresswell = \\n']\n",
      "1208 ['\\n = Berhtwald = \\n']\n",
      "1210 ['\\n = Xenon = \\n']\n",
      "1218 ['\\n = Eva Perón = \\n']\n",
      "1230 ['\\n = Halo : Uprising = \\n']\n",
      "1232 ['\\n = Species ( film ) = \\n']\n",
      "1237 ['\\n = John of Brienne = \\n']\n",
      "1243 ['\\n = Boise National Forest = \\n']\n",
      "1249 ['\\n = Pokiri = \\n']\n",
      "1254 ['\\n = 2008 Bahrain Grand Prix = \\n']\n",
      "1257 ['\\n = Kalyanasundara = \\n']\n",
      "1259 ['\\n = SS El Sol = \\n']\n",
      "1260 ['\\n = Hoover Dam = \\n']\n",
      "1270 ['\\n = Tropical Storm Abby ( 1964 ) = \\n']\n",
      "1271 ['\\n = 7 Independent Company ( Rhodesia ) = \\n']\n",
      "1275 ['\\n = Jenova Chen = \\n']\n",
      "1278 ['\\n = E. W. Hornung = \\n']\n",
      "1284 ['\\n = Nina Simone = \\n']\n",
      "1289 ['\\n = Vistara = \\n']\n",
      "1291 ['\\n = Portrait of Monsieur Bertin = \\n']\n",
      "1295 ['\\n = Harajuku Lovers Tour = \\n']\n",
      "1297 ['\\n = Laborintus II ( 2012 recording ) = \\n']\n",
      "1299 ['\\n = Residence of the United States Ambassador to the United Nations = \\n']\n",
      "1300 ['\\n = K @-@ 22 ( Kansas highway ) = \\n']\n",
      "1301 ['\\n = Tropical Storm Jose ( 2005 ) = \\n']\n",
      "1302 ['\\n = Norsk Spisevognselskap = \\n']\n",
      "1304 [\"\\n = The Actor's Children = \\n\"]\n",
      "1306 ['\\n = The Stolen Eagle = \\n']\n",
      "1310 ['\\n = Roxas ( Kingdom Hearts ) = \\n']\n",
      "1314 ['\\n = Back to Tennessee ( song ) = \\n']\n",
      "1316 ['\\n = M @-@ 108 ( Michigan highway ) = \\n']\n",
      "1317 ['\\n = Blackwyche = \\n']\n",
      "1318 ['\\n = Hoyt Wilhelm = \\n']\n",
      "1322 ['\\n = Politics of Croatia = \\n']\n",
      "1326 ['\\n = Mumia Abu @-@ Jamal = \\n']\n",
      "1333 ['\\n = Brandon Minor = \\n']\n",
      "1335 [\"\\n = Boy @-@ Scoutz'n the Hood = \\n\"]\n",
      "1338 ['\\n = Last Exit on Brooklyn = \\n', '\\n = Laurence Olivier = \\n']\n",
      "1353 ['\\n = Freakum Dress = \\n']\n",
      "1357 ['\\n = Derfflinger @-@ class battlecruiser = \\n']\n",
      "1363 [\"\\n = O 'Brien @-@ class destroyer = \\n\"]\n",
      "1367 ['\\n = Who Am I ( Casting Crowns song ) = \\n']\n",
      "1369 [\"\\n = We 'll Always Have Paris ( Star Trek : The Next Generation ) = \\n\"]\n",
      "1371 ['\\n = Burn = \\n']\n",
      "1376 ['\\n = Tropical Storm Brenda ( 1960 ) = \\n']\n",
      "1377 ['\\n = Stanley Price Weir = \\n']\n",
      "1380 ['\\n = Chapter 1 ( House of Cards ) = \\n']\n",
      "1383 ['\\n = Alice in Chains = \\n']\n",
      "1391 ['\\n = 2007 Hawaii Bowl = \\n']\n",
      "1396 ['\\n = The Boat Race 1999 = \\n']\n",
      "1397 ['\\n = Hoysala literature = \\n']\n",
      "1405 ['\\n = 1939 Pacific hurricane season = \\n']\n",
      "1406 ['\\n = G.I. Joe : Retaliation = \\n']\n",
      "1412 [\"\\n = Burns'Heir = \\n\"]\n",
      "1413 ['\\n = Cougar = \\n']\n",
      "1423 ['\\n = New York State Route 164 = \\n', '\\n = Sorraia = \\n']\n",
      "1425 ['\\n = Haifa = \\n']\n",
      "1437 ['\\n = New York State Route 185 = \\n', '\\n = Fernando Torres = \\n']\n",
      "1445 ['\\n = Mark Stockwell = \\n']\n",
      "1447 ['\\n = Dota 2 = \\n']\n",
      "1453 [\"\\n = Don 't Take It Personally, Babe, It Just Ain 't Your Story = \\n\"]\n",
      "1455 ['\\n = The Dreamscape = \\n']\n",
      "1457 ['\\n = True Blue ( Madonna song ) = \\n']\n",
      "1460 ['\\n = Invisible rail = \\n']\n",
      "1462 ['\\n = Project Chanology = \\n']\n",
      "1474 ['\\n = Transportation in Omaha = \\n']\n",
      "1479 ['\\n = Clocks ( song ) = \\n']\n",
      "1481 ['\\n = Khoo Kheng @-@ Hor = \\n']\n",
      "1484 ['\\n = The Wave ( Miike Snow song ) = \\n']\n",
      "1486 ['\\n = Washington State Route 516 = \\n']\n",
      "1487 ['\\n = Stefan Wever = \\n']\n",
      "1488 ['\\n = Djedkare Isesi = \\n']\n",
      "1495 ['\\n = Route 261 ( Delaware – Pennsylvania ) = \\n']\n",
      "1496 ['\\n = Comair Flight 5191 = \\n']\n",
      "1499 ['\\n = Berg ( station ) = \\n']\n",
      "1500 ['\\n = Adams River ( British Columbia ) = \\n']\n",
      "1505 ['\\n = The One I Love ( manga ) = \\n']\n",
      "1507 ['\\n = Hurricane Abby ( 1960 ) = \\n']\n",
      "1508 ['\\n = 2010 Alabama Crimson Tide football team = \\n']\n",
      "1516 ['\\n = James Robert Baker = \\n']\n",
      "1519 ['\\n = Ernie Cooksey = \\n']\n",
      "1521 [\"\\n = Simon de Montfort's Parliament = \\n\"]\n",
      "1523 [\"\\n = USS O 'Brien ( DD @-@ 51 ) = \\n\"]\n",
      "1525 ['\\n = Christine Hakim = \\n']\n",
      "1528 ['\\n = A Month in the Country ( film ) = \\n']\n",
      "1530 ['\\n = Hydnellum peckii = \\n']\n",
      "1533 ['\\n = Gregory Helms = \\n']\n",
      "1538 ['\\n = New York State Route 38 = \\n']\n",
      "1542 ['\\n = Westminster Assembly = \\n']\n",
      "1549 ['\\n = The Goat Puzzle = \\n']\n",
      "1550 ['\\n = Sinclair Sovereign = \\n']\n",
      "1551 ['\\n = Survivor Series ( 1992 ) = \\n']\n",
      "1555 ['\\n = Ouw Peh Tjoa = \\n']\n",
      "1556 ['\\n = HMS Comet ( <unk> ) = \\n']\n",
      "1559 ['\\n = History of artificial intelligence = \\n']\n",
      "1567 ['\\n = Cyclone Graham = \\n']\n",
      "1568 ['\\n = Languedoc @-@ Roussillon wine = \\n']\n",
      "1571 ['\\n = Silver Bullet ( roller coaster ) = \\n']\n",
      "1572 ['\\n = Blackburn Firecrest = \\n']\n",
      "1574 ['\\n = Magadheera = \\n']\n",
      "1585 ['\\n = Hurricane Flossy ( 1956 ) = \\n']\n",
      "1587 ['\\n = Battle of Binh Gia = \\n']\n",
      "1591 ['\\n = June 1941 uprising in eastern Herzegovina = \\n']\n",
      "1599 ['\\n = Crush ( video game ) = \\n']\n",
      "1602 ['\\n = Odyssey Number Five = \\n']\n",
      "1605 ['\\n = Amanita muscaria = \\n']\n",
      "1612 ['\\n = Burning of women in England = \\n']\n",
      "1615 ['\\n = Strand, London = \\n']\n",
      "1620 ['\\n = President Evil = \\n']\n",
      "1621 ['\\n = Dave Sisler = \\n']\n",
      "1624 ['\\n = Cole Hamels = \\n']\n",
      "1631 ['\\n = Kyra ( Charmed ) = \\n']\n",
      "1633 ['\\n = Réunion ibis = \\n']\n",
      "1638 ['\\n = December 1964 South Vietnamese coup = \\n']\n",
      "1646 ['\\n = 1998 National League Wild Card tie @-@ breaker game = \\n']\n",
      "1648 ['\\n = Corpus Christi Bay = \\n']\n",
      "1651 ['\\n = 1990 Pacific hurricane season = \\n']\n",
      "1658 ['\\n = Track and field = \\n']\n",
      "1672 ['\\n = Isabella Beeton = \\n']\n",
      "1677 ['\\n = Martin Keamy = \\n']\n",
      "1680 ['\\n = Kaboom ( Parks and Recreation ) = \\n']\n",
      "1682 ['\\n = Middle Colonies = \\n']\n",
      "1685 ['\\n = Bath Assembly Rooms = \\n']\n",
      "1687 ['\\n = Flash Gordon Strange Adventure Magazine = \\n']\n",
      "1688 ['\\n = Tropical Storm Olaf ( 1997 ) = \\n']\n",
      "1689 ['\\n = Road to the North Pole = \\n']\n",
      "1695 ['\\n = Spanish Hill = \\n']\n",
      "1696 ['\\n = S.R. 819 = \\n']\n",
      "1699 ['\\n = Paranthodon = \\n']\n",
      "1701 ['\\n = History of Bradford City A.F.C. = \\n']\n",
      "1707 ['\\n = Aerith Gainsborough = \\n']\n",
      "1710 ['\\n = Forbidden Fruit ( J. Cole song ) = \\n']\n",
      "1711 ['\\n = Old Pine Church = \\n']\n",
      "1715 ['\\n = God of War video game collections = \\n']\n",
      "1717 ['\\n = Territorial era of Minnesota = \\n']\n",
      "1727 ['\\n = Mitsuyo Maeda = \\n']\n",
      "1732 ['\\n = Kitsune = \\n']\n",
      "1737 ['\\n = New York State Route 448 = \\n']\n",
      "1738 ['\\n = Hurricane Tanya ( 1995 ) = \\n']\n",
      "1740 ['\\n = Sweet Love ( Chris Brown song ) = \\n']\n",
      "1741 ['\\n = Kakapo = \\n']\n",
      "1750 ['\\n = Live & Kicking = \\n']\n",
      "1752 ['\\n = Henry Hoʻolulu Pitman = \\n']\n",
      "1756 ['\\n = Church of Christ Pantocrator, Nesebar = \\n']\n",
      "1757 [\"\\n = Kir 'Shara = \\n\"]\n",
      "1760 ['\\n = <unk> Warren = \\n']\n",
      "1761 ['\\n = 2nd Battalion 9th Marines = \\n']\n",
      "1765 ['\\n = Directed acyclic graph = \\n']\n",
      "1770 ['\\n = August ( Fringe ) = \\n']\n",
      "1776 ['\\n = Yo @-@ Yo ( Nicola Roberts song ) = \\n']\n",
      "1778 ['\\n = Robbie Fowler = \\n']\n",
      "1786 ['\\n = Ode on Indolence = \\n']\n",
      "1789 ['\\n = Erving Goffman = \\n']\n",
      "1796 ['\\n = Cater 2 U = \\n']\n",
      "1800 ['\\n = Marshall Applewhite = \\n']\n",
      "1807 ['\\n = Arbeideren ( Hamar ) = \\n']\n",
      "1810 ['\\n = Journey ( 2012 video game ) = \\n']\n",
      "1815 ['\\n = Gold Beach = \\n']\n",
      "1822 ['\\n = Tautiška giesmė = \\n']\n",
      "1824 ['\\n = Crosby Garrett Helmet = \\n']\n",
      "1827 ['\\n = Liu Kang = \\n']\n",
      "1832 ['\\n = Inocybe praetervisa = \\n']\n",
      "1834 ['\\n = Hannah Primrose, Countess of Rosebery = \\n']\n",
      "1843 ['\\n = Hurricane Uleki = \\n']\n",
      "1844 ['\\n = 2016 Spanish Grand Prix = \\n']\n",
      "1849 ['\\n = Loverboy ( Mariah Carey song ) = \\n']\n",
      "1853 ['\\n = Allah = \\n']\n",
      "1857 ['\\n = Moment of Surrender = \\n']\n",
      "1860 ['\\n = Gaboon viper = \\n']\n",
      "1863 ['\\n = The Sixth Extinction = \\n']\n",
      "1866 ['\\n = Snow ( visual novel ) = \\n']\n",
      "1868 ['\\n = Miss Meyers = \\n']\n",
      "1869 ['\\n = Far Away Places ( Mad Men ) = \\n']\n",
      "1872 ['\\n = Awakening ( Star Trek : Enterprise ) = \\n']\n",
      "1874 ['\\n = Mole cricket = \\n']\n",
      "1879 ['\\n = Maryland Route 194 = \\n']\n",
      "1882 ['\\n = Jin – Song Wars = \\n']\n",
      "1895 ['\\n = T30 Howitzer Motor Carriage = \\n']\n",
      "1896 ['\\n = Memory Almost Full = \\n']\n",
      "1899 ['\\n = Imagine ( John Lennon song ) = \\n']\n",
      "1904 ['\\n = Big Boy ( song ) = \\n']\n",
      "1905 ['\\n = Chagas disease = \\n']\n",
      "1910 ['\\n = Diamond stingray = \\n']\n",
      "1912 ['\\n = George Calvert, 1st Baron Baltimore = \\n']\n",
      "1919 ['\\n = Draining and development of the Everglades = \\n']\n",
      "1929 ['\\n = DuMont Television Network = \\n']\n",
      "1936 ['\\n = Bodyline = \\n']\n",
      "1943 ['\\n = Waterfall Gully, South Australia = \\n']\n",
      "1947 ['\\n = Mariana ( poem ) = \\n']\n",
      "1950 ['\\n = Mothers of the Disappeared = \\n']\n",
      "1954 ['\\n = Katherine Pulaski = \\n']\n",
      "1956 ['\\n = Noisy miner = \\n']\n",
      "1965 ['\\n = Principe Amedeo @-@ class ironclad = \\n']\n",
      "1966 ['\\n = Cell nucleus = \\n']\n",
      "1974 ['\\n = Until the Whole World Hears = \\n']\n",
      "1976 ['\\n = Ceres ( dwarf planet ) = \\n']\n",
      "1979 ['\\n = 10 @.@ 6 ° compared to 7 ° for Mercury and 17 ° for Pluto ) and moderately eccentric ( e = \\n']\n",
      "1983 ['\\n = Skye = \\n']\n",
      "1990 ['\\n = Florida Atlantic University = \\n']\n",
      "1998 ['\\n = The Clean Tech Revolution = \\n']\n",
      "2000 ['\\n = Missouri River = \\n']\n",
      "2014 [\"\\n = Monty Can 't Buy Me Love = \\n\"]\n",
      "2016 ['\\n = Landing at Anzac Cove = \\n']\n",
      "2026 ['\\n = Michelle Rzepecki = \\n']\n",
      "2027 ['\\n = The Good Terrorist = \\n']\n",
      "2032 ['\\n = Henry of Grosmont, 1st Duke of Lancaster = \\n']\n",
      "2034 ['\\n = I Am Unicorn = \\n']\n",
      "2038 [\"\\n = Galentine's Day = \\n\"]\n",
      "2040 ['\\n = Bossy ( Lindsay Lohan song ) = \\n']\n",
      "2041 ['\\n = Biddenden Maids = \\n']\n",
      "2045 ['\\n = Ælfric of Abingdon = \\n']\n",
      "2046 ['\\n = White Dog ( Gary novel ) = \\n']\n",
      "2047 ['\\n = Condom = \\n']\n",
      "2058 ['\\n = Sovetsky Soyuz @-@ class battleship = \\n']\n",
      "2064 ['\\n = Sang Pencerah = \\n']\n",
      "2066 ['\\n = Underneath ( The X @-@ Files ) = \\n']\n",
      "2068 ['\\n = Islais Creek = \\n']\n",
      "2069 ['\\n = Iguanodon = \\n']\n",
      "2079 ['\\n = Subtropical Storm Alpha ( 1972 ) = \\n']\n",
      "2080 ['\\n = Barbarian II : The Dungeon of Drax = \\n']\n",
      "2083 ['\\n = Van Morrison : Too Late to Stop Now = \\n']\n",
      "2085 ['\\n = Music of Chrono Cross = \\n']\n",
      "2087 ['\\n = M @-@ 47 ( Michigan highway ) = \\n']\n",
      "2089 ['\\n = The Convict ( 1910 film ) = \\n']\n",
      "2100 ['\\n = 130th Engineer Brigade ( United States ) = \\n']\n",
      "2105 ['\\n = Corythosaurus = \\n']\n",
      "2111 ['\\n = Ghost in the Shell : Stand Alone Complex - Solid State Society = \\n']\n",
      "2113 ['\\n = Winston Tunnel = \\n']\n",
      "2115 ['\\n = Wales national rugby union team = \\n']\n",
      "2120 ['\\n = Anekantavada = \\n']\n",
      "2129 ['\\n = Arnhem Oosterbeek War Cemetery = \\n']\n",
      "2130 ['\\n = Ace Attorney = \\n']\n",
      "2135 ['\\n = Cambodian Campaign = \\n']\n",
      "2145 ['\\n = Verpa bohemica = \\n']\n",
      "2147 ['\\n = Varanasi = \\n']\n",
      "2157 ['\\n = Toronto Magnetic and Meteorological Observatory = \\n']\n",
      "2159 ['\\n = 1973 Atlantic hurricane season = \\n']\n",
      "2164 ['\\n = 2 / 4th Machine Gun Battalion ( Australia ) = \\n']\n",
      "2171 ['\\n = Richard Nixon presidential campaign, 1968 = \\n']\n",
      "2177 ['\\n = Cyclone Herbie = \\n']\n",
      "2178 ['\\n = Croatian independence referendum, 1991 = \\n']\n",
      "2180 ['\\n = Georgian scripts = \\n']\n",
      "2185 ['\\n = and = \\n']\n",
      "2186 ['\\n = Banai ( goddess ) = \\n']\n",
      "2189 ['\\n = Europium = \\n', '\\n = 7 / 2 ) suppresses the superconductivity, which is induced by eliminating this local moment ( J = \\n']\n",
      "2193 ['\\n = North @-@ Eastern Area Command ( RAAF ) = \\n']\n",
      "2196 ['\\n = Cape lobster = \\n']\n",
      "2197 ['\\n = A4232 road = \\n']\n",
      "2203 ['\\n = Development of Fez = \\n']\n",
      "2206 ['\\n = Hugh Walpole = \\n']\n",
      "2215 ['\\n = Domnall mac Murchada = \\n']\n",
      "2217 ['\\n = Irresistible ( The X @-@ Files ) = \\n']\n",
      "2219 ['\\n = Thunderbirds ( TV series ) = \\n']\n",
      "2231 ['\\n = T. Arthur Cottam = \\n']\n",
      "2233 ['\\n = Partington = \\n']\n",
      "2235 ['\\n = Key ( basketball ) = \\n']\n",
      "2238 ['\\n = The General in His Labyrinth = \\n']\n",
      "2246 ['\\n = Copia ( museum ) = \\n']\n",
      "2249 ['\\n = Youth on the Prow, and Pleasure at the Helm = \\n']\n",
      "2252 ['\\n = Mozambican War of Independence = \\n']\n",
      "2259 ['\\n = The Secret of Monkey Island = \\n']\n",
      "2264 ['\\n = Temple of Eshmun = \\n']\n",
      "2269 ['\\n = Wilhelm Busch = \\n']\n",
      "2279 ['\\n = Crown Fountain = \\n']\n",
      "2284 ['\\n = Canning Dam = \\n']\n",
      "2287 ['\\n = Midge ( Barbie ) = \\n']\n",
      "2289 ['\\n = The Tempest ( album ) = \\n']\n",
      "2290 ['\\n = Star = \\n']\n",
      "2302 ['\\n = Perry the Platypus = \\n']\n",
      "2306 ['\\n = Amylostereum = \\n']\n",
      "2309 ['\\n = Charles @-@ Valentin Alkan = \\n']\n",
      "2319 ['\\n = Business School ( The Office ) = \\n']\n",
      "2321 ['\\n = M @-@ 81 ( Michigan highway ) = \\n']\n",
      "2322 ['\\n = Common starling = \\n']\n",
      "2332 ['\\n = William McGregor ( football ) = \\n']\n",
      "2335 ['\\n = Climate of Buenos Aires = \\n']\n",
      "2338 ['\\n = Early skyscrapers = \\n']\n",
      "2352 ['\\n = Duke of Edinburgh @-@ class cruiser = \\n']\n",
      "2355 ['\\n = Steve Nash = \\n']\n",
      "2364 ['\\n = U.S. Route 80 in California = \\n']\n",
      "2369 [\"\\n = Harmy's Despecialized Edition = \\n\"]\n",
      "2370 ['\\n = Sense and Sensibility ( 2008 miniseries ) = \\n']\n",
      "2376 ['\\n = Marble Madness = \\n']\n",
      "2379 ['\\n = Adolf Eichmann = \\n']\n",
      "2388 ['\\n = Maryland Route 36 = \\n']\n",
      "2390 ['\\n = Who Wants to Be a Millionaire ( U.S. game show ) = \\n']\n",
      "2402 [\"\\n = Sir Bevil Grenville's Monument = \\n\"]\n",
      "2403 ['\\n = Baby Jesus theft = \\n']\n",
      "2404 ['\\n = Stocksbridge Park Steels F.C. = \\n']\n",
      "2406 ['\\n = Happy Working Song = \\n']\n",
      "2410 ['\\n = John McCain = \\n']\n",
      "2424 ['\\n = Seeley G. Mudd Chemistry Building = \\n']\n",
      "2426 ['\\n = Korketrekkeren = \\n']\n",
      "2428 ['\\n = Ammiraglio di Saint Bon @-@ class battleship = \\n']\n",
      "2430 ['\\n = Jon Challinor = \\n']\n",
      "2433 ['\\n = HMS Glorious = \\n']\n",
      "2438 ['\\n = Morgan dollar = \\n']\n",
      "2442 ['\\n = Keswick, Cumbria = \\n']\n",
      "2451 ['\\n = Drishyam = \\n']\n",
      "2455 ['\\n = Waylon Jennings = \\n']\n",
      "2462 ['\\n = Treehouse of Horror X = \\n']\n",
      "2466 ['\\n = Medal of Honor = \\n']\n",
      "2476 ['\\n = Bacteria = \\n']\n",
      "2487 ['\\n = Janet Jackson ( album ) = \\n']\n",
      "2488 ['\\n = Fanny Bullock Workman = \\n']\n",
      "2496 ['\\n = Grammy Award for Video of the Year = \\n']\n",
      "2497 ['\\n = All of Creation ( song ) = \\n']\n",
      "2498 ['\\n = Assassination of Robert F. Kennedy = \\n']\n",
      "2504 ['\\n = Watchmen = \\n']\n",
      "2516 ['\\n = Ro Laren = \\n']\n",
      "2521 ['\\n = Rock Steady Live = \\n']\n",
      "2523 ['\\n = Kellen Dunham = \\n']\n",
      "2525 ['\\n = Pumpkin bomb = \\n']\n",
      "2526 ['\\n = Barton Aqueduct = \\n']\n",
      "2528 ['\\n = Worlebury Camp = \\n']\n",
      "2530 ['\\n = Daniel Day @-@ Lewis = \\n']\n",
      "2535 ['\\n = LW11 = \\n']\n",
      "2537 ['\\n = Bates method = \\n']\n",
      "2543 ['\\n = Maneater ( 2007 film ) = \\n']\n",
      "2545 ['\\n = SM U @-@ 41 ( Austria @-@ Hungary ) = \\n']\n",
      "2547 ['\\n = Hurricane Carla = \\n']\n",
      "2554 ['\\n = Red Headed Stranger = \\n']\n",
      "2557 ['\\n = Flag of West Virginia = \\n']\n",
      "2561 ['\\n = Battle of Goliad = \\n']\n",
      "2564 ['\\n = Noise in music = \\n']\n",
      "2572 [\"\\n = 2012 – 13 Michigan Wolverines men's basketball team = \\n\"]\n",
      "2580 ['\\n = Prosperity theology = \\n']\n",
      "2587 ['\\n = Newton House, Llandeilo = \\n']\n",
      "2589 ['\\n = Codex Zacynthius = \\n']\n",
      "2591 ['\\n = Ernest Deane = \\n']\n",
      "2593 ['\\n = Kechewaishke = \\n']\n",
      "2598 ['\\n = Captive Heart ( song ) = \\n']\n",
      "2599 ['\\n = Clive Mantle = \\n']\n",
      "2604 ['\\n = Otome wa Boku ni Koishiteru = \\n']\n",
      "2610 ['\\n = Atlantis : The Lost Empire = \\n']\n",
      "2619 [\"\\n = Pam's Replacement = \\n\"]\n",
      "2620 ['\\n = WASP @-@ 43b = \\n']\n",
      "2621 ['\\n = Flag of Poland = \\n']\n",
      "2627 ['\\n = Archimyrmex = \\n']\n",
      "2630 ['\\n = Kīlauea = \\n']\n",
      "2638 ['\\n = Gray wolf = \\n']\n",
      "2656 ['\\n = Irataba = \\n']\n",
      "2664 [\"\\n = Christopher Smart's asylum confinement = \\n\"]\n",
      "2670 ['\\n = My Mother, the Fiend = \\n']\n",
      "2673 ['\\n = Rochdale Town Hall = \\n']\n",
      "2676 ['\\n = Colin McCool = \\n']\n",
      "2680 [\"\\n = Yoshi's Island = \\n\"]\n",
      "2684 ['\\n = Pierce Brosnan = \\n']\n",
      "2689 ['\\n = Charles Kanaʻina = \\n']\n",
      "2692 ['\\n = 2 / 3rd Battalion ( Australia ) = \\n']\n",
      "2698 ['\\n = Cyclone Mala = \\n']\n",
      "2701 ['\\n = George W. Johnson ( governor ) = \\n']\n",
      "2702 ['\\n = Edwin of Northumbria = \\n']\n",
      "2706 ['\\n = Oubliette ( The X @-@ Files ) = \\n']\n",
      "2709 ['\\n = 2000 Spanish Grand Prix = \\n']\n",
      "2713 ['\\n = Liberty Arming the Patriot = \\n']\n",
      "2714 ['\\n = Triaenops menamena = \\n']\n",
      "2717 ['\\n = Henry le Despenser = \\n']\n",
      "2721 ['\\n = Pluteus nevadensis = \\n']\n",
      "2723 ['\\n = Madge Syers = \\n']\n",
      "2724 ['\\n = Senate of the Roman Republic = \\n']\n",
      "2727 ['\\n = Spokane, Washington = \\n']\n",
      "2742 [\"\\n = Fortifications of Xi 'an = \\n\"]\n",
      "2744 ['\\n = Radio ( LL Cool J album ) = \\n']\n",
      "2747 ['\\n = Isaac Newton = \\n']\n",
      "2757 ['\\n = HMS Furious ( 47 ) = \\n']\n",
      "2765 ['\\n = Warning from Space = \\n']\n",
      "2767 ['\\n = Fatbeard = \\n']\n",
      "2770 [\"\\n = Ain 't It Fun ( Paramore song ) = \\n\"]\n",
      "2774 ['\\n = Fishsticks ( South Park ) = \\n']\n",
      "2777 ['\\n = Beatriz Michelena = \\n']\n",
      "2781 ['\\n = Lou Groza = \\n']\n",
      "2784 [\"\\n = O 'Brien Schofield = \\n\"]\n",
      "2786 ['\\n = Paralympic Games = \\n']\n",
      "2793 ['\\n = Maya Lindholm = \\n']\n",
      "2794 ['\\n = Speechless ( Michael Jackson song ) = \\n']\n",
      "2796 ['\\n = North American XB @-@ 21 = \\n']\n",
      "2797 ['\\n = Janee Michelle = \\n']\n",
      "2801 ['\\n = Interstate 95 in Delaware = \\n']\n",
      "2806 ['\\n = Vengeance ( 2005 ) = \\n']\n",
      "2810 ['\\n = Elizabeth Rona = \\n']\n",
      "2814 ['\\n = Interstate 94 in Michigan = \\n']\n",
      "2819 ['\\n = SMS Karlsruhe = \\n']\n",
      "2821 ['\\n = Hurricane Guillermo ( 1997 ) = \\n']\n",
      "2823 ['\\n = 1st Army Group ( Kingdom of Yugoslavia ) = \\n']\n",
      "2828 ['\\n = Open Your Heart ( Madonna song ) = \\n']\n",
      "2833 [\"\\n = Red Rackham's Treasure = \\n\"]\n",
      "2837 ['\\n = God Put a Smile upon Your Face = \\n']\n",
      "2838 ['\\n = M @-@ 66 ( Michigan highway ) = \\n']\n",
      "2842 ['\\n = 15th Sustainment Brigade = \\n']\n",
      "2844 ['\\n = Codex Carolinus = \\n']\n",
      "2845 ['\\n = Mont Aiguille = \\n']\n",
      "2846 ['\\n = German submarine U @-@ 111 ( 1940 ) = \\n']\n",
      "2848 ['\\n = Waxy ( horse ) = \\n']\n",
      "2851 ['\\n = Battle of Antioch ( 218 ) = \\n']\n",
      "2854 ['\\n = Asmara Moerni = \\n']\n",
      "2856 ['\\n = London Country North East = \\n']\n",
      "2858 ['\\n = Badlaa = \\n']\n",
      "2860 ['\\n = Jonathan Jennings = \\n']\n",
      "2869 ['\\n = A More Perfect Union : Advancing New American Rights = \\n']\n",
      "2870 ['\\n = Typhoon Nangka ( 2015 ) = \\n']\n",
      "2872 ['\\n = River Rother, West Sussex = \\n']\n",
      "2878 ['\\n = I Hear You, I See You = \\n']\n",
      "2880 [\"\\n = L 'Hermite's expedition = \\n\"]\n",
      "2882 ['\\n = Biblical Hebrew = \\n']\n",
      "2886 [\"\\n = / <unk> / < * / <unk> /'wine'), while the Southern ( Judean ) dialect instead adds in an epenthetic vowel / i /, added halfway through the first millennium BCE ( <unk> = \\n\"]\n",
      "2888 ['\\n = <unk> versus Rachel <unk> = \\n']\n",
      "2895 ['\\n = <unk> versus <unk> / <unk> / = \\n', \"\\n = Tiberian <unk> Deuteronomy 26 : 15 ) and / a / in Babylonian ( e.g. / <unk> /'item'= \\n\"]\n",
      "2896 ['\\n = Carl Tanzler = \\n']\n",
      "2899 [\"\\n = You 'll Always Find Your Way Back Home = \\n\"]\n",
      "2900 ['\\n = Marjory Stoneman Douglas = \\n']\n",
      "2908 ['\\n = Icelanders = \\n']\n",
      "2912 ['\\n = Canadian federal election, 1957 = \\n']\n",
      "2921 ['\\n = The X @-@ Files : The Album = \\n']\n",
      "2923 ['\\n = Planets beyond Neptune = \\n']\n",
      "2930 ['\\n = Redshift = \\n']\n",
      "2932 ['\\n = 0 ° ), this equation reduces to : \\n <formula> \\n For the special case that the light is approaching at right angles ( θ = \\n']\n",
      "2933 ['\\n = 0 and time t = \\n', '\\n = <unk> in the past and a distant position r = \\n', '\\n = R and is observed at r = \\n', '\\n = <unk> today and a = \\n']\n",
      "2934 ['\\n = 0 @.@ 5, and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun @-@ like spectrum had a redshift of z = \\n']\n",
      "2935 ['\\n = 1089 ( z = \\n']\n",
      "2936 ['\\n = 11 @.@ 1, corresponding to 400 million years after the Big Bang. The previous record was held by <unk> @-@ <unk> at a redshift of z = \\n', '\\n = 7 @.@ 5 and the next highest being z = \\n', '\\n = 8 @.@ 2. The most distant known quasar, <unk> <unk> + <unk>, is at z = \\n', '\\n = 5 @.@ 2 and the highest known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS <unk> + <unk> at z = \\n', '\\n = 1089, corresponding to an age of approximately 379 @,@ 000 years after the Big Bang and a comoving distance of more than 46 billion light years. The yet @-@ to @-@ be @-@ observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to be absorbed almost completely, may have redshifts in the range of 20 < z < 100. Other high @-@ redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang ( and a redshift in excess of z > 1010 ) and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of z > 1025. \\n In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = \\n']\n",
      "2937 ['\\n = Trans @-@ Europe Express ( album ) = \\n']\n",
      "2941 ['\\n = Gabriel García Márquez = \\n']\n",
      "2952 ['\\n = South Forty @-@ Foot Drain = \\n']\n",
      "2957 ['\\n = American Tragedy ( album ) = \\n']\n",
      "2960 ['\\n = Karlsruhe @-@ class cruiser = \\n']\n",
      "2962 ['\\n = Chakotay = \\n']\n",
      "2968 ['\\n = Arthur Bell ( footballer ) = \\n']\n",
      "2969 ['\\n = Domestic sheep reproduction = \\n']\n",
      "2973 ['\\n = Mortal Folly / Mortal Recoil = \\n']\n",
      "2976 ['\\n = Mathematics and architecture = \\n']\n",
      "2983 ['\\n = Tom Hooper = \\n']\n",
      "2988 ['\\n = Ernest II, Duke of Saxe @-@ Coburg and Gotha = \\n']\n",
      "2995 ['\\n = Kersal Moor = \\n']\n",
      "2999 ['\\n = Shamanism in the Qing dynasty = \\n']\n",
      "3004 ['\\n = Exponentiation = \\n']\n",
      "3005 ['\\n = b ⋅ b is called the square of b because the area of a square with side @-@ length b is b2. It is pronounced \" b squared \". \\n The expression b3 = \\n', '\\n = 3 ⋅ 3 ⋅ 3 ⋅ 3 ⋅ 3 = \\n']\n",
      "3006 ['\\n = 3 + 2 = \\n', '\\n = 3 ⋅ 2 = \\n', '\\n = 8, whereas 32 = \\n', '\\n = 2 + ( 3 + 4 ) = \\n', '\\n = 2 ⋅ ( 3 ⋅ 4 ) = \\n', '\\n = 1000 and 10 − 4 = \\n', '\\n = 0, where n > 0. \\n If the exponent is negative, the power of zero ( 0n, where n < 0 ) is undefined, because division by zero is implied. \\n If the exponent is zero, some authors define 00 = \\n', '\\n = 1. \\n If n is an odd integer, then ( − 1 ) n = \\n', '\\n = 1 for all n if b = \\n']\n",
      "3007 ['\\n = b. \\n If b is a positive real number and n is a positive integer, then there is exactly one positive real solution to xn = \\n', '\\n = 2, 81 / 3 = \\n', '\\n = b has two real solutions if b is positive, which are the positive and negative nth roots ( the positive one being denoted <formula> ). If b is negative, the equation has no solution in real numbers for even n. \\n If n is odd, then xn = \\n', '\\n = 1 and v = \\n', '\\n = − 3 and ( − 27 ) 2 / 3 = \\n', '\\n = − 1, the definition of bu / v when b is negative and v is even must use the imaginary unit i, as described more fully in the section § Powers of complex numbers. \\n Care needs to be taken when applying the power identities with negative nth roots. For instance, − 27 = \\n', '\\n = ( ( − 27 ) 2 / 3 ) 3 / 2 = \\n', '\\n = π, the nonterminating decimal representation π = \\n']\n",
      "3008 ['\\n = 4, however, can be either 2 or − 2. The principal value of 41 / 2 is 2, but − 2 is also a valid square root. If the definition of exponentiation of real numbers is extended to allow negative results then the result is no longer well @-@ behaved. \\n Neither the logarithm method nor the rational exponent method can be used to define br as a real number for a negative real number b and an arbitrary real number r. Indeed, er is positive for every real number r, so ln ( b ) is not defined as a real number for b ≤ 0. \\n The rational exponent method cannot be used for negative values of b because it relies on continuity. The function f ( r ) = \\n', '\\n = − 1. The nth root of − 1 is − 1 for every odd natural number n. So if n is an odd positive integer, ( − 1 ) ( m / n ) = \\n', '\\n = 1 if m is even. Thus the set of rational numbers q for which ( − 1 ) q = \\n']\n",
      "3009 ['\\n = r cos θ and y = \\n', '\\n = x1 + <unk>, z2 = \\n', '\\n = ( 1n, nx / n ) = \\n', \"\\n = cos x + <unk> x ; this is Euler's formula, connecting algebra to trigonometry by means of complex numbers. \\n The solutions to the equation ez = \\n\", '\\n = w, then every solution to ez = \\n', '\\n = − 1 ; ex + iy = \\n', '\\n = ln ( b ) is the unique real solution to the equation ex = \\n']\n",
      "3010 ['\\n = z1 / 2 must be a solution to the equation w2 = \\n', '\\n = 1 for a positive integer n is an nth root of unity. Geometrically, the nth roots of unity lie on the unit circle of the complex plane at the vertices of a regular n @-@ gon with one vertex on the real number 1. \\n If wn = \\n', '\\n = 1 / n and n is a positive integer. These are the nth roots of w ; they are solutions of the equation zn = \\n']\n",
      "3011 ['\\n = u2 + v2 and θ is the \" angle \" θ = \\n', '\\n = 1, θ = \\n', '\\n = 0, and d = \\n', '\\n = x ⋅ log b holds whenever b is a positive real number and x is a real number. But for the principal branch of the complex logarithm one has \\n <formula> \\n Regardless of which branch of the logarithm is used, a similar failure of the identity will exist. The best that can be said ( if only using this result ) is that : \\n <formula> \\n This identity does not hold even when considering log as a multivalued function. The possible values of log ( wz ) contain those of z ⋅ log w as a subset. Using Log ( w ) for the principal value of log ( w ) and m, n as any integers the possible values of both sides are : \\n <formula> \\n The identities ( bc ) x = \\n']\n",
      "3014 ['\\n = 327 = \\n', '\\n = 0. Alternatively, the combinatorial interpretation of b0 is the number of empty tuples of elements from a set with b elements ; there is exactly one empty tuple, even if b = \\n', '\\n = 0 unless 00 = \\n', '\\n = 0 ) if 00 = \\n', '\\n = 1 at x = \\n']\n",
      "3016 ['\\n = xy defined on D = \\n', '\\n = [ − ∞, + ∞ ], endowed with the product topology ), which will contain the points at which the function f has a limit. \\n In fact, f has a limit at all accumulation points of D, except for ( 0, 0 ), ( + ∞, 0 ), ( 1, + ∞ ) and ( 1, − ∞ ). Accordingly, this allows one to define the powers xy by continuity whenever 0 ≤ x ≤ + ∞, − ∞ ≤ y ≤ + ∞, except for 00, ( + ∞ ) 0, 1 + ∞ and 1 − ∞, which remain indeterminate forms. \\n Under this definition by continuity, we obtain : \\n x + ∞ = \\n', '\\n = 0, when 1 < x ≤ + ∞. \\n x + ∞ = \\n', '\\n = + ∞, when 0 ≤ x < 1. \\n <unk> = \\n', '\\n = + ∞, when 0 < y ≤ + ∞. \\n <unk> = \\n', '\\n = 0, when − ∞ ≤ y < 0. \\n These powers are obtained by taking limits of xy for positive values of x. This method does not permit a definition of xy when x < 0, since pairs ( x, y ) with x < 0 are not accumulation points of D. \\n On the other hand, when n is an integer, the power xn is already meaningful for all values of x, including negative ones. This may make the definition 0n = \\n', '\\n = 64 + 32 + 4. Compute the following in order : \\n 22 = \\n', '\\n = 24 = \\n', '\\n = 28 = \\n', '\\n = 216 = \\n', '\\n = 232 = \\n']\n",
      "3017 ['\\n = 264 = \\n', '\\n = 2100 = \\n', '\\n = ( sin x ) − 1 = \\n', '\\n = Fellows v. Blacksmith = \\n']\n",
      "3024 ['\\n = Guitar Hero ( video game ) = \\n']\n",
      "3029 ['\\n = Gardner Lake = \\n']\n",
      "3030 ['\\n = German submarine U @-@ 44 ( 1939 ) = \\n']\n",
      "3032 ['\\n = Group 3 element = \\n']\n",
      "3036 ['\\n = Pike @-@ Pawnee Village Site = \\n']\n",
      "3042 ['\\n = Australia national baseball team = \\n']\n",
      "3045 [\"\\n = Donkey Kong Country 3 : Dixie Kong's Double Trouble! = \\n\"]\n",
      "3048 ['\\n = Herb Pennock = \\n']\n",
      "3051 ['\\n = 4th Armoured Brigade ( Australia ) = \\n']\n",
      "3053 ['\\n = Crawl ( Chris Brown song ) = \\n']\n",
      "3056 ['\\n = Missoula, Montana = \\n']\n",
      "3067 ['\\n = Gene = \\n']\n",
      "3074 ['\\n = Dynamics of the celestial spheres = \\n']\n",
      "3077 ['\\n = Greg Wohlwend = \\n']\n",
      "3081 ['\\n = Vincent and the Doctor = \\n']\n",
      "3085 ['\\n = Quantum electrodynamics = \\n']\n",
      "3090 ['\\n = Asker Line = \\n']\n",
      "3092 ['\\n = Dakota, Minnesota and Eastern Railroad = \\n']\n",
      "3096 ['\\n = Javier López ( baseball ) = \\n']\n",
      "3101 ['\\n = Domitian = \\n']\n",
      "3115 ['\\n = The Boat Race 1861 = \\n']\n",
      "3116 ['\\n = Tropical Storm Dean ( 1983 ) = \\n']\n",
      "3117 ['\\n = 1869 Atlantic hurricane season = \\n']\n",
      "3119 ['\\n = Bjaðmunjo <unk> = \\n']\n",
      "3121 ['\\n = Danton @-@ class battleship = \\n']\n",
      "3125 ['\\n = Libor Michálek = \\n']\n",
      "3126 ['\\n = Dangerously in Love = \\n']\n",
      "3132 ['\\n = Myriostoma = \\n']\n",
      "3135 ['\\n = Dusky dolphin = \\n']\n",
      "3139 ['\\n = Bellaire, Texas = \\n']\n",
      "3146 ['\\n = Alien Spidy = \\n']\n",
      "3148 ['\\n = Australian raven = \\n']\n",
      "3153 ['\\n = Clifton Suspension Bridge = \\n']\n",
      "3157 ['\\n = Social history of viruses = \\n']\n",
      "3169 ['\\n = All things = \\n']\n",
      "3173 ['\\n = Hurricane Doria ( 1967 ) = \\n']\n",
      "3174 ['\\n = Brehon B. Somervell = \\n']\n",
      "3178 ['\\n = Littorio @-@ class battleship = \\n']\n",
      "3184 ['\\n = Typhoon = \\n']\n",
      "3187 ['\\n = Halo Original Soundtrack = \\n']\n",
      "3188 ['\\n = Icewind Dale II = \\n']\n",
      "3192 ['\\n = Wu Zuguang = \\n']\n",
      "3194 ['\\n = Zara Yaqob = \\n']\n",
      "3196 ['\\n = Fujiwara no Teika = \\n']\n",
      "3205 ['\\n = Evan Rachel Wood = \\n']\n",
      "3208 ['\\n = M @-@ 30 ( Michigan highway ) = \\n']\n",
      "3209 [\"\\n = Father's Day ( Doctor Who ) = \\n\"]\n",
      "3212 ['\\n = Cyril Newall, 1st Baron Newall = \\n']\n",
      "3215 ['\\n = Salyut 6 = \\n']\n",
      "3217 ['\\n = HMS Achilles ( 1863 ) = \\n']\n",
      "3220 ['\\n = M3 Gun Motor Carriage = \\n']\n",
      "3221 ['\\n = Óengus I = \\n']\n",
      "3224 ['\\n = 1 : Nenokkadine = \\n']\n",
      "3229 ['\\n = Kongō @-@ class ironclad = \\n']\n",
      "3232 ['\\n = Jessica Jones ( season 1 ) = \\n']\n",
      "3238 ['\\n = Deepak Tijori = \\n']\n",
      "3241 ['\\n = Triptych, May – June 1973 = \\n']\n",
      "3245 ['\\n = Julius Franks = \\n']\n",
      "3246 [\"\\n = 2009 NCAA Division I Men's Lacrosse Championship = \\n\"]\n",
      "3249 ['\\n = The <unk> = \\n']\n",
      "3250 ['\\n = Moto Racer Advance = \\n']\n",
      "3251 ['\\n = Tina Turner = \\n']\n",
      "3260 ['\\n = <unk> = \\n', '\\n = Matt Striebel = \\n']\n",
      "3262 ['\\n = Sembawang Hot Spring = \\n']\n",
      "3263 ['\\n = Detour ( The X @-@ Files ) = \\n']\n",
      "3266 ['\\n = Flora of Scotland = \\n']\n",
      "3271 ['\\n = Grasshopper = \\n']\n",
      "3276 ['\\n = Ikuhiko Hata = \\n']\n",
      "3280 ['\\n = Wonderland ( Faryl Smith album ) = \\n']\n",
      "3282 ['\\n = Orval Grove = \\n']\n",
      "3285 ['\\n = 1994 – 95 South Pacific cyclone season = \\n']\n",
      "3287 ['\\n = Taylor series = \\n']\n",
      "3288 ['\\n = 1 is \\n <formula> \\n By integrating the above Maclaurin series, we find the Maclaurin series for log ( 1 − x ), where log denotes the natural logarithm : \\n <formula> \\n and the corresponding Taylor series for log ( x ) at a = \\n', '\\n = x0 is : \\n <formula> \\n The Taylor series for the exponential function ex at a = \\n']\n",
      "3289 ['\\n = 0. The pink curve is a polynomial of degree seven : \\n <formula> \\n The error in this approximation is no more than | x | 9 / 9!. In particular, for − 1 < x < 1, the error is less than 0 @.@ <unk>. \\n In contrast, also shown is a picture of the natural logarithm function log ( 1 + x ) and some of its Taylor polynomials around a = \\n', '\\n = 0, and has all derivatives zero there. Consequently, the Taylor series of f ( x ) about x = \\n']\n",
      "3290 ['\\n = 1 / 2 and the infinite geometric series for α = \\n']\n",
      "3291 ['\\n = The Chariot ( band ) = \\n']\n",
      "3294 ['\\n = Killer7 = \\n']\n",
      "3299 ['\\n = Cold Comfort ( Inside No. 9 ) = \\n']\n",
      "3303 ['\\n = Barbeyella minutissima = \\n']\n",
      "3304 ['\\n = Hollywood A.D. = \\n']\n",
      "3307 ['\\n = Maryland Route 322 = \\n', '\\n = Asher Vollmer = \\n']\n",
      "3309 ['\\n = Battle of Unsan = \\n']\n",
      "3311 ['\\n = California King Bed = \\n']\n",
      "3314 ['\\n = Typhoon Nelson ( 1982 ) = \\n']\n",
      "3316 ['\\n = Hopeville Pond State Park = \\n']\n",
      "3317 ['\\n = And Yet It Moves = \\n']\n",
      "3319 ['\\n = Sängerfest = \\n']\n",
      "3322 ['\\n = Tom Norman = \\n']\n",
      "3324 ['\\n = Banksia sphaerocarpa = \\n']\n",
      "3328 ['\\n = Hugh of Wells = \\n']\n",
      "3330 ['\\n = Panzer Dragoon = \\n']\n",
      "3334 ['\\n = The Converted Deacon = \\n']\n",
      "3335 ['\\n = Christian metal = \\n']\n",
      "3344 ['\\n = Fifteen ( song ) = \\n']\n",
      "3348 ['\\n = Wario World = \\n']\n",
      "3349 ['\\n = Middlesex ( novel ) = \\n']\n",
      "3363 ['\\n = Utah State Route 202 = \\n', '\\n = Lifesong = \\n']\n",
      "3366 ['\\n = Barry Bonds = \\n']\n",
      "3376 ['\\n = Interactions ( The Spectacular Spider @-@ Man ) = \\n']\n",
      "3378 [\"\\n = Phillips'Sound Recording Services = \\n\"]\n",
      "3380 ['\\n = Guianan cock @-@ of @-@ the @-@ rock = \\n']\n",
      "3383 ['\\n = SpongeBob SquarePants ( season 2 ) = \\n']\n",
      "3387 ['\\n = 23rd Battalion ( Australia ) = \\n']\n",
      "3389 ['\\n = Their Child = \\n']\n",
      "3390 ['\\n = Victoria Cross for New Zealand = \\n']\n",
      "3392 ['\\n = 1999 Bridge Creek – Moore tornado = \\n']\n",
      "3399 ['\\n = Slay Tracks ( 1933 – 1969 ) = \\n']\n",
      "3402 ['\\n = Cloudland Canyon State Park = \\n']\n",
      "3404 ['\\n = Rapunzel ( Disney ) = \\n']\n",
      "3409 ['\\n = Dragon Age : Origins = \\n']\n",
      "3417 ['\\n = Shōkaku @-@ class aircraft carrier = \\n']\n",
      "3424 ['\\n = President of Croatia = \\n']\n",
      "3429 ['\\n = Bill Brown ( cricketer ) = \\n']\n",
      "3436 ['\\n = Shaun Goater = \\n']\n",
      "3439 ['\\n = Carlton Hill, Brighton = \\n']\n",
      "3445 ['\\n = White @-@ eyed river martin = \\n']\n",
      "3448 ['\\n = Italian battleship Vittorio Emanuele = \\n']\n",
      "3450 ['\\n = M @-@ 49 ( Michigan highway ) = \\n', '\\n = Huletts Landing, New York = \\n']\n",
      "3452 ['\\n = Hello Good Morning = \\n']\n",
      "3455 ['\\n = Living River Siam = \\n']\n",
      "3458 ['\\n = Battle of the Plains of Abraham = \\n']\n",
      "3463 ['\\n = The Blessing Way ( The X @-@ Files ) = \\n']\n",
      "3465 ['\\n = Ratu ( band ) = \\n']\n",
      "3470 ['\\n = Jungle Strike = \\n']\n",
      "3472 ['\\n = U.S. Route 50 in Utah = \\n']\n",
      "3474 ['\\n = Susanna Cole = \\n']\n",
      "3476 ['\\n = Battleship = \\n']\n",
      "3486 ['\\n = Oryzomys = \\n']\n",
      "3489 ['\\n = 56, FN = \\n']\n",
      "3491 ['\\n = Philosophie Zoologique = \\n']\n",
      "3494 ['\\n = Spanish general election, 1933 = \\n']\n",
      "3496 ['\\n = New York State Route 812 = \\n']\n",
      "3498 ['\\n = Runcorn = \\n']\n",
      "3505 ['\\n = Mary Brewster Hazelton = \\n']\n",
      "3507 ['\\n = John J. Crittenden = \\n']\n",
      "3518 ['\\n = Ballad of Sir Frankie Crisp ( Let It Roll ) = \\n']\n",
      "3522 [\"\\n = The Muppets'Wizard of Oz = \\n\"]\n",
      "3527 ['\\n = Mourning dove = \\n']\n",
      "3530 ['\\n = Prince Alfred of Great Britain = \\n']\n",
      "3531 ['\\n = United States Assay Commission = \\n']\n",
      "3537 ['\\n = Fortifications of Mdina = \\n']\n",
      "3540 ['\\n = Werner Mölders = \\n']\n",
      "3548 ['\\n = Brainard Homestead State Park = \\n']\n",
      "3549 ['\\n = Tropical Storm Nicole ( 2010 ) = \\n']\n",
      "3553 [\"\\n = John Madden Football'93 = \\n\"]\n",
      "3555 ['\\n = Beyond Fantasy Fiction = \\n']\n",
      "3557 ['\\n = HMS Majestic ( 1895 ) = \\n']\n",
      "3559 ['\\n = Spoken For = \\n']\n",
      "3561 ['\\n = Flying Eagle cent = \\n']\n",
      "3565 ['\\n = Looking After Our Own = \\n']\n",
      "3567 ['\\n = Do or Die ( Thirty Seconds to Mars song ) = \\n']\n",
      "3570 ['\\n = Illmatic = \\n']\n",
      "3588 ['\\n = Siege of Constantinople ( 717 – 718 ) = \\n']\n",
      "3594 ['\\n = Denmark Street = \\n']\n",
      "3598 ['\\n = History of Sesame Street = \\n']\n",
      "3608 ['\\n = Hands All Over ( Maroon 5 song ) = \\n']\n",
      "3610 ['\\n = Glassheart Tour = \\n']\n",
      "3611 ['\\n = New Jersey Route 177 = \\n']\n",
      "3612 ['\\n = Dan Leno = \\n']\n",
      "3619 ['\\n = 1980 National League West tie @-@ breaker game = \\n']\n",
      "3621 ['\\n = Hutchinson Letters Affair = \\n']\n",
      "3623 ['\\n = Massive Attack ( song ) = \\n']\n",
      "3625 ['\\n = Shepseskare = \\n']\n",
      "3629 ['\\n = Lum You = \\n']\n",
      "3630 ['\\n = Henry Allingham = \\n']\n",
      "3634 ['\\n = Ottoman ironclad Âsâr @-@ ı Tevfik = \\n']\n",
      "3638 ['\\n = Jill Marsden ( EastEnders ) = \\n']\n",
      "3643 ['\\n = Roxy Ann Peak = \\n']\n",
      "3646 ['\\n = I Take Thee Quagmire = \\n']\n",
      "3648 ['\\n = Episode 3 ( Twin Peaks ) = \\n']\n",
      "3650 ['\\n = Gunther E. Rothenberg = \\n']\n",
      "3653 ['\\n = Symphony Six = \\n']\n",
      "3656 ['\\n = Alisia Dragoon = \\n']\n",
      "3658 ['\\n = Maurice ( Shelley ) = \\n']\n",
      "3661 ['\\n = Concurrent use registration = \\n']\n",
      "3664 ['\\n = TV Everywhere = \\n']\n",
      "3667 ['\\n = Stanley Internment Camp = \\n']\n",
      "3671 ['\\n = Monster Cable = \\n']\n",
      "3674 ['\\n = SummerSlam ( 2007 ) = \\n']\n",
      "3678 ['\\n = Magia ( Shakira album ) = \\n']\n",
      "3679 ['\\n = Goodies ( album ) = \\n']\n",
      "3682 [\"\\n = 2012 – 13 Big Ten Conference men's basketball season = \\n\"]\n",
      "3686 ['\\n = Cool Hand Peter = \\n']\n",
      "3688 ['\\n = University of Campinas = \\n']\n",
      "3695 ['\\n = Ones ( album ) = \\n']\n",
      "3697 ['\\n = Day One ( Torchwood ) = \\n']\n",
      "3700 ['\\n = 808s & Heartbreak = \\n']\n",
      "3706 ['\\n = History of supernova observation = \\n']\n",
      "3710 ['\\n = W. B. Yeats = \\n']\n",
      "3718 ['\\n = The Baby Show = \\n']\n",
      "3719 ['\\n = SMS Prinz Adalbert ( 1865 ) = \\n']\n",
      "3721 ['\\n = Battle of Lissa ( 1811 ) = \\n']\n",
      "3725 ['\\n = British Royal Navy, = \\n', '\\n = Jean @-@ Mathieu @-@ Philibert Sérurier = \\n']\n",
      "3732 ['\\n = Daniel Santos ( singer ) = \\n']\n",
      "3735 ['\\n = 2007 Bernard Matthews H5N1 outbreak = \\n']\n",
      "3737 ['\\n = Norman Whiteside = \\n']\n",
      "3741 ['\\n = Horace Greeley = \\n']\n",
      "3751 ['\\n = Stede Bonnet = \\n']\n",
      "3756 ['\\n = Henry DeWolf Smyth = \\n']\n",
      "3759 ['\\n = Sebaceous gland = \\n']\n",
      "3762 [\"\\n = Nansen's Fram expedition = \\n\"]\n",
      "3771 ['\\n = The Monkey Suit = \\n']\n",
      "3773 ['\\n = Horse = \\n']\n",
      "3784 ['\\n = Patience ( The X @-@ Files ) = \\n']\n",
      "3789 ['\\n = Ersatz Yorck @-@ class battlecruiser = \\n']\n",
      "3792 ['\\n = Rameswaram = \\n']\n",
      "3798 ['\\n = Archaeoraptor = \\n']\n",
      "3802 ['\\n = <unk> = \\n']\n",
      "3804 ['\\n = Idiotest = \\n']\n",
      "3806 ['\\n = Puzzlejuice = \\n']\n",
      "3807 ['\\n = Potlatch River = \\n']\n",
      "3809 ['\\n = When You Reach Me = \\n']\n",
      "3813 ['\\n = Flekkefjord Station = \\n']\n",
      "3814 ['\\n = Case of the Dean of St Asaph = \\n']\n",
      "3816 ['\\n = Colin Hall Simpson = \\n']\n",
      "3819 ['\\n = Prince of Wales ( 1786 ship ) = \\n']\n",
      "3823 ['\\n = Garbage Museum = \\n']\n",
      "3824 ['\\n = The Boat Race 1950 = \\n']\n",
      "3826 ['\\n = Sandsfoot Castle = \\n']\n",
      "3828 ['\\n = Salvia divinorum = \\n']\n",
      "3838 ['\\n = New York State Route 251 = \\n']\n",
      "3839 ['\\n = Tropical Storm Lester ( 2004 ) = \\n']\n",
      "3840 ['\\n = Tvrđa = \\n']\n",
      "3845 ['\\n = HMS Daring ( <unk> ) = \\n']\n",
      "3846 ['\\n = Kaga Rebellion = \\n']\n",
      "3848 ['\\n = Clavaria zollingeri = \\n']\n",
      "3849 ['\\n = Ashdod = \\n']\n",
      "3856 ['\\n = Héctor López = \\n']\n",
      "3859 ['\\n = Maria Goeppert @-@ Mayer = \\n']\n",
      "3861 ['\\n = Powhatan Beaty = \\n']\n",
      "3863 ['\\n = Ra.One = \\n']\n",
      "3871 ['\\n = Jonathan Strange & Mr Norrell = \\n']\n",
      "3879 ['\\n = Union Bank of Switzerland = \\n']\n",
      "3886 ['\\n = Ginga Legend Weed = \\n']\n",
      "3889 ['\\n = Manuel Alberti = \\n']\n",
      "3891 ['\\n = Horses in the Middle Ages = \\n']\n",
      "3899 ['\\n = Billy Meredith = \\n']\n",
      "3905 ['\\n = Jean Abraham Grill = \\n']\n",
      "3907 ['\\n = Metabolism = \\n']\n",
      "3915 ['\\n = Bangla Desh ( song ) = \\n']\n",
      "3920 ['\\n = Anna Akhmatova = \\n']\n",
      "3927 ['\\n = Isaac = \\n']\n",
      "3931 ['\\n = Yugoslav destroyer Dubrovnik = \\n']\n",
      "3934 [\"\\n = Sozin's Comet : The Final Battle = \\n\"]\n",
      "3937 ['\\n = Magnus Carlsen = \\n']\n",
      "3950 ['\\n = Louis Antoine de Saint @-@ Just = \\n']\n",
      "3958 ['\\n = Feeling This = \\n']\n",
      "3960 ['\\n = Scipione Piattoli = \\n']\n",
      "3963 ['\\n = Sudirman = \\n']\n",
      "3976 ['\\n = István Szabó = \\n']\n",
      "3980 ['\\n = Jim and Mary McCartney = \\n']\n",
      "3984 ['\\n = M @-@ 99 ( Michigan highway ) = \\n']\n",
      "3986 ['\\n = 1853 Atlantic hurricane season = \\n']\n",
      "3987 ['\\n = Egon Mayer = \\n']\n",
      "3990 ['\\n = Alberta and Great Waterways Railway scandal = \\n']\n",
      "3996 ['\\n = A Short Walk in the Hindu Kush = \\n']\n",
      "4000 ['\\n = Philip Humber = \\n']\n",
      "4007 ['\\n = Jonathan Lethem = \\n']\n",
      "4010 ['\\n = Nki National Park = \\n']\n",
      "4012 [\"\\n = Mauritius women's national football team = \\n\"]\n",
      "4013 [\"\\n = Hari's on Tour ( Express ) = \\n\"]\n",
      "4017 ['\\n = Vine Street, London = \\n']\n",
      "4018 ['\\n = 4 Minutes = \\n']\n",
      "4024 ['\\n = Greed ( film ) = \\n']\n",
      "4035 ['\\n = BabyFirst = \\n']\n",
      "4037 ['\\n = John Michael Wright = \\n']\n",
      "4041 [\"\\n = On the Internet, nobody knows you're a dog = \\n\"]\n",
      "4042 ['\\n = I Am God = \\n']\n",
      "4044 ['\\n = June 1941 uprising in eastern Herzegovina = \\n']\n",
      "4052 ['\\n = Katori @-@ class battleship = \\n']\n",
      "4054 ['\\n = Sonnet 86 = \\n']\n",
      "4055 ['\\n = ictus, a metrically strong syllabic position. × = \\n']\n",
      "4057 ['\\n = New York State Route 47 = \\n']\n",
      "4060 ['\\n = Battle of Nanking = \\n']\n",
      "4067 ['\\n = Eagle Boys = \\n']\n",
      "4069 ['\\n = Wood thrush = \\n']\n",
      "4072 ['\\n = Kristen Bell = \\n']\n",
      "4077 ['\\n = Ratatouille ( film ) = \\n']\n",
      "4084 ['\\n = Jon Hamm = \\n']\n",
      "4087 ['\\n = USS Yancey ( AKA @-@ 93 ) = \\n']\n",
      "4092 ['\\n = Leopard 2E = \\n']\n",
      "4094 ['\\n = Indian Ocean raid ( 1944 ) = \\n']\n",
      "4096 ['\\n = Geneforge = \\n']\n",
      "4099 ['\\n = Giant mouse lemur = \\n']\n",
      "4105 ['\\n = Copper shark = \\n']\n",
      "4109 ['\\n = Fantastic ( magazine ) = \\n']\n",
      "4116 ['\\n = Mountain of Madness = \\n']\n",
      "4117 ['\\n = Descent : FreeSpace – The Great War = \\n']\n",
      "4121 ['\\n = Ontario Highway 72 = \\n']\n",
      "4122 ['\\n = Croatia – Serbia border dispute = \\n']\n",
      "4125 ['\\n = The Boat Race 1958 = \\n']\n",
      "4126 ['\\n = She : A History of Adventure = \\n']\n",
      "4136 ['\\n = Lynn Bomar = \\n']\n",
      "4141 ['\\n = Sitting Bull = \\n']\n",
      "4147 ['\\n = The Boat Race 1905 = \\n']\n",
      "4148 ['\\n = Tropical Storm Elena ( 1979 ) = \\n']\n",
      "4149 ['\\n = Michigan State University = \\n']\n",
      "4160 ['\\n = Phoenix Wright : Ace Attorney − Justice for All = \\n']\n",
      "4165 ['\\n = Bjorøy Tunnel = \\n']\n",
      "4168 ['\\n = Smedley Butler = \\n']\n",
      "4177 ['\\n = King vulture = \\n']\n",
      "4181 ['\\n = William R. Purnell = \\n']\n",
      "4183 ['\\n = Adam Air Flight 172 = \\n']\n",
      "4184 ['\\n = Aonchotheca forresteri = \\n']\n",
      "4185 ['\\n = Killswitch Engage = \\n']\n",
      "4190 ['\\n = Tomahawk ( album ) = \\n']\n",
      "4191 ['\\n = Pirates of the Caribbean ( film series ) = \\n']\n",
      "4195 ['\\n = California State Route 266 = \\n', '\\n = Plasma ( physics ) = \\n']\n",
      "4202 ['\\n = Neaira ( hetaera ) = \\n']\n",
      "4203 ['\\n = Serranus Clinton Hastings = \\n']\n",
      "4205 ['\\n = Datchet Bridge = \\n']\n",
      "4207 ['\\n = M @-@ 137 ( Michigan highway ) = \\n', '\\n = Henry Burrell ( admiral ) = \\n']\n",
      "4210 ['\\n = British contribution to the Manhattan Project = \\n']\n",
      "4219 ['\\n = John Lennon = \\n']\n",
      "4233 ['\\n = 411th Engineer Brigade ( United States ) = \\n']\n",
      "4235 ['\\n = Stanley Matthews = \\n']\n",
      "4244 ['\\n = Choiseul pigeon = \\n']\n",
      "4247 ['\\n = Charles Boycott = \\n']\n",
      "4253 ['\\n = Neighbours = \\n']\n",
      "4261 ['\\n = Francis Harvey = \\n']\n",
      "4264 ['\\n = DayZ ( mod ) = \\n']\n",
      "4266 ['\\n = Ted Bundy = \\n']\n",
      "4284 ['\\n = Lovely ( Desperate Housewives ) = \\n']\n",
      "4286 ['\\n = Hurricane Blanca ( 2015 ) = \\n']\n",
      "4288 ['\\n = Coral Springs, Florida = \\n']\n",
      "4293 ['\\n = Young Blood ( Sophie Ellis @-@ Bextor song ) = \\n']\n",
      "4294 ['\\n = University of North Carolina at Chapel Hill = \\n']\n",
      "4304 ['\\n = Ames Project = \\n']\n",
      "4309 ['\\n = Vildanden ( airline ) = \\n']\n",
      "4312 ['\\n = Your Love ( Nicki Minaj song ) = \\n']\n",
      "4314 ['\\n = Miles Copeland ( Home and Away ) = \\n']\n",
      "4319 ['\\n = Bristol Britannia = \\n']\n",
      "4325 ['\\n = Tales ( series ) = \\n']\n",
      "4331 [\"\\n = Zimbabwe women's national field hockey team at the 1980 Summer Olympics = \\n\"]\n",
      "4333 ['\\n = Air Greenland = \\n']\n",
      "4338 ['\\n = Murali Kartik = \\n']\n",
      "4346 ['\\n = 2001 Gator Bowl = \\n']\n",
      "4355 ['\\n = Invisible Circles = \\n']\n",
      "4359 ['\\n = Halifax Central Library = \\n']\n",
      "4362 ['\\n = Bombardment of Papeete = \\n']\n",
      "4365 ['\\n = Forbes Field = \\n']\n",
      "4371 ['\\n = Mark Stimson = \\n']\n",
      "4373 ['\\n = Steve Fossett = \\n']\n",
      "4382 ['\\n = WCLG ( AM ) = \\n']\n",
      "4383 ['\\n = Edward Elric = \\n']\n",
      "4387 ['\\n = Picardy Spaniel = \\n']\n",
      "4388 ['\\n = U @-@ Drop Inn = \\n']\n",
      "4390 [\"\\n = They Don 't Care About Us = \\n\"]\n",
      "4393 ['\\n = Benzodiazepine = \\n']\n",
      "4405 ['\\n = Claire Underwood = \\n']\n",
      "4408 ['\\n = The Utility of Force = \\n']\n",
      "4412 ['\\n = Fighter Squadron RAAF = \\n']\n",
      "4413 ['\\n = One Times Square = \\n']\n",
      "4415 ['\\n = USS Henry R. Mallory ( ID @-@ 1280 ) = \\n']\n",
      "4417 ['\\n = Italian ironclad Francesco Morosini = \\n']\n",
      "4418 ['\\n = Hollywood Rip Ride Rockit = \\n']\n",
      "4420 ['\\n = Doreen Valiente = \\n']\n",
      "4427 ['\\n = Lung cancer = \\n']\n",
      "4434 ['\\n = Canadians = \\n']\n",
      "4438 ['\\n = SMS Prinzregent Luitpold = \\n']\n",
      "4443 ['\\n = Michèle Mouton = \\n']\n",
      "4450 ['\\n = Dan Borislow = \\n']\n",
      "4453 ['\\n = Sarus crane = \\n']\n",
      "4459 ['\\n = United Nations Parliamentary Assembly = \\n']\n",
      "4466 ['\\n = Dave Stamper = \\n']\n",
      "4467 ['\\n = Advanced Gemini = \\n']\n",
      "4472 ['\\n = MacPaint = \\n']\n",
      "4473 ['\\n = Night Out ( The Office ) = \\n']\n",
      "4475 ['\\n = Raining Men ( Rihanna song ) = \\n']\n",
      "4478 ['\\n = Brooks Laich = \\n']\n",
      "4481 ['\\n = Tempus Fugit ( The X @-@ Files ) = \\n']\n",
      "4483 ['\\n = Pain in My Heart = \\n']\n",
      "4485 ['\\n = Southampton town walls = \\n']\n",
      "4489 [\"\\n = Peter's Two Dads = \\n\"]\n",
      "4491 ['\\n = Black mamba = \\n']\n",
      "4496 ['\\n = Civil War token = \\n']\n",
      "4497 ['\\n = Gulf Oil = \\n']\n",
      "4505 ['\\n = <unk> = \\n']\n",
      "4507 ['\\n = William C. Chase = \\n']\n",
      "4510 ['\\n = <unk> injury = \\n']\n",
      "4515 ['\\n = Word of God Speak = \\n']\n",
      "4516 ['\\n = James Bowie = \\n']\n",
      "4524 ['\\n = Hygrophorus agathosmus = \\n']\n",
      "4525 [\"\\n = Could've Been You = \\n\"]\n",
      "4526 ['\\n = Battle of Kalavrye = \\n']\n",
      "4529 ['\\n = Down to Earth ( Justin Bieber song ) = \\n']\n",
      "4530 ['\\n = Paddy Moran ( ice hockey ) = \\n']\n",
      "4531 ['\\n = Typhoon Halong ( 2002 ) = \\n']\n",
      "4534 ['\\n = RAF Uxbridge = \\n']\n",
      "4540 ['\\n = Audrey Pauley = \\n']\n",
      "4542 ['\\n = Pedra Branca dispute = \\n']\n",
      "4556 ['\\n = 1994 Fairchild Air Force Base B @-@ 52 crash = \\n']\n",
      "4560 ['\\n = Miles Fisher = \\n']\n",
      "4562 ['\\n = Pilot ( Once Upon a Time ) = \\n']\n",
      "4566 ['\\n = Paul Butterfield = \\n']\n",
      "4572 ['\\n = Timothy Everest = \\n']\n",
      "4574 ['\\n = Symphony in White, No. 3 = \\n']\n",
      "4576 ['\\n = 2 Become 1 = \\n']\n",
      "4580 ['\\n = Crimes Act of 1790 = \\n']\n",
      "4585 ['\\n = Witch trials in early modern Scotland = \\n']\n",
      "4588 ['\\n = Loboc Church = \\n']\n",
      "4591 ['\\n = Robert A. Little = \\n']\n",
      "4593 ['\\n = Winnebago War = \\n']\n",
      "4596 ['\\n = The Modern Cook = \\n']\n",
      "4599 ['\\n = Ted Petoskey = \\n']\n",
      "4602 ['\\n = Psilocybe hispanica = \\n']\n",
      "4603 ['\\n = Valhalla = \\n']\n",
      "4608 ['\\n = NSB Di 3 = \\n']\n",
      "4611 ['\\n = Queen Vic Fire Week = \\n']\n",
      "4615 ['\\n = Cyclone Kate ( 2006 ) = \\n']\n",
      "4616 ['\\n = Metal Gear Solid = \\n']\n",
      "4622 ['\\n = Delaware Route 26 = \\n']\n",
      "4623 ['\\n = The Stripped Mixes = \\n']\n",
      "4624 ['\\n = Sir William Gordon @-@ Cumming, 4th Baronet = \\n']\n",
      "4627 ['\\n = Lycoperdon echinatum = \\n']\n",
      "4629 ['\\n = Speak Now ( song ) = \\n']\n",
      "4631 ['\\n = American Water Spaniel = \\n']\n",
      "4633 ['\\n = Arniston ( East Indiaman ) = \\n']\n",
      "4636 ['\\n = United States Capitol shooting incident ( 1998 ) = \\n']\n",
      "4637 ['\\n = Hurricane Kathleen ( 1976 ) = \\n']\n",
      "4639 ['\\n = Mamoru Miyano = \\n']\n",
      "4641 ['\\n = Japanese ironclad Hiei = \\n']\n",
      "4654 ['\\n = Be Someone Else = \\n']\n",
      "4657 ['\\n = Delichon = \\n']\n",
      "4660 ['\\n = The Firefly ( Fringe ) = \\n']\n",
      "4663 ['\\n = Ironsword : Wizards & Warriors II = \\n']\n",
      "4667 ['\\n = Unas = \\n']\n",
      "4672 ['\\n = Bulldog Drummond = \\n']\n",
      "4675 ['\\n = Lydford Castle = \\n']\n",
      "4679 ['\\n = Typhoon Cimaron ( 2006 ) = \\n']\n",
      "4682 ['\\n = Big Inch = \\n']\n",
      "4687 ['\\n = Horace Robertson = \\n']\n",
      "4692 ['\\n = Guilty ( Awake ) = \\n']\n",
      "4694 ['\\n = Curiosity ( EP ) = \\n']\n",
      "4695 ['\\n = W. E. B. Du Bois = \\n']\n",
      "4710 ['\\n = Sectionals = \\n']\n",
      "4712 ['\\n = Theodore N. Kaufman = \\n']\n",
      "4714 ['\\n = Pull Up Some Dust and Sit Down = \\n']\n",
      "4719 [\"\\n = Losing My Religion ( Grey's Anatomy ) = \\n\"]\n",
      "4721 ['\\n = M @-@ 44 ( Michigan highway ) = \\n']\n",
      "4723 ['\\n = Road to Rhode Island = \\n']\n",
      "4724 ['\\n = Pilot ( Millennium ) = \\n']\n",
      "4726 ['\\n = Flag of Japan = \\n']\n",
      "4734 ['\\n = Construction of the Trans @-@ Alaska Pipeline System = \\n']\n",
      "4744 ['\\n = Diolkos = \\n']\n",
      "4747 ['\\n = Xue Susu = \\n']\n",
      "4748 ['\\n = 5th Avenue Theatre = \\n']\n",
      "4752 ['\\n = Who Really Cares ( Featuring the Sound of Insanity ) = \\n']\n",
      "4754 ['\\n = SMS Erzherzog Ferdinand Max ( 1865 ) = \\n']\n",
      "4756 [\"\\n = Victoria's Secret Fashion Show = \\n\"]\n",
      "4758 ['\\n = Boston Police Strike = \\n']\n",
      "4763 ['\\n = Charles Lloyd ( Australian general ) = \\n']\n",
      "4764 ['\\n = Nonviolent Communication = \\n']\n",
      "4770 [\"\\n = Who's Your Neighbor? = \\n\"]\n",
      "4773 ['\\n = Sydney Rowell = \\n']\n",
      "4778 ['\\n = Tatuidris = \\n']\n",
      "4781 ['\\n = Go, Stewie, Go! = \\n']\n",
      "4783 ['\\n = Gettysburg ( The Office ) = \\n']\n",
      "4785 ['\\n = John Deere House and Shop = \\n']\n",
      "4786 ['\\n = Polygon ( website ) = \\n']\n",
      "4787 ['\\n = Hogwarts Express ( Universal Orlando Resort ) = \\n']\n",
      "4791 ['\\n = 509th Composite Group = \\n']\n",
      "4797 ['\\n = Italian cruiser Giovanni Bausan = \\n']\n",
      "4799 ['\\n = Typhoon Karen = \\n']\n",
      "4803 ['\\n = Brønnøysund Airport, Brønnøy = \\n']\n",
      "4806 ['\\n = 1981 Atlantic hurricane season = \\n']\n",
      "4810 ['\\n = Günther Lützow = \\n']\n",
      "4817 ['\\n = Imperator torosus = \\n']\n",
      "4820 ['\\n = Kareena Kapoor = \\n']\n",
      "4830 ['\\n = Ryan Hanigan = \\n']\n",
      "4832 ['\\n = SS Pennsylvanian = \\n']\n",
      "4835 ['\\n = Cabell Breckinridge = \\n']\n",
      "4837 ['\\n = Typhoon Oliwa = \\n']\n",
      "4839 ['\\n = Tops in Science Fiction = \\n']\n",
      "4840 ['\\n = John Gardner ( British writer ) = \\n']\n",
      "4842 ['\\n = Jordan Kovacs = \\n']\n",
      "4845 ['\\n = TNA X Division Championship = \\n']\n",
      "4847 ['\\n = Andrianjaka = \\n']\n",
      "4850 ['\\n = No Way Out ( 2004 ) = \\n']\n",
      "4853 ['\\n = Scout Taylor @-@ Compton = \\n']\n",
      "4855 [\"\\n = Goin'Home ( Archie Shepp and Horace Parlan album ) = \\n\"]\n",
      "4856 ['\\n = Hurricane Humberto ( 2007 ) = \\n']\n",
      "4859 ['\\n = Theoren Fleury = \\n']\n",
      "4866 ['\\n = Carnotaurus = \\n']\n",
      "4871 [\"\\n = Go Missin'= \\n\"]\n",
      "4872 ['\\n = You Know What You Did = \\n']\n",
      "4873 ['\\n = Early Winter = \\n']\n",
      "4875 ['\\n = Blue Dragon = \\n']\n",
      "4884 ['\\n = Tintin in America = \\n']\n",
      "4889 ['\\n = Romney Literary Society = \\n']\n",
      "4894 ['\\n = Bill Stein = \\n']\n",
      "4898 ['\\n = Ranavalona I = \\n']\n",
      "4905 ['\\n = Richard Buxton ( botanist ) = \\n']\n",
      "4906 ['\\n = 2012 Formula One season = \\n']\n",
      "4922 ['\\n = Typhoon Shanshan ( 2006 ) = \\n']\n",
      "4924 ['\\n = Type II supernova = \\n']\n",
      "4928 ['\\n = Pulveroboletus ravenelii = \\n']\n",
      "4929 [\"\\n = Johnny's Theme = \\n\"]\n",
      "4931 ['\\n = Blue Velvet ( film ) = \\n']\n",
      "4938 ['\\n = Is Google Making Us Stupid? = \\n']\n",
      "4944 ['\\n = Jon Lieber = \\n']\n",
      "4949 ['\\n = Destiny Fulfilled = \\n']\n",
      "4954 ['\\n = Moses Hardy = \\n']\n",
      "4956 ['\\n = Fresh Blood ( Supernatural ) = \\n']\n",
      "4959 ['\\n = Spanish conquest of Guatemala = \\n']\n",
      "4975 [\"\\n = 1994 Giro d 'Italia = \\n\"]\n",
      "4978 ['\\n = Riverton – Belvidere Bridge = \\n']\n",
      "4980 ['\\n = Cleeve Abbey = \\n']\n",
      "4983 ['\\n = Serbia – United States relations = \\n']\n",
      "4988 ['\\n = M @-@ 221 ( Michigan highway ) = \\n', '\\n = Juniper MX @-@ Series = \\n']\n",
      "4989 ['\\n = Lord Howe Island = \\n']\n",
      "5000 ['\\n = Romance ( Luis Miguel album ) = \\n']\n",
      "5003 ['\\n = Sora ( Kingdom Hearts ) = \\n']\n",
      "5007 ['\\n = Dennis Johnson = \\n']\n",
      "5012 ['\\n = The World Before the Flood = \\n']\n",
      "5015 ['\\n = When Love Takes Over = \\n']\n",
      "5020 ['\\n = Kepler @-@ 7 = \\n']\n",
      "5021 ['\\n = Church of Saint Oswald, King and Martyr, Oswaldkirk = \\n']\n",
      "5022 ['\\n = F @-@ Zero ( video game ) = \\n']\n",
      "5026 ['\\n = Body Count ( album ) = \\n']\n",
      "5030 ['\\n = Turtles All the Way Down = \\n']\n",
      "5033 ['\\n = Report about Case Srebrenica = \\n']\n",
      "5036 ['\\n = Ach Gott, wie manches Herzeleid, BWV 3 = \\n']\n",
      "5038 ['\\n = Hastings Line = \\n']\n",
      "5045 ['\\n = Music of Final Fantasy IV = \\n']\n",
      "5048 ['\\n = Pool of Radiance = \\n']\n",
      "5053 ['\\n = Elimination Chamber ( 2010 ) = \\n']\n",
      "5057 ['\\n = Mario Bros. = \\n']\n",
      "5060 ['\\n = Mary Isenhour = \\n']\n",
      "5062 ['\\n = Isle of Portland = \\n']\n",
      "5068 ['\\n = Craig McAllister = \\n']\n",
      "5070 ['\\n = <unk> = \\n']\n",
      "5071 ['\\n = Donovan Warren = \\n']\n",
      "5073 ['\\n = Antarctica : Empire of the Penguin = \\n']\n",
      "5075 ['\\n = Smithfield, London = \\n']\n",
      "5081 ['\\n = HMS Havelock ( <unk> ) = \\n']\n",
      "5083 ['\\n = Sunda slow loris = \\n']\n",
      "5092 ['\\n = Amber Room = \\n']\n",
      "5095 ['\\n = Lipid = \\n']\n",
      "5100 [\"\\n = Don 't Wake Me Up ( song ) = \\n\"]\n",
      "5103 ['\\n = Battle of Île Ronde = \\n']\n",
      "5105 ['\\n = Jean @-@ Marie Defrance = \\n']\n",
      "5106 ['\\n = California Southern Railroad = \\n']\n",
      "5110 ['\\n = Salt = \\n']\n",
      "5116 ['\\n = SMS Heimdall = \\n']\n",
      "5117 ['\\n = Thomas Bryan Martin = \\n']\n",
      "5121 ['\\n = John Maulbetsch = \\n']\n",
      "5127 ['\\n = Order of Canada = \\n']\n",
      "5132 ['\\n = Ontario Highway 403 = \\n']\n",
      "5137 ['\\n = Viewing Party = \\n']\n",
      "5138 ['\\n = Bart Gets an \" F \" = \\n']\n",
      "5142 ['\\n = MediEvil : Resurrection = \\n']\n",
      "5144 ['\\n = Infamous Second Son = \\n']\n",
      "5150 ['\\n = 1973 Pacific hurricane season = \\n']\n",
      "5152 ['\\n = Varaha Upanishad = \\n']\n",
      "5157 ['\\n = Vettor Pisani @-@ class cruiser = \\n']\n",
      "5159 ['\\n = Radius of maximum wind = \\n']\n",
      "5160 ['\\n = William Wilberforce = \\n']\n",
      "5170 ['\\n = Clover ( creature ) = \\n']\n",
      "5172 ['\\n = Knut ( polar bear ) = \\n']\n",
      "5177 [\"\\n = Michael John O 'Leary = \\n\"]\n",
      "5180 ['\\n = Mari, Syria = \\n']\n",
      "5186 ['\\n = OMG ( song ) = \\n']\n",
      "5189 ['\\n = Clavaria fragilis = \\n']\n",
      "5191 ['\\n = Natural dye = \\n']\n",
      "5197 ['\\n = Random Access Memories = \\n']\n",
      "5205 ['\\n = Peyton Manning = \\n']\n",
      "5221 ['\\n = Grand Theft Auto IV = \\n']\n",
      "5229 ['\\n = Norwood – 205th Street ( IND Concourse Line ) = \\n']\n",
      "5230 ['\\n = Corey Taylor = \\n']\n",
      "5233 ['\\n = International Association for Plant Taxonomy = \\n']\n",
      "5234 ['\\n = Lhasa ( prefecture @-@ level city ) = \\n']\n",
      "5246 ['\\n = 33 ( Battlestar Galactica ) = \\n']\n",
      "5249 [\"\\n = Ralph d 'Escures = \\n\"]\n",
      "5251 ['\\n = Bow Back Rivers = \\n']\n",
      "5256 ['\\n = OSI ( band ) = \\n']\n",
      "5258 ['\\n = First interracial kiss on television = \\n']\n",
      "5259 ['\\n = Geographical name changes in Turkey = \\n']\n",
      "5262 ['\\n = Solar energy = \\n']\n",
      "5268 ['\\n = The Garden of Earthly Delights = \\n']\n",
      "5278 ['\\n = 1994 Gator Bowl = \\n']\n",
      "5286 ['\\n = Baggage ( U.S. game show ) = \\n']\n",
      "5287 ['\\n = Churchill War Rooms = \\n']\n",
      "5289 ['\\n = Plitvice Lakes incident = \\n']\n",
      "5292 ['\\n = Silky sifaka = \\n']\n",
      "5297 ['\\n = Transformer = \\n']\n",
      "5307 ['\\n = Jeff Hanneman = \\n']\n",
      "5310 ['\\n = Robert McLachlan ( cinematographer ) = \\n']\n",
      "5311 ['\\n = Otto Julius Zobel = \\n']\n",
      "5315 [\"\\n = The M + M's Tour = \\n\"]\n",
      "5317 ['\\n = Gateway Protection Programme = \\n']\n",
      "5320 ['\\n = Dreamcast = \\n']\n",
      "5332 [\"\\n = St Caian's Church, Tregaian = \\n\"]\n",
      "5335 ['\\n = Vampire : The Masquerade – Bloodlines = \\n']\n",
      "5344 [\"\\n = Won 't Get Fooled Again = \\n\"]\n",
      "5346 ['\\n = Homer at the Bat = \\n']\n",
      "5349 ['\\n = Ottoman cruiser Berk @-@ i Satvet = \\n']\n",
      "5351 ['\\n = Ohio State Route 607 = \\n']\n",
      "5352 ['\\n = SK Ull = \\n']\n",
      "5354 [\"\\n = Can 't Hold Us Down = \\n\"]\n",
      "5357 ['\\n = Station model = \\n']\n",
      "5359 ['\\n = Béla I of Hungary = \\n']\n",
      "5362 ['\\n = Su Song = \\n']\n",
      "5368 ['\\n = Periyar E. V. Ramasamy = \\n']\n",
      "5379 ['\\n = Moonraker ( novel ) = \\n']\n",
      "5385 ['\\n = Spooked ( The Office ) = \\n']\n",
      "5386 ['\\n = Primer ( film ) = \\n']\n",
      "5390 ['\\n = Schiehallion experiment = \\n']\n",
      "5393 ['\\n = Battle of Ban Me Thuot = \\n']\n",
      "5401 ['\\n = Art Farmer = \\n']\n",
      "5405 ['\\n = Karim Benzema = \\n']\n",
      "5415 ['\\n = Roanoke Building = \\n']\n",
      "5417 ['\\n = Donnie Fatso = \\n']\n",
      "5418 ['\\n = Hand in Glove = \\n']\n",
      "5421 ['\\n = Darlington F.C. = \\n']\n",
      "5426 ['\\n = Andry Rajoelina = \\n']\n",
      "5432 ['\\n = Supergirl ( Hannah Montana song ) = \\n']\n",
      "5433 ['\\n = Leccinum manzanitae = \\n']\n",
      "5435 ['\\n = Ramariopsis kunzei = \\n']\n",
      "5436 ['\\n = By the Grace of God ( song ) = \\n']\n",
      "5438 ['\\n = Young at Heart ( The X @-@ Files ) = \\n']\n",
      "5440 ['\\n = SMS Stralsund = \\n']\n",
      "5442 ['\\n = Aaron Sorkin = \\n']\n",
      "5449 ['\\n = Delaware Route 72 = \\n']\n",
      "5451 ['\\n = Arthur Compton = \\n']\n",
      "5452 ['\\n = 0 ° ) and twice the Compton wavelength of the electron ( for θ = \\n']\n",
      "5454 ['\\n = Lockheed Have Blue = \\n']\n",
      "5458 ['\\n = William Barley = \\n']\n",
      "5461 ['\\n = Battle of St. Louis = \\n']\n",
      "5463 ['\\n = Ray Jones ( footballer, born 1988 ) = \\n']\n",
      "5464 ['\\n = Gheorghe Tătărescu = \\n']\n",
      "5470 ['\\n = Someday ( I Will Understand ) = \\n']\n",
      "5472 ['\\n = Willie Mount = \\n']\n",
      "5473 ['\\n = No. 36 Squadron RAAF = \\n']\n",
      "5477 ['\\n = Cucurbita = \\n']\n",
      "5485 ['\\n = Beautiful Liar = \\n']\n",
      "5489 ['\\n = Star Wars : Rogue Squadron = \\n']\n",
      "5493 ['\\n = HMS Glatton ( 1914 ) = \\n']\n",
      "5496 ['\\n = Leo Martello = \\n']\n",
      "5499 ['\\n = Tropical Storm Julio ( 2008 ) = \\n']\n",
      "5500 ['\\n = Hubble Space Telescope = \\n']\n",
      "5513 ['\\n = California spiny lobster = \\n']\n",
      "5515 ['\\n = Hybrid Theory = \\n']\n",
      "5519 ['\\n = Eurasian wryneck = \\n']\n",
      "5522 ['\\n = Dream a Little Dream of Me ( Supernatural ) = \\n']\n",
      "5524 ['\\n = W. S. Gilbert = \\n']\n",
      "5533 ['\\n = Ned Manning = \\n']\n",
      "5535 ['\\n = Burnside Burn = \\n']\n",
      "5536 ['\\n = Interstate 295 ( Delaware – New Jersey ) = \\n']\n",
      "5541 [\"\\n = Blockhaus d 'Éperlecques = \\n\"]\n",
      "5546 ['\\n = Agreement on Friendship and Cooperation between Bosnia and Herzegovina and Croatia = \\n']\n",
      "5549 ['\\n = Wizards & Warriors = \\n']\n",
      "5552 ['\\n = Disasterpieces = \\n']\n",
      "5553 ['\\n = Spike Spiegel = \\n']\n",
      "5556 ['\\n = No. 1 Squadron RAAF = \\n']\n",
      "5563 [\"\\n = Australia women's national wheelchair basketball team at the 2012 Summer Paralympics = \\n\"]\n",
      "5564 ['\\n = played, W = \\n', '\\n = lost, PF = \\n', '\\n = points against, PD = \\n', '\\n = points, <unk> = \\n']\n",
      "5569 ['\\n = Dungeons & Dragons = \\n']\n",
      "5574 ['\\n = Ninja Gaiden ( 2004 video game ) = \\n']\n",
      "5581 ['\\n = Milutin Bojić = \\n']\n",
      "5588 ['\\n = Royal Question = \\n']\n",
      "5592 ['\\n = The Indian Princess ( play ) = \\n']\n",
      "5596 ['\\n = 1996 Atlantic hurricane season = \\n']\n",
      "5603 ['\\n = Riot Act ( album ) = \\n']\n",
      "5606 ['\\n = Oasis ( band ) = \\n']\n",
      "5614 ['\\n = Harry Potter and the Deathly Hallows = \\n']\n",
      "5623 ['\\n = Apollo Global Management = \\n']\n",
      "5627 [\"\\n = Bankers'Toadies incident = \\n\"]\n",
      "5629 [\"\\n = Hôtel d 'Alluye = \\n\"]\n",
      "5631 ['\\n = The Last Temptation of Homer = \\n']\n",
      "5634 ['\\n = European storm petrel = \\n']\n",
      "5640 ['\\n = <unk> ribbontail ray = \\n']\n",
      "5642 ['\\n = Douglas Jardine = \\n']\n",
      "5656 ['\\n = Maniac Mansion = \\n']\n",
      "5663 ['\\n = East Carolina University = \\n']\n",
      "5669 ['\\n = We Are the World 25 for Haiti = \\n']\n",
      "5672 ['\\n = Gethsemane ( The X @-@ Files ) = \\n']\n",
      "5675 ['\\n = Cultura Profética = \\n']\n",
      "5677 ['\\n = Bridgewater Canal = \\n']\n",
      "5685 ['\\n = Jules Massenet = \\n']\n",
      "5693 ['\\n = Everything in Time = \\n']\n",
      "5695 ['\\n = Tom Driberg = \\n']\n",
      "5704 ['\\n = SMS Pommern = \\n']\n",
      "5706 ['\\n = Sleaford = \\n']\n",
      "5717 ['\\n = Tsugaru clan = \\n']\n",
      "5719 ['\\n = Neptunium = \\n']\n",
      "5731 ['\\n = Cl, Br ) were obtained in 1966 in concentrated LiCl and <unk> solutions, respectively : for the latter, 1970 experiments discovered that the NpO3 + \\n 2 ion could form sulfate complexes in acidic solutions, such as NpO \\n 2SO + \\n 4 and NpO \\n 2 ( SO \\n 4 ) − \\n 2 ; these were found to have higher stability constants than the neptunyl ion ( NpO2 + \\n 2 ). A great many complexes for the other neptunium oxidation states are known : the inorganic ligands involved are the halides, iodate, azide, nitride, nitrate, thiocyanate, sulfate, carbonate, chromate, and phosphate. Many organic ligands are known to be able to be used in neptunium coordination complexes : they include acetate, <unk>, <unk>, lactate, oxalate, malonate, <unk>, <unk>, and citrate. \\n Analogously to its neighbours, uranium and plutonium, the order of the neptunium ions in terms of complex formation ability is Np4 + > NpO2 + \\n 2 ≥ Np3 + > NpO + \\n 2. ( The relative order of the middle two neptunium ions depends on the ligands and solvents used. ) The stability sequence for Np ( IV ), Np ( V ), and Np ( VI ) complexes with monovalent inorganic ligands is F − > H \\n <unk> − \\n 4 > SCN − > NO − \\n 3 > Cl − > ClO − \\n 4 ; the order for divalent inorganic ligands is CO2 − \\n 3 > <unk> − \\n 4 > SO2 − \\n 4. These follow the strengths of the corresponding acids. The divalent ligands are more strongly complexing than the monovalent ones. NpO + \\n 2 can also form the complex ions [ NpO + \\n <unk> + ] ( M = \\n']\n",
      "5732 ['\\n = Michael ( Glee ) = \\n']\n",
      "5738 ['\\n = George Mikan = \\n']\n",
      "5743 ['\\n = Harry Strom = \\n']\n",
      "5746 ['\\n = Wind It Up ( Gwen Stefani song ) = \\n']\n",
      "5748 ['\\n = Tropical Storm Podul ( 2013 ) = \\n']\n",
      "5749 ['\\n = Washington State Route 213 = \\n']\n",
      "5750 ['\\n = M @-@ 10 ( Michigan highway ) = \\n']\n",
      "5753 ['\\n = Vanaja ( film ) = \\n']\n",
      "5757 ['\\n = New York and New Jersey campaign = \\n']\n",
      "5761 ['\\n = Sauganash Hotel = \\n']\n",
      "5762 ['\\n = Ed the Happy Clown = \\n']\n",
      "5769 ['\\n = Lilioid monocots = \\n']\n",
      "5773 ['\\n = Nunney Castle = \\n']\n",
      "5774 ['\\n = John Newham = \\n']\n",
      "5776 ['\\n = Parasaurolophus = \\n']\n",
      "5782 ['\\n = Action of 24 June 1801 = \\n']\n",
      "5784 ['\\n = Burton v. United States = \\n']\n",
      "5790 [\"\\n = ( I've Just Begun ) Having My Fun = \\n\"]\n",
      "5791 ['\\n = River Don Navigation = \\n']\n",
      "5801 ['\\n = The Alliance ( The Office ) = \\n']\n",
      "5803 ['\\n = The Boat Race 1975 = \\n']\n",
      "5804 ['\\n = Reginald de Warenne = \\n']\n",
      "5806 ['\\n = 1988 Winter Olympics = \\n']\n",
      "5811 ['\\n = Harry Kim ( Star Trek ) = \\n']\n",
      "5815 ['\\n = Boyce McDaniel = \\n']\n",
      "5816 ['\\n = What Is... Cliff Clavin? = \\n']\n",
      "5817 ['\\n = Chrono Break = \\n']\n",
      "5819 ['\\n = Whitefriars, Bristol = \\n']\n",
      "5820 ['\\n = National Hurricane Center = \\n']\n",
      "5822 ['\\n = Ranavalona III = \\n']\n",
      "5828 ['\\n = Flight Unlimited = \\n']\n",
      "5835 ['\\n = Octavarium ( album ) = \\n']\n",
      "5838 ['\\n = Peniophora quercina = \\n']\n",
      "5839 ['\\n = Paper War of 1752 – 1753 = \\n']\n",
      "5841 ['\\n = Temperatures Rising = \\n']\n",
      "5846 ['\\n = New York State Route 382 = \\n', '\\n = Zahir al @-@ Umar = \\n']\n",
      "5861 ['\\n = M @-@ 204 ( Michigan highway ) = \\n', '\\n = What Goes Around... Comes Around = \\n']\n",
      "5865 ['\\n = Woodstock Library = \\n']\n",
      "5867 ['\\n = Ragnarök = \\n']\n",
      "5873 ['\\n = Chiswick Bridge = \\n']\n",
      "5874 ['\\n = Kubah = \\n']\n",
      "5876 ['\\n = Broaching ( metalworking ) = \\n']\n",
      "5881 ['\\n = Uncanny Tales ( Canadian pulp magazine ) = \\n']\n",
      "5882 ['\\n = Playmate to Jesus = \\n']\n",
      "5883 ['\\n = A Perfect Circle = \\n']\n",
      "5887 ['\\n = Nguyễn Chánh Thi = \\n']\n",
      "5896 ['\\n = James Newland = \\n']\n",
      "5899 ['\\n = Japanese battleship Yamashiro = \\n']\n",
      "5905 ['\\n = Stuyvesant High School = \\n']\n",
      "5911 ['\\n = George S. Armstrong = \\n']\n",
      "5914 ['\\n = Arizona State Route 85 = \\n']\n",
      "5916 ['\\n = Rugby union at the Summer Olympics = \\n']\n",
      "5919 ['\\n = United States Senate election in New York, 2000 = \\n']\n",
      "5926 ['\\n = The Boat Race 1981 = \\n']\n",
      "5927 ['\\n = Launch Party = \\n']\n",
      "5929 ['\\n = French battleship Courbet ( 1911 ) = \\n']\n",
      "5931 [\"\\n = I'm That Chick = \\n\"]\n",
      "5933 ['\\n = Expo 67 = \\n']\n",
      "5939 ['\\n = British Aerospace Sea Harrier = \\n']\n",
      "5944 ['\\n = Pied currawong = \\n']\n",
      "5948 ['\\n = Love in This Club = \\n']\n",
      "5951 ['\\n = Banagher = \\n']\n",
      "5964 ['\\n = Nancy Mitford = \\n']\n",
      "5973 ['\\n = Amphicoelias = \\n']\n",
      "5977 ['\\n = Kesha = \\n']\n",
      "5983 ['\\n = The Devil Wears Nada = \\n']\n",
      "5986 ['\\n = Ocean Rain = \\n']\n",
      "5989 ['\\n = Washington State Route 161 = \\n']\n",
      "5990 ['\\n = Glutinoglossum glutinosum = \\n']\n",
      "5992 ['\\n = Phil Nevin = \\n']\n",
      "5997 ['\\n = Body Language ( Kylie Minogue album ) = \\n']\n",
      "6002 ['\\n = Heather Mills = \\n']\n",
      "6010 ['\\n = Harry Bassett = \\n']\n",
      "6011 ['\\n = SM UB @-@ 13 = \\n']\n",
      "6014 ['\\n = I Am... Yours : An Intimate Performance at Wynn Las Vegas = \\n']\n",
      "6018 ['\\n = 2012 ( film ) = \\n']\n",
      "6021 ['\\n = New York State Route 306 = \\n']\n",
      "6022 ['\\n = Coming Home ( Diddy – Dirty Money song ) = \\n']\n",
      "6026 ['\\n = Cardiff Blues vs Leicester Tigers ( 2008 – 09 Heineken Cup ) = \\n']\n",
      "6029 ['\\n = Movement for the Intellectually Disabled of Singapore = \\n']\n",
      "6030 ['\\n = Yankee Hotel Foxtrot = \\n']\n",
      "6032 ['\\n = 1948 Summer Olympics torch relay = \\n']\n",
      "6034 ['\\n = 2011 – 12 Washington Capitals season = \\n']\n",
      "6039 ['\\n = Games played ; G = \\n', '\\n = Assists ; Pts = \\n', '\\n = Plus / Minus ; PIM = \\n', '\\n = Games Played ; Min = \\n', '\\n = Wins ; L = \\n', '\\n = Overtime Losses ; GA = \\n', '\\n = Saves ; Sv % = \\n']\n",
      "6040 ['\\n = Cris and Cru Kahui homicides = \\n']\n",
      "6043 ['\\n = Frank Underwood ( House of Cards ) = \\n']\n",
      "6050 ['\\n = Reflections on Having Left a Place of Retirement = \\n']\n",
      "6052 ['\\n = HMS Pioneer ( <unk> ) = \\n']\n",
      "6053 ['\\n = Pythagorean theorem = \\n']\n",
      "6055 ['\\n = AB2. \\n Similarly, it can be shown that rectangle <unk> must have the same area as square <unk> = \\n', '\\n = BD × BK + KL × KC \\n Since BD = \\n', '\\n = BD ( BK + KC ) = \\n']\n",
      "6057 ['\\n = 0, y = \\n', '\\n = c2, there exists a triangle with sides a, b and c, and every such triangle has a right angle between the sides of lengths a and b. \\n An alternative statement is : \\n For any triangle with sides a, b, c, if a2 + b2 = \\n', '\\n = c2. Construct a second triangle with sides of length a and b containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length c = \\n', '\\n = c2, then the triangle is right. \\n If a2 + b2 > c2, then the triangle is acute. \\n If a2 + b2 < c2, then the triangle is obtuse. \\n Edsger Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language : \\n sgn ( α + β − γ ) = \\n']\n",
      "6059 ['\\n = c2, so A + B = \\n', '\\n = C for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle C on its hypotenuse, and two similar right triangles ( A and B ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus A + B = \\n', '\\n = π / 2, ADB becomes a right triangle, r + s = \\n']\n",
      "6061 ['\\n = 2 and n = \\n', '\\n = 3! / 2! ( 3 @-@ 2 )! = \\n', '\\n = 6 / 2 = \\n']\n",
      "6062 ['\\n = 27, b2 = \\n']\n",
      "6065 ['\\n = Signs and Wonders ( The X @-@ Files ) = \\n']\n",
      "6068 ['\\n = Cody Hodgson = \\n']\n",
      "6074 ['\\n = Simba = \\n']\n",
      "6079 ['\\n = Altrincham = \\n']\n",
      "6086 ['\\n = Towson United Methodist Church = \\n']\n",
      "6091 ['\\n = Lovejoy Columns = \\n']\n",
      "6092 ['\\n = The Boat Race 1870 = \\n']\n",
      "6093 ['\\n = Marco Kartodikromo = \\n']\n",
      "6095 ['\\n = Sharptooth houndshark = \\n']\n",
      "6097 ['\\n = 1933 Cuba – Brownsville hurricane = \\n']\n",
      "6099 ['\\n = Tom Swift = \\n']\n",
      "6104 ['\\n = Nannygate = \\n']\n",
      "6110 ['\\n = Royal Gold Cup = \\n']\n",
      "6117 ['\\n = Giant freshwater stingray = \\n']\n",
      "6119 ['\\n = Live action role @-@ playing game = \\n']\n",
      "6123 ['\\n = Friday ( Rebecca Black song ) = \\n']\n",
      "6128 ['\\n = Homeless ( Leona Lewis song ) = \\n']\n",
      "6129 ['\\n = Until the Quiet Comes = \\n']\n",
      "6135 ['\\n = George Eyre = \\n']\n",
      "6139 ['\\n = Queer Eye = \\n']\n",
      "6141 ['\\n = Nothhelm = \\n']\n",
      "6142 ['\\n = Duff Cooley = \\n']\n",
      "6143 ['\\n = England expects that every man will do his duty = \\n']\n",
      "6145 ['\\n = Waddesdon Road railway station = \\n']\n",
      "6147 ['\\n = Son of Three = \\n']\n",
      "6148 ['\\n = Ef : A Fairy Tale of the Two. = \\n']\n",
      "6155 ['\\n = Social identity theory = \\n']\n",
      "6157 ['\\n = SMS Jagd = \\n']\n",
      "6158 ['\\n = William Shirley = \\n']\n",
      "6166 ['\\n = 1925 Florida tropical storm = \\n']\n",
      "6168 ['\\n = Joan Curran = \\n']\n",
      "6169 ['\\n = The Last Temptation of Krust = \\n']\n",
      "6171 ['\\n = Reg Saunders = \\n']\n",
      "6174 ['\\n = Hurricane Cosme ( 1989 ) = \\n']\n",
      "6175 ['\\n = Hurricane Greta – Olivia = \\n']\n",
      "6177 ['\\n = Radiohead = \\n']\n",
      "6188 ['\\n = White @-@ eared titi = \\n']\n",
      "6190 [\"\\n = Lamellerie's expedition = \\n\"]\n",
      "6192 [\"\\n = The Witch's Tales = \\n\"]\n",
      "6193 [\"\\n = Djibouti women's national football team = \\n\"]\n",
      "6194 [\"\\n = L 'Arianna = \\n\"]\n",
      "6198 ['\\n = Speaker of the United States House of Representatives election, October 2015 = \\n']\n",
      "6202 ['\\n = Montana Vigilantes = \\n']\n",
      "6212 ['\\n = Viewtiful Joe = \\n']\n",
      "6216 ['\\n = Constitutional history of Zimbabwe = \\n']\n",
      "6220 ['\\n = National Intercollegiate Band = \\n']\n",
      "6222 ['\\n = Hurricane Lily ( 1971 ) = \\n']\n",
      "6223 ['\\n = Fidel Castro = \\n']\n",
      "6242 ['\\n = 1896 Michigan Wolverines football team = \\n']\n",
      "6245 ['\\n = Destination Moon ( comics ) = \\n']\n",
      "6249 ['\\n = Greg Skrepenak = \\n']\n",
      "6252 ['\\n = Weinreb ketone synthesis = \\n']\n",
      "6253 ['\\n = Through a Glass Darkly ( Koen novel ) = \\n']\n",
      "6256 ['\\n = United Airlines Flight 736 = \\n']\n",
      "6259 ['\\n = A Rugrats Passover = \\n']\n",
      "6262 ['\\n = A1 ( Croatia ) = \\n']\n",
      "6268 ['\\n = Typhoon Sarah ( 1959 ) = \\n']\n",
      "6270 ['\\n = Dunster Working Watermill = \\n', '\\n = German submarine U @-@ 64 ( 1939 ) = \\n']\n",
      "6271 ['\\n = HMS Exeter ( 68 ) = \\n']\n",
      "6276 ['\\n = Pennsylvania Route 666 = \\n']\n",
      "6277 ['\\n = Nandanar = \\n']\n",
      "6282 ['\\n = M249 light machine gun = \\n']\n",
      "6287 ['\\n = Ursa Minor = \\n']\n",
      "6291 ['\\n = Japanese settlement in Palau = \\n']\n",
      "6294 ['\\n = Tropical Storm Zeta = \\n']\n",
      "6296 ['\\n = Connor ( Angel ) = \\n']\n",
      "6303 ['\\n = Profumo affair = \\n']\n",
      "6311 ['\\n = Peter Lumsden = \\n']\n",
      "6313 ['\\n = Three Studies for Figures at the Base of a Crucifixion = \\n']\n",
      "6317 ['\\n = King @-@ Size Homer = \\n']\n",
      "6319 ['\\n = Spring Creek Dam = \\n']\n",
      "6323 ['\\n = New Jersey Route 42 = \\n']\n",
      "6325 ['\\n = 47 Ursae Majoris c = \\n', '\\n = Pentemont Abbey = \\n']\n",
      "6327 ['\\n = Dyspanopeus sayi = \\n']\n",
      "6329 ['\\n = Field Spaniel = \\n']\n",
      "6330 ['\\n = Scandium = \\n']\n",
      "6333 ['\\n = Interstate 70 in Utah = \\n']\n",
      "6338 [\"\\n = Can 't Be Tamed ( song ) = \\n\"]\n",
      "6342 ['\\n = Tokyo Tower = \\n']\n",
      "6345 ['\\n = Camping ( Parks and Recreation ) = \\n']\n",
      "6347 ['\\n = Tiffany Doggett = \\n']\n",
      "6350 ['\\n = Victoria Cross = \\n']\n",
      "6357 ['\\n = USS Philadelphia ( 1776 ) = \\n']\n",
      "6359 ['\\n = Mundo Perdido, Tikal = \\n']\n",
      "6368 ['\\n = A8 ( Croatia ) = \\n']\n",
      "6372 ['\\n = Lil Freak = \\n']\n",
      "6374 ['\\n = Parks and Recreation ( season 1 ) = \\n']\n",
      "6380 ['\\n = Millennium ( season 1 ) = \\n']\n",
      "6381 ['\\n = Algonquin Hotel = \\n']\n",
      "6383 ['\\n = Pilot ( Devious Maids ) = \\n']\n",
      "6386 ['\\n = Dassault Rafale = \\n']\n",
      "6400 ['\\n = A Journey = \\n']\n",
      "6405 ['\\n = Aaron Loup = \\n']\n",
      "6408 ['\\n = System Shock 2 = \\n']\n",
      "6412 ['\\n = Battle of Plataea = \\n']\n",
      "6420 ['\\n = Henry Martyn = \\n']\n",
      "6422 ['\\n = Carl Hans Lody = \\n']\n",
      "6436 ['\\n = Ontario Highway 77 = \\n']\n",
      "6437 ['\\n = Chambersburg, Pennsylvania = \\n']\n",
      "6444 [\"\\n = Soldier ( Destiny's Child song ) = \\n\"]\n",
      "6449 ['\\n = SM U @-@ 40 ( Austria @-@ Hungary ) = \\n']\n",
      "6452 ['\\n = Battle of Marathon = \\n']\n",
      "6461 ['\\n = Glenrothes = \\n']\n",
      "6472 ['\\n = Liberum veto = \\n']\n",
      "6474 ['\\n = The Boat Race 1865 = \\n']\n",
      "6475 ['\\n = Mount Vesuvius = \\n']\n",
      "6476 ['\\n = \" not \" prefixed to a root from or related to the Greek word <unk> = \\n', '\\n = \" I hurl \" and <unk> \" violence \", \" hurling violence \", * <unk>, taking advantage of the collateral form. \\n From an Indo @-@ European root, * <unk> < * <unk> < * ( a ) <unk>, \" shine \" sense \" the one who lightens \", through Latin or <unk>. \\n From an Indo @-@ European root * wes = \\n']\n",
      "6482 [\"\\n = Grey's Anatomy ( season 6 ) = \\n\"]\n",
      "6485 ['\\n = 1899 Carrabelle hurricane = \\n']\n",
      "6487 ['\\n = Enter the Wu @-@ Tang ( 36 Chambers ) = \\n']\n",
      "6488 ['\\n = 36. \\n In reference to the 1978 kung fu film The 36th Chamber of Shaolin that the group enjoyed watching, the Clan considered themselves as lyrical masters of the 36 chambers, and arrived onto the rap scene while appearing to be ahead, and more advanced over others, with \" knowledge of 36 chambers of hip hop music when everyone else in hip hop was striving to attain the knowledge of 35 lessons. \" Also, while the human body has 108 pressure points ( 1 + 0 + 8 = \\n', '\\n = 45 ) ( 4 + 5 = \\n']\n",
      "6492 ['\\n = Barn owl = \\n']\n",
      "6500 ['\\n = Loch Arkaig treasure = \\n']\n",
      "6501 ['\\n = History of Indiana = \\n']\n",
      "6516 ['\\n = Mongol siege of Kaifeng = \\n']\n",
      "6519 ['\\n = Rymdkapsel = \\n']\n",
      "6520 ['\\n = Beefsteak ( banquet ) = \\n']\n",
      "6522 ['\\n = Fermium = \\n']\n",
      "6523 ['\\n = 100 ) required more material, as the yield was expected to be at least an order of magnitude lower than that of element 99, and so contaminated coral from the Enewetak atoll ( where the test had taken place ) was shipped to the University of California Radiation Laboratory in Berkeley, California, for processing and analysis. About two months after the test, a new component was isolated emitting high @-@ energy α @-@ particles ( 7 @.@ 1 MeV ) with a half @-@ life of about a day. With such a short half @-@ life, it could only arise from the β − decay of an isotope of einsteinium, and so had to be an isotope of the new element 100 : it was quickly identified as 255Fm ( t = \\n']\n",
      "6524 ['\\n = 20 @.@ 07 ( 7 ) hours ) as this isotope can be easily isolated as required as the decay product of 255Es ( t1 / 2 = \\n']\n",
      "6525 ['\\n = Hurricane Bob = \\n']\n",
      "6531 ['\\n = Banjo @-@ Tooie = \\n']\n",
      "6534 [\"\\n = Hit'Em Up = \\n\"]\n",
      "6538 ['\\n = Maryland Route 413 = \\n']\n",
      "6540 ['\\n = Miniopterus aelleni = \\n']\n",
      "6542 ['\\n = Not Your Kind of People = \\n']\n",
      "6547 ['\\n = Playtex = \\n']\n",
      "6550 ['\\n = Solo ( Boyd novel ) = \\n']\n",
      "6553 ['\\n = Middlesbrough F.C. = \\n']\n",
      "6559 ['\\n = Government of the Han dynasty = \\n']\n",
      "6572 ['\\n = Subtropical cyclone = \\n']\n",
      "6575 ['\\n = The Orb = \\n']\n",
      "6583 ['\\n = The Boat Race 1948 = \\n']\n",
      "6584 ['\\n = Bobby Pulido = \\n']\n",
      "6587 ['\\n = Siege of Port Royal ( 1707 ) = \\n']\n",
      "6590 ['\\n = Survivor Series ( 2007 ) = \\n']\n",
      "6593 ['\\n = More Demi Moore = \\n']\n",
      "6595 [\"\\n = Big Girls Don 't Cry ( book ) = \\n\"]\n",
      "6597 ['\\n = Sabre Wulf = \\n']\n",
      "6599 ['\\n = Fasci Siciliani = \\n']\n",
      "6604 ['\\n = 3rd Ranger Infantry Company ( United States ) = \\n']\n",
      "6609 ['\\n = Varagavank = \\n']\n",
      "6612 ['\\n = British cavalry during the First World War = \\n']\n",
      "6617 [\"\\n = Skeptic's Toolbox = \\n\"]\n",
      "6618 ['\\n = Marty Mayberry = \\n']\n",
      "6619 ['\\n = Red Museum = \\n']\n",
      "6621 ['\\n = Henry Chadwick ( theologian ) = \\n']\n",
      "6624 ['\\n = Proto @-@ Indo @-@ European root = \\n']\n",
      "6628 ['\\n = M @-@ 14 ( Michigan highway ) = \\n']\n",
      "6629 ['\\n = The Ghost Network = \\n']\n",
      "6632 ['\\n = Breathing ( Jason Derulo song ) = \\n']\n",
      "6634 ['\\n = Scottish art in the eighteenth century = \\n']\n",
      "6638 ['\\n = Clathrus ruber = \\n']\n",
      "6641 ['\\n = Garamond = \\n']\n",
      "6648 ['\\n = Texas Recreational Road 8 = \\n']\n",
      "6649 ['\\n = U.S. Route 223 = \\n']\n",
      "6652 ['\\n = Ralph Richardson = \\n']\n",
      "6662 ['\\n = Battle of Halmyros = \\n']\n",
      "6668 ['\\n = SS Kommandøren = \\n']\n",
      "6671 ['\\n = Warren County, Indiana = \\n']\n",
      "6678 ['\\n = 2011 Cofidis season = \\n']\n",
      "6683 ['\\n = James VI and I = \\n']\n",
      "6692 ['\\n = Jahanpanah = \\n']\n",
      "6696 ['\\n = Roon @-@ class cruiser = \\n']\n",
      "6699 ['\\n = Haymarket affair = \\n']\n",
      "6708 ['\\n = M @-@ 185 ( Michigan highway ) = \\n']\n",
      "6711 ['\\n = Learie Constantine = \\n']\n",
      "6722 ['\\n = Ucu Agustin = \\n']\n",
      "6724 ['\\n = Hurricane Howard ( 2004 ) = \\n']\n",
      "6725 ['\\n = Irving Gottesman = \\n']\n",
      "6729 ['\\n = Third Amendment to the United States Constitution = \\n']\n",
      "6731 ['\\n = Psittacosaurus = \\n']\n",
      "6741 ['\\n = Wait & See ( Risk ) = \\n']\n",
      "6743 ['\\n = Battle of Bonchurch = \\n']\n",
      "6745 ['\\n = Tim Rogers ( journalist ) = \\n']\n",
      "6747 ['\\n = Battle of Borgerhout = \\n']\n",
      "6752 ['\\n = Lazarus W. Powell = \\n']\n",
      "6753 ['\\n = Gymnopilus maritimus = \\n']\n",
      "6757 ['\\n = Brooks & Dunn = \\n']\n",
      "6763 ['\\n = E language = \\n']\n",
      "6764 ['\\n = Barlow Road = \\n']\n",
      "6766 ['\\n = Barbados at the 2008 Summer Olympics = \\n']\n",
      "6768 ['\\n = Qualified for the next round \\n q = \\n', '\\n = National record \\n N / A = \\n']\n",
      "6769 ['\\n = Medal race ; EL = \\n']\n",
      "6770 ['\\n = 500 euro note = \\n']\n",
      "6772 ['\\n = Tropical Storm Bret ( 1981 ) = \\n']\n",
      "6774 ['\\n = Operation Ironside = \\n']\n",
      "6776 ['\\n = Cody Ross = \\n']\n",
      "6781 ['\\n = Keith Houchen = \\n']\n",
      "6786 ['\\n = Bad Elk v. United States = \\n']\n",
      "6788 ['\\n = Ctenophora = \\n']\n",
      "6796 ['\\n = K @-@ 140 ( Kansas highway ) = \\n']\n",
      "6797 ['\\n = Ianto Jones = \\n']\n",
      "6806 ['\\n = Hurricane Iniki = \\n']\n",
      "6809 [\"\\n = Fightin'Texas Aggie Band = \\n\"]\n",
      "6815 ['\\n = Crucifixion darkness = \\n']\n",
      "6818 ['\\n = Sleight of hand = \\n']\n",
      "6819 ['\\n = Southern Rhodesia in World War I = \\n']\n",
      "6834 ['\\n = Road to Europe = \\n']\n",
      "6836 ['\\n = Three Horses Beer = \\n']\n",
      "6838 ['\\n = George Headley = \\n']\n",
      "6849 ['\\n = Interstate 81 in West Virginia = \\n']\n",
      "6850 ['\\n = Literature in early modern Scotland = \\n']\n",
      "6855 ['\\n = Ridge Racer Revolution = \\n']\n",
      "6856 ['\\n = Soul Sound = \\n']\n",
      "6858 ['\\n = Miguel Treviño Morales = \\n']\n",
      "6863 ['\\n = R & B Junkie = \\n']\n",
      "6865 ['\\n = Hurricane Kristy ( 2006 ) = \\n']\n",
      "6867 ['\\n = Baden @-@ Powell House = \\n']\n",
      "6868 ['\\n = Botany = \\n']\n",
      "6879 ['\\n = Odin @-@ class coastal defense ship = \\n']\n",
      "6881 ['\\n = Siege of Damascus ( 1148 ) = \\n']\n",
      "6884 ['\\n = Tropical Storm Josephine ( 1996 ) = \\n']\n",
      "6887 ['\\n = Marry You = \\n']\n",
      "6889 ['\\n = Erie, Pennsylvania = \\n']\n",
      "6896 ['\\n = Sid Luckman = \\n']\n",
      "6898 ['\\n = Battle of Gospić = \\n']\n",
      "6901 ['\\n = Sidney Mashbir = \\n']\n",
      "6904 ['\\n = Black Coffee ( All Saints song ) = \\n']\n",
      "6909 ['\\n = Juniata County, Pennsylvania = \\n']\n",
      "6918 ['\\n = Olivia Shakespear = \\n']\n",
      "6925 ['\\n = Coelurus = \\n', '\\n = hollow + <unk>, oura = \\n']\n",
      "6939 ['\\n = Carom billiards = \\n']\n",
      "6943 ['\\n = 2012 Hawaii Bowl = \\n']\n",
      "6947 ['\\n = SM U @-@ 21 ( Germany ) = \\n']\n",
      "6951 ['\\n = Homer the Whopper = \\n']\n",
      "6952 ['\\n = Overhill Cherokee = \\n']\n",
      "6958 ['\\n = Least weasel = \\n']\n",
      "6963 ['\\n = Aseroe coccinea = \\n']\n",
      "6964 ['\\n = Spit & Eggs = \\n']\n",
      "6967 ['\\n = United Nations = \\n']\n",
      "6976 ['\\n = 12 Days = \\n']\n",
      "6978 ['\\n = Salesforce Marketing Cloud = \\n']\n",
      "6979 ['\\n = Belgian ship A4 = \\n']\n",
      "6980 ['\\n = Egypt at the 2012 Summer Olympics = \\n']\n",
      "6981 ['\\n = Competitor won the match ; L = \\n']\n",
      "6982 ['\\n = Qualified for the next round ; N / A = \\n', '\\n = Competitor lost the match ; BM = \\n', '\\n = Competitor won the match ; L = \\n']\n",
      "6983 ['\\n = Competitor won the match ; L = \\n', '\\n = Bronze medal match ; N / A = \\n']\n",
      "6984 ['\\n = Floor exercise ; PH = \\n', '\\n = Rings ; V = \\n', '\\n = Parallel bars ; HB = \\n', '\\n = Round not applicable for the event \\n Women \\n F = \\n', '\\n = <unk> horse ; UB = \\n', '\\n = Balance beam ; N / A = \\n']\n",
      "6985 ['\\n = Points earned towards total score ; WR = \\n', '\\n = Olympic record ; NR = \\n', '\\n = Qualified for final A ( medal ) ; FB = \\n', '\\n = Qualified for final C ( non @-@ medal ) ; FD = \\n', '\\n = Qualified for final E ( non @-@ medal ) ; FF = \\n', '\\n = Qualified for semifinals A / B ; SC / D = \\n', '\\n = Qualified for semifinals E / F ; QF = \\n', '\\n = Qualified for repechage ; WR = \\n', '\\n = Olympic record ; NR = \\n', '\\n = Round not applicable for the event ; Bye = \\n', '\\n = Medal race ; EL = \\n']\n",
      "6986 ['\\n = World record ; OR = \\n', '\\n = Qualified for the next round ; WR = \\n', '\\n = Olympic record ; NR = \\n', '\\n = Round not applicable for the event ; Bye = \\n']\n",
      "6987 ['\\n = Bronze medal match ; N / A = \\n', '\\n = Athlete not required to compete in round ; SDP = \\n']\n",
      "6988 ['\\n = World record ; OR = \\n']\n",
      "6989 ['\\n = Bronze medal match ; VT = \\n', '\\n = Decision by Points – the loser with technical points ; PO = \\n', '\\n = Siege of Sidney Street = \\n']\n",
      "6996 ['\\n = Delaware Route 41 = \\n']\n",
      "6997 ['\\n = In Marge We Trust = \\n']\n",
      "6998 ['\\n = George Brett ( general ) = \\n']\n",
      "7001 ['\\n = Pale crag martin = \\n']\n",
      "7005 ['\\n = Grus ( constellation ) = \\n']\n",
      "7008 ['\\n = Pedro del Valle = \\n']\n",
      "7010 ['\\n = September 11 attacks = \\n']\n",
      "7025 ['\\n = Loyalty to Loyalty = \\n']\n",
      "7028 ['\\n = Lice ( The Office ) = \\n']\n",
      "7030 ['\\n = Christina Milian ( album ) = \\n']\n",
      "7033 ['\\n = The Old Man and the \" C \" Student = \\n']\n",
      "7035 ['\\n = Lynching of Jesse Washington = \\n']\n",
      "7041 ['\\n = Love Kraft = \\n']\n",
      "7043 ['\\n = SS Fort Stikine = \\n']\n",
      "7047 ['\\n = A Trip to the Moon = \\n']\n",
      "7054 ['\\n = Rivadavia @-@ class battleship = \\n']\n",
      "7059 ['\\n = Cyclogenesis = \\n']\n",
      "7061 ['\\n = Tartu Offensive = \\n']\n",
      "7064 ['\\n = Tropical Storm Gabrielle ( 1995 ) = \\n']\n",
      "7065 ['\\n = Connecticut Indian Land Claims Settlement = \\n']\n",
      "7067 ['\\n = North American XB @-@ 70 Valkyrie = \\n']\n",
      "7076 [\"\\n = Eurovision Song Contest's Greatest Hits = \\n\"]\n",
      "7077 ['\\n = Broken World ( Millennium ) = \\n']\n",
      "7079 ['\\n = Battle of Jajau = \\n']\n",
      "7081 ['\\n = Green Wing = \\n']\n",
      "7087 ['\\n = No. 457 Squadron RAAF = \\n']\n",
      "7090 ['\\n = Development of The Last of Us = \\n']\n",
      "7098 ['\\n = Ma Chengyuan = \\n']\n",
      "7100 ['\\n = Scotland under the Commonwealth = \\n']\n",
      "7104 ['\\n = Dæmonen = \\n']\n",
      "7105 ['\\n = 1996 Football League Third Division play @-@ off Final = \\n']\n",
      "7107 ['\\n = East Indies theatre of the French Revolutionary Wars = \\n']\n",
      "7115 ['\\n = Cooke and Wheatstone telegraph = \\n']\n",
      "7118 ['\\n = Caleb Strong = \\n']\n",
      "7121 ['\\n = Press pass = \\n']\n",
      "7122 ['\\n = Winkler County nurse whistleblower case = \\n']\n",
      "7125 ['\\n = ESRB re @-@ rating of The Elder Scrolls IV : Oblivion = \\n']\n",
      "7132 ['\\n = A Tale of Two Cities ( Lost ) = \\n']\n",
      "7135 ['\\n = Heffernan v. City of Paterson = \\n']\n",
      "7139 ['\\n = Regulamentul Organic = \\n']\n",
      "7146 ['\\n = Western Chalukya Empire = \\n']\n",
      "7157 ['\\n = Pinoy = \\n']\n",
      "7158 ['\\n = California condor = \\n']\n",
      "7164 ['\\n = Spencer ( surname ) = \\n']\n",
      "7169 ['\\n = Main sequence = \\n']\n",
      "7171 [\"\\n = <unk> \\n where σ is the Stefan – Boltzmann constant. As the position of a star on the HR diagram shows its approximate luminosity, this relation can be used to estimate its radius. \\n The mass, radius and luminosity of a star are closely interlinked, and their respective values can be approximated by three relations. First is the Stefan – Boltzmann law, which relates the luminosity L, the radius R and the surface temperature Teff. Second is the mass – luminosity relation, which relates the luminosity L and the mass M. Finally, the relationship between M and R is close to linear. The ratio of M to R increases by a factor of only three over 2 @.@ 5 orders of magnitude of M. This relation is roughly proportional to the star's inner temperature TI, and its extremely slow increase reflects the fact that the rate of energy generation in the core strongly depends on this temperature, whereas it has to fit the mass – luminosity relation. Thus, a too high or too low temperature will result in stellar instability. \\n A better approximation is to take ε = \\n\"]\n",
      "7175 ['\\n = SS Christopher Columbus = \\n']\n",
      "7177 ['\\n = Jim Bottomley = \\n']\n",
      "7180 ['\\n = HMS Vigilant ( 1777 ) = \\n']\n",
      "7181 ['\\n = David Falk = \\n']\n",
      "7187 ['\\n = Auricularia auricula @-@ judae = \\n']\n",
      "7192 ['\\n = Jersey Act = \\n']\n",
      "7194 ['\\n = Little Wapwallopen Creek = \\n']\n",
      "7198 [\"\\n = What It's Like Being Alone = \\n\"]\n",
      "7201 ['\\n = Economy of England in the Middle Ages = \\n']\n",
      "7213 ['\\n = Mortensrud ( station ) = \\n']\n",
      "7214 ['\\n = Vorbunker = \\n']\n",
      "7216 ['\\n = Francis Amasa Walker = \\n']\n",
      "7223 ['\\n = Manal al @-@ Sharif = \\n']\n",
      "7225 ['\\n = Samus Aran = \\n']\n",
      "7230 ['\\n = Morihei Ueshiba = \\n']\n",
      "7235 [\"\\n = Deuce'n Domino = \\n\"]\n",
      "7236 ['\\n = My Love Is Pink = \\n']\n",
      "7237 ['\\n = Moses Gill = \\n']\n",
      "7239 ['\\n = Meteorological history of Hurricane Katrina = \\n']\n",
      "7241 ['\\n = Bernard Stone = \\n']\n",
      "7245 ['\\n = History of the board game Monopoly = \\n']\n",
      "7255 ['\\n = Humbert Roque Versace = \\n']\n",
      "7258 ['\\n = Plug @-@ in electric vehicle = \\n']\n",
      "7289 ['\\n = Wonder World Tour ( Miley Cyrus ) = \\n']\n",
      "7294 ['\\n = Jon Huntsman presidential campaign, 2012 = \\n']\n",
      "7298 ['\\n = Robin Olds = \\n']\n",
      "7307 ['\\n = Indio Comahue Monument = \\n']\n",
      "7308 ['\\n = 2012 Football League Cup Final = \\n']\n",
      "7314 ['\\n = The Quest Begins = \\n']\n",
      "7316 ['\\n = Ancient Egyptian literature = \\n']\n",
      "7326 ['\\n = Music of Barbados = \\n']\n",
      "7332 ['\\n = Brazilian Dreams = \\n']\n",
      "7333 ['\\n = Hurricane Allen = \\n']\n",
      "7336 ['\\n = 2000 – 01 South @-@ West Indian Ocean cyclone season = \\n']\n",
      "7342 ['\\n = Vnukovo Airlines Flight 2801 = \\n']\n",
      "7346 ['\\n = Alapalooza = \\n']\n",
      "7348 ['\\n = HMS Cochrane ( 1905 ) = \\n']\n",
      "7350 ['\\n = French destroyer Jaguar = \\n']\n",
      "7351 ['\\n = The Boat Race 1841 = \\n']\n",
      "7352 ['\\n = Polaris expedition = \\n']\n",
      "7357 ['\\n = Ayumi Hamasaki = \\n']\n",
      "7369 ['\\n = HMCS Protecteur ( AOR 509 ) = \\n']\n",
      "7372 ['\\n = Ipswich serial murders = \\n']\n",
      "7377 ['\\n = Arlen F. Gregorio = \\n']\n",
      "7378 ['\\n = German destroyer Z11 Bernd von Arnim = \\n']\n",
      "7380 ['\\n = Joe Darling = \\n']\n",
      "7384 ['\\n = 2012 Gatorade Duels = \\n']\n",
      "7389 ['\\n = Vainglory ( video game ) = \\n']\n",
      "7390 ['\\n = Henry Hoʻolulu Pitman = \\n']\n",
      "7397 ['\\n = Red rail = \\n']\n",
      "7401 ['\\n = Sergeant Reckless = \\n']\n",
      "7405 ['\\n = Jaws : The Revenge = \\n']\n",
      "7410 ['\\n = Scene7 = \\n']\n",
      "7413 ['\\n = Atheism = \\n']\n",
      "7423 ['\\n = Joust 2 : Survival of the Fittest = \\n']\n",
      "7424 ['\\n = Revolt of the Comuneros ( Paraguay ) = \\n']\n",
      "7432 ['\\n = Nikolai Tikhonov = \\n']\n",
      "7435 ['\\n = CQC @-@ 6 = \\n']\n",
      "7437 ['\\n = Robert Dover ( equestrian ) = \\n']\n",
      "7438 ['\\n = Richard M. Scrushy = \\n']\n",
      "7441 ['\\n = Subject 13 = \\n']\n",
      "7444 ['\\n = Indiana Territory = \\n']\n",
      "7451 ['\\n = Big Bang = \\n']\n",
      "7456 [\"\\n = <unk>, \\n where \\n v is the recessional velocity of the galaxy or other distant object, \\n D is the comoving distance to the object, and \\n H0 is Hubble's constant, measured to be 70 @.@ 4 + 1 @.@ 3 \\n − 1 @.@ 4 km / s / Mpc by the WMAP probe. \\n Hubble's law has two possible explanations. Either we are at the center of an explosion of galaxies — which is untenable given the Copernican principle — or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker. \\n The theory requires the relation v = \\n\"]\n",
      "7461 ['\\n = Hasta la Raíz = \\n']\n",
      "7465 ['\\n = Nebraska Highway 250 = \\n', '\\n = Americium = \\n']\n",
      "7468 ['\\n = 346 @.@ 8 pm and c = \\n']\n",
      "7470 ['\\n = 896 @.@ 3 ± 0 @.@ 8 pm, b = \\n', '\\n = 453 @.@ 2 ± 0 @.@ 6 pm \\n <unk> <unk> : a = \\n', '\\n = 712 @.@ 1 ± 0 @.@ 3 pm. They can also be prepared by reacting metallic americium with an appropriate mercury halide <unk>, where X = \\n']\n",
      "7473 ['\\n = Peter ( Fringe ) = \\n']\n",
      "7476 ['\\n = Theory of Literature = \\n']\n",
      "7483 ['\\n = HMS Valiant ( 1863 ) = \\n']\n",
      "7485 ['\\n = George Macaulay = \\n']\n",
      "7491 ['\\n = A Trick of the Tail = \\n']\n",
      "7493 ['\\n = James Graham ( British Army soldier ) = \\n']\n",
      "7497 ['\\n = Hanky Panky ( Madonna song ) = \\n']\n",
      "7500 ['\\n = Tucker : The Man and His Dream = \\n']\n",
      "7503 ['\\n = Aggie Bonfire = \\n']\n",
      "7509 ['\\n = George Pickett = \\n']\n",
      "7515 ['\\n = Hurricane Fred ( 2015 ) = \\n']\n",
      "7518 ['\\n = Lake Vostok = \\n']\n",
      "7522 ['\\n = Get Sexy = \\n']\n",
      "7524 ['\\n = Operation Pedestal = \\n']\n",
      "7536 [\"\\n = Tomorrow's Modern Boxes = \\n\"]\n",
      "7538 ['\\n = El Celler de Can Roca = \\n']\n",
      "7540 ['\\n = 2012 Delhi gang rape = \\n']\n",
      "7550 ['\\n = Boden Fortress = \\n']\n",
      "7561 ['\\n = Jeff Hardy = \\n']\n",
      "7574 ['\\n = Wagiman language = \\n']\n",
      "7580 ['\\n = The Garden of Words = \\n']\n",
      "7588 ['\\n = Polio vaccine = \\n']\n",
      "7593 ['\\n = Raid on Unadilla and Onaquaga = \\n']\n",
      "7594 ['\\n = Salford, Greater Manchester = \\n']\n",
      "7605 ['\\n = Bærum Tunnel = \\n']\n",
      "7606 ['\\n = HMS Fame ( <unk> ) = \\n']\n",
      "7610 ['\\n = Smokers v Non @-@ Smokers = \\n']\n",
      "7612 ['\\n = Masked shrike = \\n']\n",
      "7615 ['\\n = Space Science Fiction Magazine = \\n']\n",
      "7616 ['\\n = 2007 Pacific hurricane season = \\n']\n",
      "7621 ['\\n = Vengeance ( 2006 ) = \\n']\n",
      "7626 ['\\n = Red @-@ headed myzomela = \\n']\n",
      "7628 ['\\n = F.E.A.R. = \\n']\n",
      "7633 ['\\n = SM U @-@ 31 ( Austria @-@ Hungary ) = \\n']\n",
      "7636 ['\\n = Adelaide Anne Procter = \\n']\n",
      "7638 ['\\n = Darius Morris = \\n']\n",
      "7641 ['\\n = Forza Motorsport 4 = \\n']\n",
      "7645 ['\\n = Super Mario 64 DS = \\n']\n",
      "7648 ['\\n = Apogee Stadium = \\n']\n",
      "7652 ['\\n = 1939 German ultimatum to Lithuania = \\n']\n",
      "7654 ['\\n = Neil Brooks = \\n']\n",
      "7660 ['\\n = Irruputuncu = \\n']\n",
      "7663 ['\\n = Broken Sword : The Shadow of the Templars = \\n']\n",
      "7668 ['\\n = Don Tallon with the Australian cricket team in England in 1948 = \\n']\n",
      "7673 ['\\n = Limp Bizkit = \\n']\n",
      "7682 ['\\n = Battle of Höchstädt ( 1800 ) = \\n']\n",
      "7685 ['\\n = York Park = \\n']\n",
      "7689 ['\\n = Birkebeineren Ski Stadium = \\n']\n",
      "7691 ['\\n = Oil shale in Estonia = \\n']\n",
      "7705 ['\\n = Hemorrhoid = \\n']\n",
      "7709 ['\\n = Frank Tarr = \\n']\n",
      "7711 ['\\n = Miscellaneous solo piano compositions ( Rachmaninoff ) = \\n']\n",
      "7712 ['\\n = Han campaigns against Minyue = \\n']\n",
      "7714 ['\\n = Non @-@ constituency Member of Parliament = \\n']\n",
      "7719 [\"\\n = Eliel Saarinen's Tribune Tower design = \\n\"]\n",
      "7720 ['\\n = Evolution ( advertisement ) = \\n']\n",
      "7724 ['\\n = 2011 NATO attack in Pakistan = \\n']\n",
      "7735 ['\\n = Tremplin du Praz = \\n']\n",
      "7737 ['\\n = Ontario Highway 412 = \\n']\n",
      "7738 ['\\n = Mayored to the Mob = \\n']\n",
      "7740 ['\\n = Underneath It All = \\n']\n",
      "7741 ['\\n = <unk> sparrow = \\n']\n",
      "7745 ['\\n = Aruba at the 2004 Summer Olympics = \\n', '\\n = Qualified for the next round \\n q = \\n', '\\n = National record \\n N / A = \\n']\n",
      "7746 ['\\n = 373rd ( Croatian ) Infantry Division ( Wehrmacht ) = \\n']\n",
      "7751 ['\\n = Dick Turpin = \\n']\n",
      "7759 ['\\n = 1933 FA Cup Final = \\n']\n",
      "7761 ['\\n = Arbiter ( Halo ) = \\n']\n",
      "7766 ['\\n = Guitar Hero : Warriors of Rock = \\n']\n",
      "7773 ['\\n = Tom Pryce = \\n']\n",
      "7778 ['\\n = 2000 German Grand Prix = \\n']\n",
      "7783 ['\\n = Lakselv Airport, Banak = \\n']\n",
      "7786 ['\\n = Warning ( Green Day album ) = \\n']\n",
      "7789 ['\\n = Welcome to the Hellmouth = \\n']\n",
      "7794 ['\\n = Parineeta ( 2005 film ) = \\n']\n",
      "7798 ['\\n = Japanese aircraft carrier Amagi = \\n']\n",
      "7800 ['\\n = Finn M. W. Caspersen = \\n']\n",
      "7804 ['\\n = Nine Hours, Nine Persons, Nine Doors = \\n']\n",
      "7809 ['\\n = Omarska camp = \\n']\n",
      "7813 ['\\n = Monsters ( 2010 film ) = \\n']\n",
      "7817 ['\\n = James Ferguson, Lord Pitfour = \\n']\n",
      "7819 ['\\n = American Arts Commemorative Series medallions = \\n']\n",
      "7821 ['\\n = Orchha Fort complex = \\n']\n",
      "7823 ['\\n = Cyclone Gretelle = \\n']\n",
      "7825 ['\\n = Ayu Tsukimiya = \\n']\n",
      "7829 ['\\n = Variation of Trusts Act 1958 = \\n']\n",
      "7832 ['\\n = Hurricane Gordon ( 1994 ) = \\n']\n",
      "7837 ['\\n = Travis Ishikawa = \\n']\n",
      "7841 ['\\n = Mycena adonis = \\n']\n",
      "7843 ['\\n = Chi ( Chobits ) = \\n']\n",
      "7845 ['\\n = Defender ( 1981 video game ) = \\n']\n",
      "7850 ['\\n = New York State Route 474 = \\n']\n",
      "7851 ['\\n = Hurricane Florence ( 2000 ) = \\n']\n",
      "7852 ['\\n = John Cooke ( Royal Navy officer ) = \\n']\n",
      "7855 ['\\n = House of Lords Act 1999 = \\n']\n",
      "7858 ['\\n = Mega Man 6 = \\n']\n",
      "7861 ['\\n = Sahifah of al @-@ Ridha = \\n']\n",
      "7863 ['\\n = Battle of Solachon = \\n']\n",
      "7865 ['\\n = Tales of a Third Grade Nothing = \\n']\n",
      "7867 ['\\n = Space Pilot 3000 = \\n']\n",
      "7869 [\"\\n = Boys Don 't Cry ( film ) = \\n\"]\n",
      "7881 ['\\n = These Days : Live in Concert = \\n', '\\n = Huangshan = \\n']\n",
      "7883 ['\\n = M21 Mortar Motor Carriage = \\n']\n",
      "7884 ['\\n = Choor Singh = \\n']\n",
      "7888 ['\\n = Rumination syndrome = \\n']\n",
      "7891 ['\\n = Lactarius argillaceifolius = \\n']\n",
      "7892 ['\\n = Jeffrey Pollack = \\n']\n",
      "7895 ['\\n = Marvel One @-@ Shots = \\n']\n",
      "7899 ['\\n = 2016 Russian Grand Prix = \\n']\n",
      "7903 ['\\n = Nirvana ( band ) = \\n']\n",
      "7910 ['\\n = Oskar Gröning = \\n']\n",
      "7914 ['\\n = Adentro = \\n']\n",
      "7916 ['\\n = Battle of Agua Dulce = \\n']\n",
      "7919 ['\\n = Königsberg @-@ class cruiser ( 1915 ) = \\n']\n",
      "7922 ['\\n = Thomas Pennant = \\n']\n",
      "7928 ['\\n = <unk> blacktip shark = \\n']\n",
      "7930 ['\\n = Ariwara no Narihira = \\n']\n",
      "7934 ['\\n = Flywheel, Shyster, and Flywheel = \\n']\n",
      "7938 ['\\n = Loring Air Force Base = \\n']\n",
      "7949 ['\\n = Talbot Tagora = \\n']\n",
      "7952 ['\\n = Trish Stratus = \\n']\n",
      "7958 ['\\n = Ray Farquharson = \\n']\n",
      "7960 ['\\n = Early mainframe games = \\n']\n",
      "7963 ['\\n = Thalaba the Destroyer = \\n']\n",
      "7967 ['\\n = Teenage Dream : The Complete Confection = \\n']\n",
      "7969 ['\\n = Dr Pepper Ballpark = \\n']\n",
      "7971 ['\\n = The Tempest = \\n']\n",
      "7985 ['\\n = Uniforms of the Confederate States military forces = \\n']\n",
      "7991 ['\\n = Nauru reed warbler = \\n']\n",
      "7993 ['\\n = Christine Chapel = \\n']\n",
      "7996 ['\\n = Bound 2 = \\n']\n",
      "7998 ['\\n = Unforgiven ( 2005 ) = \\n']\n",
      "8002 ['\\n = Fight for You = \\n']\n",
      "8004 ['\\n = Anarchy Online = \\n']\n",
      "8010 ['\\n = George Kenney = \\n']\n",
      "8015 ['\\n = Pyotr Ilyich Tchaikovsky and The Five = \\n']\n",
      "8024 ['\\n = Ben Daniels = \\n']\n",
      "8025 [\"\\n = Newton's parakeet = \\n\"]\n",
      "8029 ['\\n = Hush ( Buffy the Vampire Slayer ) = \\n']\n",
      "8035 ['\\n = SM UB @-@ 14 = \\n']\n",
      "8039 ['\\n = Ryan Leaf = \\n']\n",
      "8044 ['\\n = 1903 Jamaica hurricane = \\n']\n",
      "8047 ['\\n = Banksia grossa = \\n']\n",
      "8050 ['\\n = LG G2 = \\n']\n",
      "8054 ['\\n = Tales of Symphonia = \\n']\n",
      "8058 ['\\n = CAC / PAC JF @-@ 17 Thunder = \\n']\n",
      "8068 ['\\n = Carbon = \\n']\n",
      "8076 ['\\n = Hurricane Ekeka = \\n']\n",
      "8077 ['\\n = Knicks – Nuggets brawl = \\n']\n",
      "8079 ['\\n = Tropical Storm Linda ( 1997 ) = \\n']\n",
      "8081 ['\\n = Cape Moreton Light = \\n']\n",
      "8083 ['\\n = History of American football = \\n']\n",
      "8099 ['\\n = Thomas de Buittle = \\n']\n",
      "8101 ['\\n = Anna Maria Rückerschöld = \\n']\n",
      "8103 ['\\n = Imagination ( magazine ) = \\n']\n",
      "8106 ['\\n = Adventure ( Atari 2600 ) = \\n']\n",
      "8110 ['\\n = Joshua Prawer = \\n']\n",
      "8112 ['\\n = Malpuech facial clefting syndrome = \\n']\n",
      "8115 ['\\n = Battle of Ceresole = \\n']\n",
      "8120 ['\\n = Surface weather observation = \\n']\n",
      "8122 ['\\n = Steve Irwin = \\n']\n",
      "8127 ['\\n = See You Again ( Miley Cyrus song ) = \\n']\n",
      "8130 ['\\n = 2 / 31st Battalion ( Australia ) = \\n']\n",
      "8133 ['\\n = Fritz the Cat ( film ) = \\n']\n",
      "8141 ['\\n = Nea Salamis Famagusta FC = \\n']\n",
      "8148 ['\\n = Arnold Bax = \\n']\n",
      "8156 ['\\n = Nathaniel Backus House = \\n']\n",
      "8157 ['\\n = Chris Traeger = \\n']\n",
      "8161 ['\\n = Eyeshield 21 = \\n']\n",
      "8165 ['\\n = Silvia ( song ) = \\n']\n",
      "8167 ['\\n = Upsilon Andromedae d = \\n']\n",
      "8168 ['\\n = Bomber Mafia = \\n']\n",
      "8169 ['\\n = M @-@ 71 ( Michigan highway ) = \\n']\n",
      "8170 ['\\n = Typhoon Forrest ( 1983 ) = \\n']\n",
      "8172 ['\\n = Ico = \\n']\n",
      "8177 ['\\n = The Livestock Conservancy = \\n']\n",
      "8181 ['\\n = Nevada State Route 28 = \\n']\n",
      "8182 ['\\n = Tower of London = \\n']\n",
      "8193 ['\\n = Melville Island ( Nova Scotia ) = \\n']\n",
      "8199 ['\\n = Saskatchewan Highway 7 = \\n']\n",
      "8201 ['\\n = Edward Millen = \\n']\n",
      "8203 ['\\n = 1941 Florida hurricane = \\n']\n",
      "8206 ['\\n = Helvellyn = \\n']\n",
      "8213 ['\\n = Toni Preckwinkle = \\n']\n",
      "8217 ['\\n = Kill the Alligator and Run = \\n']\n",
      "8219 ['\\n = Noel Gallagher = \\n']\n",
      "8230 ['\\n = Michael Novogratz = \\n']\n",
      "8231 ['\\n = Louis Nolan = \\n']\n",
      "8236 [\"\\n = 2010 New Year's Eve tornado outbreak = \\n\"]\n",
      "8239 ['\\n = Marvel vs. Capcom 3 : Fate of Two Worlds = \\n']\n",
      "8242 ['\\n = 2012 Critérium du Dauphiné = \\n']\n",
      "8250 ['\\n = McKinsey & Company = \\n']\n",
      "8256 ['\\n = Georgetown University = \\n']\n",
      "8266 ['\\n = The Boat Race 1858 = \\n']\n",
      "8267 ['\\n = Old Church of St Gwenllwyfo, Llanwenllwyfo = \\n']\n",
      "8268 ['\\n = The Boat Races 2015 = \\n']\n",
      "8273 ['\\n = Joyas Prestadas = \\n']\n",
      "8275 ['\\n = Globular cluster = \\n']\n",
      "8280 ['\\n = 18 pc ) and Palomar 14 ( Rh = \\n']\n",
      "8281 ['\\n = − 7 @.@ 20 ± 0 @.@ 13, σ = \\n']\n",
      "8283 ['\\n = Hapalopilus nidulans = \\n']\n",
      "8285 ['\\n = Alexander Cameron Rutherford = \\n']\n",
      "8293 ['\\n = Mr. Hankey, the Christmas Poo = \\n']\n",
      "8299 ['\\n = Ghost of a Chance ( Homicide : Life on the Street ) = \\n']\n",
      "8302 ['\\n = El Greco = \\n']\n",
      "8310 ['\\n = Film noir = \\n']\n",
      "8325 ['\\n = John Gielgud = \\n']\n",
      "8337 ['\\n = Saga ( comic book ) = \\n']\n",
      "8345 ['\\n = Doug Ring with the Australian cricket team in England in 1948 = \\n']\n",
      "8350 ['\\n = <unk> catshark = \\n']\n",
      "8353 ['\\n = New York State Route 359 = \\n', '\\n = Edward Low = \\n']\n",
      "8357 ['\\n = Henry Rollins = \\n']\n",
      "8364 ['\\n = Mulder and Scully ( song ) = \\n']\n",
      "8366 ['\\n = Cynfarwy = \\n']\n",
      "8367 ['\\n = Carl Michael Bellman = \\n']\n",
      "8372 ['\\n = Lions ( album ) = \\n']\n",
      "8376 ['\\n = Above All State Park = \\n']\n",
      "8377 ['\\n = Paul Kariya = \\n']\n",
      "8390 ['\\n = Simpson and Delilah = \\n']\n",
      "8392 ['\\n = Opus Dei = \\n']\n",
      "8403 ['\\n = John Alan Coey = \\n']\n",
      "8406 ['\\n = California State Route 247 = \\n']\n",
      "8407 ['\\n = Tubas = \\n']\n",
      "8411 ['\\n = HMS Illustrious ( 1896 ) = \\n']\n",
      "8412 ['\\n = Guillaume de Dole = \\n']\n",
      "8415 ['\\n = Candyman ( Christina Aguilera song ) = \\n']\n",
      "8418 ['\\n = Boredoms = \\n']\n",
      "8421 ['\\n = Ivy Valentine = \\n']\n",
      "8425 ['\\n = Rudd Concession = \\n']\n",
      "8440 ['\\n = 1920 Buffalo All @-@ Americans season = \\n']\n",
      "8444 ['\\n = The Man with the Golden Gun ( novel ) = \\n']\n",
      "8447 ['\\n = Martha Wise = \\n']\n",
      "8449 ['\\n = Typhoon Wipha ( 2007 ) = \\n']\n",
      "8452 ['\\n = 2009 Philadelphia Phillies season = \\n']\n",
      "8462 ['\\n = Games played ; AB = \\n', '\\n = Runs scored ; H = \\n', '\\n = Doubles ; 3B = \\n', '\\n = Home runs ; RBI = \\n', '\\n = Batting average ; SB = \\n', '\\n = Wins ; L = \\n', '\\n = Earned run average ; G = \\n', '\\n = Games started ; SV = \\n', '\\n = Innings pitched ; R = \\n', '\\n = Earned runs allowed ; BB = \\n']\n",
      "8463 ['\\n = American Idiot ( musical ) = \\n']\n",
      "8468 ['\\n = Boy Meets Curl = \\n']\n",
      "8473 ['\\n = Zesh Rehman = \\n']\n",
      "8478 ['\\n = <unk> of Zimbabwe = \\n']\n",
      "8482 ['\\n = Washington State Route 11 = \\n']\n",
      "8484 ['\\n = Redback spider = \\n']\n",
      "8491 ['\\n = Richard Wright ( musician ) = \\n']\n",
      "8496 ['\\n = Hemiptera = \\n']\n",
      "8501 ['\\n = Tech Tower = \\n']\n",
      "8505 ['\\n = Dryandra Woodland = \\n']\n",
      "8508 ['\\n = Effects of Hurricane Dean in the Lesser Antilles = \\n']\n",
      "8511 ['\\n = Slow Life = \\n']\n",
      "8513 ['\\n = Music of Final Fantasy XIV = \\n']\n",
      "8516 ['\\n = University of Colorado Denver = \\n']\n",
      "8521 ['\\n = No. 78 Squadron RAAF = \\n']\n",
      "8524 ['\\n = Dookudu = \\n']\n",
      "8529 ['\\n = Iwo Jima ( video game ) = \\n']\n",
      "8530 ['\\n = 1998 – 99 Manchester United F.C. season = \\n', '\\n = Manchester United win ; Yellow = \\n']\n",
      "8533 ['\\n = Manchester United win ; Yellow = \\n']\n",
      "8534 ['\\n = Manchester United win ; Yellow = \\n', '\\n = Manchester United win ; Yellow = \\n', '\\n = Manchester United win ; Yellow = \\n']\n",
      "8535 ['\\n = Manchester United win ; Yellow = \\n']\n",
      "8537 ['\\n = Teddy Riner = \\n']\n",
      "8538 ['\\n = William Windsor ( goat ) = \\n']\n",
      "8540 ['\\n = Streptococcus iniae = \\n']\n",
      "8542 ['\\n = Drax power station = \\n']\n",
      "8548 ['\\n = Katrina Kaif = \\n']\n",
      "8554 ['\\n = State Route 1002 ( Lehigh County, Pennsylvania ) = \\n']\n",
      "8556 ['\\n = Meteorological history of Hurricane Georges = \\n']\n",
      "8559 ['\\n = Cyclone Jokwe = \\n']\n",
      "8561 ['\\n = Sprang = \\n']\n",
      "8562 ['\\n = I Dream of Jesus = \\n']\n",
      "8564 ['\\n = William de Braose, 3rd Lord of Bramber = \\n']\n",
      "8566 ['\\n = Ontario Highway 148 = \\n', '\\n = The Boat Race 1887 = \\n']\n",
      "8567 ['\\n = Battle of Labuan = \\n']\n",
      "8575 ['\\n = From Russia, with Love ( novel ) = \\n']\n",
      "8580 [\"\\n = Who's That Chick? = \\n\"]\n",
      "8583 ['\\n = Stoicism = \\n']\n",
      "8588 ['\\n = Kikuuiki = \\n']\n",
      "8592 ['\\n = EastEnders Live = \\n']\n",
      "8597 ['\\n = Ibrox Stadium = \\n']\n",
      "8602 ['\\n = Eliurus petteri = \\n']\n",
      "8604 ['\\n = 1985 – 86 Calgary Flames season = \\n']\n",
      "8605 ['\\n = Games played, W = \\n', '\\n = Losses, T = \\n', '\\n = Points, GF = \\n']\n",
      "8606 ['\\n = Games played ; G = \\n', '\\n = Assists ; Pts = \\n', '\\n = Games played ; TOI = \\n']\n",
      "8607 ['\\n = Wins ; L = \\n', '\\n = Overtime / shootout losses ; GA = \\n', '\\n = Shutouts ; GAA = \\n', \"\\n = Give Peace a Chance ( Grey's Anatomy ) = \\n\"]\n",
      "8610 ['\\n = Cross @-@ country skiing ( sport ) = \\n']\n",
      "8614 ['\\n = Homotopy groups of spheres = \\n']\n",
      "8615 ['\\n = 1 \\n This is the set of points in 3 @-@ dimensional Euclidean space found exactly one unit away from the origin. It is called the 2 @-@ sphere, S2, for reasons given below. The same idea applies for any dimension n ; the equation x02 + x12 + ⋯ + <unk> = \\n', '\\n = 1. If a balloon is punctured and spread flat it produces a disk ; this construction repairs the puncture, like pulling a drawstring. The slash, pronounced \" modulo \", means to take the topological space on the left ( the disk ) and in it join together as one all the points on the right ( the circle ). The region is 2 @-@ dimensional, which is why topology calls the resulting topological space a 2 @-@ sphere. Generalized, <unk> / Sn − 1 produces Sn. For example, D1 is a line segment, and the construction joins its ends to make a circle. An equivalent description is that the boundary of an n @-@ dimensional disk is glued to a point, producing a CW complex. \\n Suspension of equator : written in topology as <unk> \\n This construction, though simple, is of great theoretical importance. Take the circle S1 to be the equator, and sweep each point on it to one point above ( the North Pole ), producing the northern hemisphere, and to one point below ( the South Pole ), producing the southern hemisphere. For each positive integer n, the n @-@ sphere x02 + x12 + ⋯ + <unk> = \\n']\n",
      "8618 ['\\n = Hn ( Sn ) = \\n', '\\n = Z2 × Z2 ). Extended tables of homotopy groups of spheres are given at the end of the article. \\n The first two rows of this table are straightforward. The homotopy groups πi ( S0 ) of the 0 @-@ dimensional sphere are trivial for i > 0, because any base point preserving map from an i @-@ sphere to a 0 @-@ sphere is a one @-@ point mapping. Similarly, the homotopy groups πi ( S1 ) of the 1 @-@ sphere are trivial for i > 1, because the universal covering space, R, which has the same higher homotopy groups, is contractible. \\n Beyond these two rows, the higher homotopy groups ( i > n ) appear to be chaotic, but in fact there are many patterns, some obvious and some very subtle. \\n The groups below the jagged black line are constant along the diagonals ( as indicated by the red, green and blue coloring ). \\n Most of the groups are finite. The only unstable groups which are not are either on the main diagonal or immediately above the jagged line ( highlighted in yellow ). \\n The third and fourth rows of the table are the same starting in the third column ( i.e., πi ( S2 ) = \\n']\n",
      "8619 ['\\n = 1 form a 3 @-@ sphere, and their ratios z0 / z1 cover the complex plane plus infinity, a 2 @-@ sphere. The Hopf map S3 → S2 sends any such pair to its ratio. \\n Similarly, there are generalized Hopf fibrations \\n <formula> \\n <formula> \\n constructed using pairs of quaternions or <unk> instead of complex numbers ( Hatcher 2002 ). Here, too, π3 ( S7 ) and <unk> ( S15 ) are zero. Thus the long exact sequences again break into families of split short exact sequences, implying two families of relations. \\n <formula> \\n <formula> \\n The three fibrations have base space Sn with n = \\n', '\\n = 1, 2, 3. A fibration does exist for S1 ( m = \\n', '\\n = 2p − 3. The case of 2 @-@ dimensional spheres is slightly different : the first p @-@ torsion occurs for k = \\n']\n",
      "8620 ['\\n = 1 ⁄ 504. \\n The stable homotopy groups of spheres are the direct sum of the image of the J @-@ homomorphism, and the kernel of the Adams e @-@ invariant, a homomorphism from these groups to Q / Z. Roughly speaking, the image of the J @-@ homomorphism is the subgroup of \" well understood \" or \" easy \" elements of the stable homotopy groups. These well understood elements account for most elements of the stable homotopy groups of spheres in small dimensions. The quotient of πnS by the image of the J @-@ homomorphism is considered to be the \" hard \" part of the stable homotopy groups of spheres ( Adams 1966 ). ( Adams also introduced certain order 2 elements <unk> of πnS for n = \\n', '\\n = 0 and g ⋅ h = \\n']\n",
      "8622 ['\\n = <unk> + 10 ( S10 ) = \\n']\n",
      "8623 ['\\n = Missing My Baby = \\n']\n",
      "8624 ['\\n = The Problem Solvers = \\n']\n",
      "8627 ['\\n = The Land of Lost Content = \\n']\n",
      "8628 ['\\n = Lazer Beam = \\n']\n",
      "8631 ['\\n = Cheers ( Drink to That ) = \\n']\n",
      "8634 ['\\n = Dylan and Cole Sprouse = \\n']\n",
      "8636 ['\\n = Operation Ring = \\n']\n",
      "8639 ['\\n = McLaren M2B = \\n']\n",
      "8642 ['\\n = Controversies surrounding Grand Theft Auto V = \\n']\n",
      "8644 ['\\n = The Splendid Source = \\n']\n",
      "8646 ['\\n = Esham = \\n']\n",
      "8649 ['\\n = Ohio State Route 370 = \\n', '\\n = E.T. the Extra @-@ Terrestrial = \\n']\n",
      "8657 ['\\n = Electric ( Robyn song ) = \\n']\n",
      "8658 [\"\\n = Gerard K. O 'Neill = \\n\"]\n",
      "8664 ['\\n = Working on a Dream Tour = \\n']\n",
      "8675 ['\\n = World Series of Poker Casino Employee Championship = \\n']\n",
      "8676 ['\\n = Campaign history of the Roman military = \\n']\n",
      "8694 ['\\n = Battle of Alton = \\n']\n",
      "8696 ['\\n = Trams in Rouen = \\n']\n",
      "8702 ['\\n = Listen to the Rain on the Roof = \\n']\n",
      "8705 ['\\n = Kakashi Hatake = \\n']\n",
      "8709 ['\\n = Licence to Kill = \\n']\n",
      "8715 ['\\n = Mediterranean cuisine = \\n']\n",
      "8720 ['\\n = Thebe ( moon ) = \\n']\n",
      "8721 ['\\n = James Meredith ( footballer ) = \\n']\n",
      "8723 ['\\n = New York State Route 36 = \\n']\n",
      "8726 ['\\n = M @-@ 156 ( Michigan highway ) = \\n']\n",
      "8727 ['\\n = Hartebeest = \\n']\n",
      "8734 ['\\n = Oh Father = \\n']\n",
      "8740 [\"\\n = Adolf Hitler's 50th birthday = \\n\"]\n",
      "8742 ['\\n = Sheffield Rules = \\n']\n",
      "8747 ['\\n = Thin Man ( nuclear bomb ) = \\n']\n",
      "8750 ['\\n = Code of Vengeance = \\n']\n",
      "8753 ['\\n = Betty Shabazz = \\n']\n",
      "8757 ['\\n = Paul E. Patton = \\n']\n",
      "8766 ['\\n = Ice Hockey Hair = \\n']\n",
      "8769 ['\\n = 1st Parachute Brigade ( United Kingdom ) = \\n']\n",
      "8775 [\"\\n = Train Kept A @-@ Rollin'= \\n\"]\n",
      "8780 ['\\n = Yellow Submarine ( album ) = \\n']\n",
      "8783 ['\\n = Srinivasa Ramanujan = \\n']\n",
      "8785 ['\\n = 2, n = \\n']\n",
      "8788 ['\\n = − 4 × 58 = \\n', '\\n = 2 ( note that 5 × 7 × 13 × 58 = \\n', '\\n = 99 × 99 ; 396 = \\n']\n",
      "8789 ['\\n = 13 + 123 = \\n']\n",
      "8791 [\"\\n = 1919 Polish coup d 'état attempt in Lithuania = \\n\"]\n",
      "8794 ['\\n = Todd Packer ( The Office ) = \\n']\n",
      "8796 ['\\n = Work It Out ( Beyoncé song ) = \\n']\n",
      "8800 ['\\n = Earth Song = \\n']\n",
      "8802 ['\\n = Scandinavian Scotland = \\n']\n",
      "8811 ['\\n = Dragon Quest ( video game ) = \\n']\n",
      "8820 ['\\n = The Holocaust in Lithuania = \\n']\n",
      "8825 [\"\\n = The Pasha's Daughter = \\n\"]\n",
      "8826 ['\\n = Arrested Development ( TV series ) = \\n']\n",
      "8833 ['\\n = San Sebastian Church ( Manila ) = \\n']\n",
      "8834 ['\\n = Waterfalls in Ricketts Glen State Park = \\n']\n",
      "8839 ['\\n = 2003 Football League Cup Final = \\n']\n",
      "8843 ['\\n = The Number Devil = \\n']\n",
      "8844 ['\\n = Vojislav Lukačević = \\n']\n",
      "8848 ['\\n = Carlos Menem = \\n']\n",
      "8855 ['\\n = Pacer ( album ) = \\n']\n",
      "8856 ['\\n = Gravity ( Lecrae album ) = \\n']\n",
      "8861 ['\\n = Tropical Storm Edouard ( 2002 ) = \\n']\n",
      "8863 ['\\n = Major urinary proteins = \\n']\n",
      "8866 ['\\n = Li Jiawei = \\n']\n",
      "8869 ['\\n = P.T. ( video game ) = \\n']\n",
      "8873 ['\\n = Katyusha rocket launcher = \\n']\n",
      "8877 ['\\n = Typhoon Nora ( 1973 ) = \\n']\n",
      "8879 [\"\\n = Falcon's Fury = \\n\"]\n",
      "8882 ['\\n = Battle of Logorište = \\n']\n",
      "8894 ['\\n = Gordon Bell ( American football ) = \\n']\n",
      "8899 ['\\n = Ontario Highway 406 = \\n']\n",
      "8900 ['\\n = Hurricane Elena = \\n']\n",
      "8911 ['\\n = Together Again ( Janet Jackson song ) = \\n']\n",
      "8914 ['\\n = Upsilon Andromedae c = \\n']\n",
      "8915 ['\\n = Coquitlam = \\n']\n",
      "8923 ['\\n = Walter Alston = \\n']\n",
      "8927 ['\\n = Tafl games = \\n']\n",
      "8931 ['\\n = Ved Vejen = \\n']\n",
      "8932 ['\\n = Final Fantasy XIII = \\n']\n",
      "8941 ['\\n = When Harry Met Sally... = \\n']\n",
      "8945 ['\\n = SMS Dresden ( 1917 ) = \\n']\n",
      "8947 ['\\n = New Jersey Route 284 = \\n']\n",
      "8948 ['\\n = Piotr Skarga = \\n']\n",
      "8950 ['\\n = HMAS Sydney ( D48 ) = \\n']\n",
      "8961 ['\\n = Joe McGinnity = \\n']\n",
      "8966 ['\\n = Decompression practice = \\n']\n",
      "8971 ['\\n = Actual depth at altitude × Pressure at sea level ÷ Pressure at altitude \\n Decompression stop depths are also corrected, using the ratio of surface pressures, and will produce actual stop depths which are shallower than the sea level stop depths. \\n Stop depth at altitude = \\n']\n",
      "8977 ['\\n = Banksia integrifolia = \\n']\n",
      "8981 ['\\n = Dhoom 2 = \\n']\n",
      "8986 ['\\n = Battle of Ap Bac = \\n']\n",
      "8993 ['\\n = Battle of Green Spring = \\n']\n",
      "8995 ['\\n = The Distrest Poet = \\n']\n",
      "8997 ['\\n = Real Irish Republican Army = \\n']\n",
      "9004 ['\\n = Tropic Thunder = \\n']\n",
      "9013 ['\\n = Italian ironclad Conte Verde = \\n']\n",
      "9014 ['\\n = Brioni Agreement = \\n']\n",
      "9016 ['\\n = <unk> shark = \\n']\n",
      "9019 ['\\n = Talking to the Moon ( song ) = \\n']\n",
      "9021 ['\\n = Rova of Antananarivo = \\n']\n",
      "9032 ['\\n = Siege of Port Royal ( 1710 ) = \\n']\n",
      "9035 ['\\n = Delaware Route 273 = \\n']\n",
      "9036 ['\\n = Jimmy Chamberlin = \\n']\n",
      "9040 ['\\n = Derivative = \\n', '\\n = f ( x ) of a variable x is a measure of the rate at which the value y of the function changes with respect to the change of the variable x. It is called the derivative of f with respect to x. If x and y are real numbers, and if the graph of f is plotted against x, the derivative is the slope of this graph at each point. \\n The simplest case, apart from the trivial case of a constant function, is when y is a linear function of x, meaning that the graph of y divided by x is a line. In this case, y = \\n']\n",
      "9041 ['\\n = 0. If the limit <unk> → <unk> ( h ) exists, meaning that there is a way of choosing a value for Q ( 0 ) that makes Q a continuous function, then the function f is differentiable at a, and its derivative at a equals Q ( 0 ). \\n In practice, the existence of a continuous extension of the difference quotient Q ( h ) to h = \\n', '\\n = f ( x ) at a real point x can be defined as the shadow of the quotient ∆ y / ∆ x for infinitesimal ∆ x, where ∆ y = \\n', '\\n = x2 is differentiable at x = \\n', '\\n = 0, because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when h = \\n']\n",
      "9042 ['\\n = 3 is f ′ ( 3 ) = \\n', '\\n = a is f ′ ( a ) = \\n', '\\n = f ( x ) is differentiable at a, then f must also be continuous at a. As an example, choose a point a and let f be the step function that returns a value, say 1, for all x less than a, and returns a different value, say 10, for all x greater than or equal to a. f cannot have a derivative at a. If h is negative, then a + h is on the low part of the step, so the secant line from a to a + h is very steep, and as h tends to zero the slope tends to infinity. If h is positive, then a + h is on the high part of the step, so the secant line from a to a + h has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist. \\n However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function y = \\n', '\\n = 0, but it is not differentiable there. If h is positive, then the slope of the secant line from 0 to h is one, whereas if h is negative, then the slope of the secant line from 0 to h is negative one. This can be seen graphically as a \" kink \" or a \" cusp \" in the graph at x = \\n', '\\n = x1 / 3 is not differentiable at x = \\n', '\\n = f ′ ( a ). \\n For comparison, consider the doubling function f ( x ) = \\n']\n",
      "9043 ['\\n = 2, f ( 2 ) = \\n', '\\n = 0 of the function y = \\n', '\\n = 0 of the function y = \\n', '\\n = f ( x ) is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by \\n <formula> \\n and was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation \\n <formula> \\n for the nth derivative of y = \\n']\n",
      "9044 ['\\n = 0. When r = \\n']\n",
      "9045 ['\\n = x. These are measured using directional derivatives. Choose a vector \\n <formula> \\n The directional derivative of f in the direction of v at the point x is the limit \\n <formula> \\n In some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that v = \\n']\n",
      "9048 ['\\n = Cheryl Tunt = \\n']\n",
      "9050 ['\\n = Oil shale industry = \\n']\n",
      "9053 ['\\n = 1906 Florida Keys hurricane = \\n']\n",
      "9055 ['\\n = Ellen van Dijk = \\n']\n",
      "9060 ['\\n = over time limit, DNF = \\n', '\\n = Hugo Award = \\n']\n",
      "9064 ['\\n = SpongeBob SquarePants ( season 4 ) = \\n']\n",
      "9066 ['\\n = HMS Dainty ( <unk> ) = \\n']\n",
      "9068 ['\\n = Panggilan Darah = \\n']\n",
      "9069 ['\\n = Sega v. Accolade = \\n']\n",
      "9073 ['\\n = I Shall Not Be Moved ( poetry ) = \\n']\n",
      "9074 ['\\n = Newfoundland expedition ( 1702 ) = \\n']\n",
      "9075 ['\\n = Manchester ( The West Wing ) = \\n']\n",
      "9077 ['\\n = Telopea oreades = \\n']\n",
      "9080 ['\\n = SMS Kaiser Max ( 1862 ) = \\n']\n",
      "9082 ['\\n = The Negro Motorist Green Book = \\n']\n",
      "9089 [\"\\n = A Hard Day's Night ( film ) = \\n\"]\n",
      "9094 ['\\n = Leccinum holopus = \\n']\n",
      "9096 ['\\n = Cyclone Alibera = \\n']\n",
      "9098 ['\\n = Marge vs. the Monorail = \\n']\n",
      "9100 ['\\n = Esther Drummond = \\n']\n",
      "9103 ['\\n = Remember Paul? = \\n']\n",
      "9105 ['\\n = Nur jedem das Seine, BWV 163 = \\n']\n",
      "9107 ['\\n = Dysprosium = \\n']\n",
      "9112 ['\\n = 1984 Intercontinental Cup = \\n']\n",
      "9114 ['\\n = If U Seek Amy = \\n']\n",
      "9118 ['\\n = History of Milton Keynes = \\n']\n",
      "9123 ['\\n = 52 @,@ 931 \\n 1967 Designation \\n 1969 The Open University moves to its new headquarters in Walton Hall. \\n 1970s \\n 1970 First section of new city grid road system ( H2 from V4 Watling Street ). The Stables presents its first concert. \\n 1971 First major housing scheme ( Galley Hill, Stony Stratford ). \\n 1971 Census : Population of future Borough of Milton Keynes = \\n', '\\n = 46 @,@ 500 \\n 1974 Stantonbury Campus opens ; first major grid road ( V8 Marlborough Street connects Bletchley to Stantonbury ( for New Bradwell and Wolverton ) ; work begins on the Shopping Building ( now The Centre : MK ) ; the first balancing lake ( Willen Lake ) is finished. Borough of Milton Keynes created ( as a district of Buckinghamshire until 1997 ) \\n 1975 The first office building in Central Milton Keynes ( Lloyds Court ) opens. \\n 1979 First concert at Milton Keynes Bowl ( Desmond Dekker / Geno Washington ) ; Margaret Thatcher opens the Shopping Building. \\n 1980s \\n 1980 <unk> opens, bypassing Watling Street ; Peace Pagoda dedicated at Willen. \\n 1981 Homeworld, Central Library, Willen Hospice \\n 1981 Census : Population of the Borough of Milton Keynes = \\n', '\\n = 179 @,@ 252 ; \\n population of designated area = \\n']\n",
      "9124 ['\\n = 207 @,@ 063 ; \\n population of designated area = \\n', '\\n = Kenya Airways = \\n']\n",
      "9128 ['\\n = Black Rock Harbor Light = \\n']\n",
      "9129 ['\\n = I Vampiri = \\n']\n",
      "9132 ['\\n = Pitot @-@ static system = \\n']\n",
      "9135 ['\\n = The Boat Race 1878 = \\n', '\\n = Star Trek ( film ) = \\n']\n",
      "9149 ['\\n = Elephant & Castle tube station = \\n']\n",
      "9151 ['\\n = Breaking Point ( Keri Hilson song ) = \\n']\n",
      "9154 ['\\n = Fortezza of Rethymno = \\n']\n",
      "9155 ['\\n = Quién Dijo Ayer = \\n']\n",
      "9157 ['\\n = Romances ( Luis Miguel album ) = \\n']\n",
      "9160 ['\\n = Deddington Castle = \\n']\n",
      "9162 ['\\n = 2 / 2nd Machine Gun Battalion ( Australia ) = \\n']\n",
      "9166 ['\\n = German submarine U @-@ 853 = \\n']\n",
      "9169 ['\\n = Abu Ali Iyad = \\n']\n",
      "9172 ['\\n = Pilot ( Community ) = \\n']\n",
      "9174 ['\\n = Hurst Castle = \\n']\n",
      "9179 ['\\n = Kepler @-@ 11c = \\n']\n",
      "9180 ['\\n = Neutopia = \\n']\n",
      "9183 ['\\n = Partenope @-@ class cruiser = \\n']\n",
      "9184 ['\\n = House of Plantagenet = \\n']\n",
      "9198 ['\\n = Russian battleship Georgii Pobedonosets = \\n']\n",
      "9201 ['\\n = Bolton = \\n']\n",
      "9208 ['\\n = Black bean aphid = \\n']\n",
      "9210 ['\\n = Chika Ideal = \\n']\n",
      "9211 ['\\n = Tasmanian devil = \\n']\n",
      "9226 ['\\n = Clam dip = \\n']\n",
      "9227 ['\\n = Christmas Tree ( Lady Gaga song ) = \\n']\n",
      "9229 ['\\n = Ayane ( Dead or Alive ) = \\n']\n",
      "9237 ['\\n = Swim Good = \\n']\n",
      "9239 ['\\n = Music of the Final Fantasy Crystal Chronicles series = \\n']\n",
      "9242 ['\\n = Battle of the Miljevci Plateau = \\n']\n",
      "9244 ['\\n = California State Route 177 = \\n']\n",
      "9245 ['\\n = Monetary Policy Committee = \\n']\n",
      "9248 ['\\n = Prick Up Your Ears ( Family Guy ) = \\n']\n",
      "9250 ['\\n = Quietly Confident Quartet = \\n']\n",
      "9253 ['\\n = Long @-@ term potentiation = \\n']\n",
      "9259 ['\\n = The War for Late Night = \\n']\n",
      "9261 ['\\n = Jack Russell Terrier = \\n']\n",
      "9266 ['\\n = Fracture ( Fringe ) = \\n']\n",
      "9268 ['\\n = William George Malone = \\n']\n",
      "9272 ['\\n = Trans Polar = \\n']\n",
      "9274 ['\\n = Sinclair C5 = \\n']\n",
      "9285 ['\\n = Hart Lake ( Oregon ) = \\n']\n",
      "9287 ['\\n = Bill Johnston with the Australian cricket team in England in 1948 = \\n']\n",
      "9295 ['\\n = George M. Stratton = \\n']\n",
      "9302 ['\\n = Cambridge Water Co Ltd v Eastern Counties Leather plc = \\n']\n",
      "9305 ['\\n = United Abominations = \\n']\n",
      "9307 ['\\n = Huginn and Muninn = \\n']\n",
      "9310 ['\\n = Military service of Ian Smith = \\n']\n",
      "9315 ['\\n = Jiggs Parrott = \\n']\n",
      "9317 ['\\n = 1964 Atlantic hurricane season = \\n']\n",
      "9323 ['\\n = Flicker ( song ) = \\n']\n",
      "9324 ['\\n = Philosophy of science = \\n']\n",
      "9331 ['\\n = John Beilein = \\n']\n",
      "9338 ['\\n = Falkner Island Light = \\n']\n",
      "9339 ['\\n = New York State Route 64 = \\n']\n",
      "9341 [\"\\n = Rock N Roll McDonald's = \\n\"]\n",
      "9342 ['\\n = Adlertag = \\n']\n",
      "9349 ['\\n = Enugu = \\n']\n",
      "9356 ['\\n = The Tales of Beedle the Bard = \\n']\n",
      "9360 ['\\n = SM U @-@ 67 = \\n']\n",
      "9361 ['\\n = Monica Reyes = \\n']\n",
      "9363 ['\\n = Black Arrow = \\n']\n",
      "9365 ['\\n = Ngo Dinh Diem presidential visit to Australia = \\n']\n",
      "9369 ['\\n = 2012 – 13 Vancouver Canucks season = \\n']\n",
      "9375 ['\\n = Born to Make You Happy = \\n']\n",
      "9377 ['\\n = Jan Dekert = \\n']\n",
      "9379 ['\\n = Operation Copperhead = \\n']\n",
      "9380 ['\\n = Madeline Montalban = \\n']\n",
      "9384 ['\\n = Cranham = \\n']\n",
      "9386 ['\\n = Thomas R. Cornelius = \\n']\n",
      "9387 ['\\n = Thirsty Merc = \\n']\n",
      "9390 ['\\n = Laurentia Tan = \\n']\n",
      "9392 ['\\n = The First Time ( Glee ) = \\n']\n",
      "9397 ['\\n = Alpine chough = \\n']\n",
      "9400 ['\\n = British contribution to the Manhattan Project = \\n']\n",
      "9409 ['\\n = Delta County Courthouse = \\n', '\\n = Yoga Yajnavalkya = \\n']\n",
      "9417 ['\\n = North Carolina @-@ class battleship = \\n']\n",
      "9430 ['\\n = Shadow of the Colossus = \\n']\n",
      "9436 ['\\n = Optional Protocol to the Convention on the Elimination of All Forms of Discrimination against Women = \\n']\n",
      "9438 ['\\n = North Carolina Highway 68 = \\n']\n",
      "9439 ['\\n = Potential cultural impact of extraterrestrial contact = \\n']\n",
      "9448 ['\\n = Australian blacktip shark = \\n']\n",
      "9450 ['\\n = Freedom Monument = \\n']\n",
      "9454 ['\\n = Junior Hemingway = \\n']\n",
      "9456 ['\\n = Special Boat Service = \\n']\n",
      "9460 ['\\n = The Obvious Child = \\n']\n",
      "9461 ['\\n = Renown @-@ class battlecruiser = \\n']\n",
      "9470 ['\\n = I Know Why the Caged Bird Sings = \\n']\n",
      "9480 ['\\n = One Meridian Plaza = \\n']\n",
      "9484 ['\\n = 1948 Bermuda – Newfoundland hurricane = \\n']\n",
      "9486 ['\\n = Russian cruiser Gromoboi = \\n']\n",
      "9490 ['\\n = Star Wars : Episode III – Revenge of the Sith = \\n']\n",
      "9500 ['\\n = Halo : Ghosts of Onyx = \\n']\n",
      "9502 ['\\n = William Harper ( Rhodesian politician ) = \\n']\n",
      "9506 ['\\n = Stephen Newton = \\n']\n",
      "9508 ['\\n = Hillsboro Civic Center = \\n']\n",
      "9510 ['\\n = Tropical Depression One ( 1993 ) = \\n']\n",
      "9511 ['\\n = Abyssal plain = \\n']\n",
      "9517 ['\\n = Did I Stutter? = \\n']\n",
      "9520 ['\\n = Gwen Stefani = \\n']\n",
      "9526 ['\\n = We Can Do It! = \\n']\n",
      "9529 ['\\n = So Under Pressure = \\n']\n",
      "9530 [\"\\n = Let's Stay Together ( 30 Rock ) = \\n\"]\n",
      "9533 ['\\n = West Virginia Mountaineers football = \\n']\n",
      "9547 ['\\n = You Really Got Me = \\n']\n",
      "9551 ['\\n = Hurricane Linda ( 1997 ) = \\n']\n",
      "9553 ['\\n = G. Ledyard Stebbins = \\n']\n",
      "9558 ['\\n = The Man with Two Brians = \\n']\n",
      "9559 ['\\n = Community Trolls = \\n']\n",
      "9561 ['\\n = McKinley Birthplace Memorial dollar = \\n']\n",
      "9564 ['\\n = Hammersmith & City line = \\n']\n",
      "9567 ['\\n = Gustav Holst = \\n']\n",
      "9579 ['\\n = Hervey le Breton = \\n']\n",
      "9580 ['\\n = Too Much Too Soon ( album ) = \\n']\n",
      "9584 ['\\n = Go Man Go = \\n']\n",
      "9587 ['\\n = French ironclad Invincible = \\n']\n",
      "9588 ['\\n = No. 114 Mobile Control and Reporting Unit RAAF = \\n']\n",
      "9589 ['\\n = The Proposal ( film ) = \\n']\n",
      "9592 ['\\n = 1960 South Vietnamese coup attempt = \\n']\n",
      "9603 ['\\n = Poole = \\n']\n",
      "9613 ['\\n = Commission to <unk> the Ballistic Missile Threat to the United States = \\n']\n",
      "9615 ['\\n = Covenant ( Millennium ) = \\n']\n",
      "9617 ['\\n = Inon Zur = \\n']\n",
      "9618 ['\\n = Dead Letters ( Millennium ) = \\n']\n",
      "9620 [\"\\n = Marburg's Bloody Sunday = \\n\"]\n",
      "9622 ['\\n = Ian Craig = \\n']\n",
      "9628 ['\\n = Phellodon = \\n']\n",
      "9631 ['\\n = HMS Jamaica ( 44 ) = \\n']\n",
      "9634 ['\\n = Thatgamecompany = \\n']\n",
      "9636 ['\\n = Roger le Poer = \\n']\n",
      "9638 ['\\n = Arizona State Route 89A = \\n']\n",
      "9639 ['\\n = Play @-@ Doh = \\n']\n",
      "9641 ['\\n = Metroid Prime 2 : Echoes = \\n']\n",
      "9645 ['\\n = Delrina = \\n']\n",
      "9652 ['\\n = LW12 = \\n']\n",
      "9655 ['\\n = First Roumanian @-@ American Congregation = \\n']\n",
      "9661 ['\\n = Osteopathic medicine in the United States = \\n']\n",
      "9667 ['\\n = Foramen spinosum = \\n']\n",
      "9668 ['\\n = Blue wildebeest = \\n']\n",
      "9673 ['\\n = Thriller ( Michael Jackson album ) = \\n']\n",
      "9680 ['\\n = Work Bus = \\n']\n",
      "9682 ['\\n = 32 Old Slip = \\n', '\\n = Tropical Storm Wukong ( 2006 ) = \\n']\n",
      "9683 ['\\n = First Test, 1948 Ashes series = \\n']\n",
      "9694 ['\\n = Periodic table = \\n']\n",
      "9704 [\"\\n = He Loves to Fly and He D 'ohs = \\n\"]\n",
      "9706 ['\\n = Imperial War Museum = \\n']\n",
      "9714 ['\\n = Michael Dokeianos = \\n']\n",
      "9715 ['\\n = Sea of Japan naming dispute = \\n']\n",
      "9720 ['\\n = A Short History of Progress = \\n']\n",
      "9723 ['\\n = The Fight ( Parks and Recreation ) = \\n']\n",
      "9725 ['\\n = Japanese battleship Aki = \\n']\n",
      "9727 ['\\n = Justin ( consul 540 ) = \\n']\n",
      "9729 ['\\n = 1933 Outer Banks hurricane = \\n']\n",
      "9730 ['\\n = Metagenomics = \\n']\n",
      "9736 ['\\n = New York State Route 420 = \\n']\n",
      "9737 ['\\n = Danie Mellor = \\n']\n",
      "9740 ['\\n = Louvre = \\n']\n",
      "9749 ['\\n = Two Birds ( Awake ) = \\n']\n",
      "9751 ['\\n = Typhoon Nabi = \\n']\n",
      "9755 ['\\n = The Plays of William Shakespeare = \\n']\n",
      "9758 ['\\n = Loughor Castle = \\n']\n",
      "9760 ['\\n = SpongeBob SquarePants ( season 6 ) = \\n']\n",
      "9764 ['\\n = Stephen Harper = \\n']\n",
      "9777 ['\\n = New York @-@ class battleship = \\n']\n",
      "9781 ['\\n = J.D.B. v. North Carolina = \\n']\n",
      "9782 ['\\n = Helmut Lent = \\n']\n",
      "9788 ['\\n = Let Me In ( film ) = \\n']\n",
      "9794 ['\\n = When Flanders Failed = \\n']\n",
      "9796 ['\\n = Characters of Final Fantasy VIII = \\n']\n",
      "9807 ['\\n = Bud Dunn = \\n']\n",
      "9808 ['\\n = SS El Oriente = \\n']\n",
      "9810 ['\\n = USS Bennington ( PG @-@ 4 ) = \\n']\n",
      "9819 ['\\n = New York State Route 75 = \\n']\n",
      "9821 ['\\n = Hayden Epstein = \\n']\n",
      "9823 ['\\n = Cape Verde at the 2008 Summer Olympics = \\n', '\\n = Autonomous Republic of Northern Epirus = \\n']\n",
      "9828 ['\\n = Halifax Explosion = \\n']\n",
      "9836 [\"\\n = Odex's actions against file @-@ sharing = \\n\"]\n",
      "9840 ['\\n = 2007 – 08 Sunderland A.F.C. season = \\n']\n",
      "9844 ['\\n = Welcome Party = \\n']\n",
      "9845 ['\\n = Providence, Rhode Island = \\n']\n",
      "9854 ['\\n = Chris Sheridan ( writer ) = \\n']\n",
      "9855 ['\\n = Gay Witch Hunt = \\n']\n",
      "9856 ['\\n = Bouldering = \\n']\n",
      "9859 ['\\n = The Red Throne = \\n']\n",
      "9861 ['\\n = Running to Stand Still = \\n']\n",
      "9865 ['\\n = Diddy Kong Racing = \\n']\n",
      "9868 ['\\n = HMS Forester ( <unk> ) = \\n']\n",
      "9871 ['\\n = Wilfred Arthur = \\n']\n",
      "9874 ['\\n = Ronald Reagan Presidential Library = \\n']\n",
      "9877 ['\\n = Portland Castle = \\n']\n",
      "9879 ['\\n = Mighty Jill Off = \\n']\n",
      "9882 ['\\n = Graham Westley = \\n']\n",
      "9888 ['\\n = SR V Schools class = \\n']\n",
      "9891 ['\\n = Come to the Well = \\n']\n",
      "9894 ['\\n = Saint Paul, Minnesota = \\n']\n",
      "9901 ['\\n = To the Stars ( album ) = \\n']\n",
      "9903 ['\\n = Mahavira = \\n']\n",
      "9906 ['\\n = James Rainwater = \\n']\n",
      "9908 ['\\n = Romney Academy = \\n']\n",
      "9912 ['\\n = Ontario Highway 51 = \\n', '\\n = Gilbert and Sullivan = \\n']\n",
      "9926 ['\\n = Cumberland ( rugby league team ) = \\n']\n",
      "9928 ['\\n = Pattie Boyd = \\n']\n",
      "9930 ['\\n = 49th Battalion ( Australia ) = \\n']\n",
      "9934 ['\\n = Rated @-@ RKO = \\n']\n",
      "9936 [\"\\n = No Man's Land ( Kumi Koda song ) = \\n\"]\n",
      "9939 ['\\n = Gender Bender ( The X @-@ Files ) = \\n']\n",
      "9941 ['\\n = Of Human Action = \\n']\n",
      "9943 ['\\n = Minimum Foundation Program = \\n']\n",
      "9944 ['\\n = Jon Hess ( lacrosse ) = \\n']\n",
      "9945 ['\\n = Anshei Israel Synagogue = \\n']\n",
      "9946 ['\\n = Lincoln Theatre ( Washington, D.C. ) = \\n']\n",
      "9948 ['\\n = Sind sparrow = \\n']\n",
      "9950 ['\\n = History of paleontology = \\n']\n",
      "9958 ['\\n = Doomsday ( 2008 film ) = \\n']\n",
      "9962 ['\\n = Nouvelles Extraordinaires de Divers <unk> = \\n']\n",
      "9964 ['\\n = Furry convention = \\n']\n",
      "9966 ['\\n = History of Dallas ( 1874 – 1929 ) = \\n']\n",
      "9968 ['\\n = House at 130 Mohegan Avenue = \\n']\n",
      "9969 ['\\n = Confidence Man ( Lost ) = \\n']\n",
      "9971 ['\\n = Out from Under = \\n']\n",
      "9973 ['\\n = DuPont Manual High School = \\n']\n",
      "9978 ['\\n = Excellence in Broadcasting = \\n']\n",
      "9981 ['\\n = G.I. Joe : A Real American Hero ( Marvel Comics ) = \\n']\n",
      "9986 ['\\n = Hermann Fegelein = \\n']\n",
      "9992 ['\\n = Green Day = \\n']\n",
      "9999 ['\\n = Great North of Scotland Railway = \\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\n\\s*=[^=]*=\\s*\\n\"\n",
    "def color_pattern(s, p=pattern):\n",
    "    \"\"\"colors p in s using termcolor\"\"\"\n",
    "    return re.sub(p, colored(r\"\\g<0>\", \"blue\", attrs=['bold']), s)\n",
    "\n",
    "# for idx in range(len(tokens)):\n",
    "#     example = tokens[idx]\n",
    "#     print(color_pattern(tokenizer.decode(example)))\n",
    "#     if idx > 100:\n",
    "#         break\n",
    "\n",
    "for idx in range(len(tokens)):\n",
    "    example = tokens[idx]\n",
    "    s = tokenizer.decode(example)\n",
    "    matches = re.findall(pattern, s)\n",
    "    if len(matches) > 0:\n",
    "        print(idx, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5730d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Mary and John went to the store, John gave a drink to a family member. \n",
      " = = = The householder = = = \n",
      " The house was built by the architect William Horton, in the early 17th century. When the house\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model.model, tokenizer=tokenizer)\n",
    "prefix = \"When Mary and John went to the store, John gave a drink to\"\n",
    "print(generator(prefix, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4243692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Mary and John went to the store, John gave a drink to Mary’s mother of the children and said, “We’re going to get you something nice for me and I’m going to get you something good for me and I\n"
     ]
    }
   ],
   "source": [
    "orig_generator = pipeline('text-generation', model=orig_model, tokenizer=tokenizer)\n",
    "print(orig_generator(prefix, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb2ac746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://github.com/neelnanda-io/TransformerLens\n",
      "  Cloning https://github.com/neelnanda-io/TransformerLens to /tmp/pip-req-build-z8brjx98\n",
      "  Running command git clone -q https://github.com/neelnanda-io/TransformerLens /tmp/pip-req-build-z8brjx98\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rich<13.0.0,>=12.6.0\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jaxtyping<0.3.0,>=0.2.11\n",
      "  Downloading jaxtyping-0.2.14-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.7.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (2.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21; python_version < \"3.10\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (1.24.1)\n",
      "Requirement already satisfied: wandb<0.14.0,>=0.13.5 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (0.13.7)\n",
      "Requirement already satisfied: torch<2.0,>=1.10 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (1.13.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.25.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (4.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (4.64.1)\n",
      "Collecting einops<0.7.0,>=0.6.0\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fancy-einsum<0.0.4,>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.1.5 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformer-lens==0.0.0) (1.5.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from rich<13.0.0,>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0; python_version < \"3.9\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from rich<13.0.0,>=12.6.0->transformer-lens==0.0.0) (4.4.0)\n",
      "Collecting typeguard>=2.13.3\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: multiprocess in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.11.1)\n",
      "Requirement already satisfied: packaging in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (22.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2022.11.0)\n",
      "Requirement already satisfied: xxhash in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/taufeeque/codebook/lib/python3.8/site-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.3.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.0.11)\n",
      "Requirement already satisfied: pathtools in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (3.1.30)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (8.1.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (5.9.4)\n",
      "Requirement already satisfied: setproctitle in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0; python_version < \"3.9\" and sys_platform == \"linux\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.12.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from torch<2.0,>=1.10->transformer-lens==0.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from torch<2.0,>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from torch<2.0,>=1.10->transformer-lens==0.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /home/taufeeque/codebook/lib/python3.8/site-packages (from torch<2.0,>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (0.13.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from pandas<2.0.0,>=1.1.5->transformer-lens==0.0.0) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from pandas<2.0.0,>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/taufeeque/codebook/lib/python3.8/site-packages (from responses<0.19->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.26.13)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/taufeeque/codebook/lib/python3.8/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/taufeeque/codebook/lib/python3.8/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/taufeeque/codebook/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
      "Requirement already satisfied: wheel in /home/taufeeque/codebook/lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch<2.0,>=1.10->transformer-lens==0.0.0) (0.34.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/taufeeque/codebook/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
      "Building wheels for collected packages: transformer-lens\n",
      "  Building wheel for transformer-lens (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=86546 sha256=15550e5e8de27d17f5ed02e4ca692228cc788bc7d9dc36fef6a671358b7cf34b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gpe84sek/wheels/9c/1e/c7/87b4157fdb1e8133b38dd8647aadf56877e42cd4191bef6d65\n",
      "Successfully built transformer-lens\n",
      "Installing collected packages: commonmark, rich, typeguard, jaxtyping, einops, fancy-einsum, transformer-lens\n",
      "Successfully installed commonmark-0.9.1 einops-0.6.0 fancy-einsum-0.0.3 jaxtyping-0.2.14 rich-12.6.0 transformer-lens-0.0.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/neelnanda-io/TransformerLens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b8d7442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "taufeeque/tiny-gpt2 not found. Valid official model names (excl aliases): ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', 'stanford-crfm/alias-gpt2-small-x21', 'stanford-crfm/battlestar-gpt2-small-x49', 'stanford-crfm/caprica-gpt2-small-x81', 'stanford-crfm/darkmatter-gpt2-small-x343', 'stanford-crfm/expanse-gpt2-small-x777', 'stanford-crfm/arwen-gpt2-medium-x21', 'stanford-crfm/beren-gpt2-medium-x49', 'stanford-crfm/celebrimbor-gpt2-medium-x81', 'stanford-crfm/durin-gpt2-medium-x343', 'stanford-crfm/eowyn-gpt2-medium-x777', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b', 'EleutherAI/pythia-12b', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-1b-deduped', 'EleutherAI/pythia-1.4b-deduped', 'EleutherAI/pythia-2.8b-deduped', 'EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-12b-deduped', 'NeelNanda/SoLU_1L_v9_old', 'NeelNanda/SoLU_2L_v10_old', 'NeelNanda/SoLU_4L_v11_old', 'NeelNanda/SoLU_6L_v13_old', 'NeelNanda/SoLU_8L_v21_old', 'NeelNanda/SoLU_10L_v22_old', 'NeelNanda/SoLU_12L_v23_old', 'NeelNanda/SoLU_1L512W_C4_Code', 'NeelNanda/SoLU_2L512W_C4_Code', 'NeelNanda/SoLU_3L512W_C4_Code', 'NeelNanda/SoLU_4L512W_C4_Code', 'NeelNanda/SoLU_6L768W_C4_Code', 'NeelNanda/SoLU_8L1024W_C4_Code', 'NeelNanda/SoLU_10L1280W_C4_Code', 'NeelNanda/SoLU_12L1536W_C4_Code', 'NeelNanda/GELU_1L512W_C4_Code', 'NeelNanda/GELU_2L512W_C4_Code', 'NeelNanda/GELU_3L512W_C4_Code', 'NeelNanda/GELU_4L512W_C4_Code', 'NeelNanda/Attn_Only_1L512W_C4_Code', 'NeelNanda/Attn_Only_2L512W_C4_Code', 'NeelNanda/Attn_Only_3L512W_C4_Code', 'NeelNanda/Attn_Only_4L512W_C4_Code', 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr', 'NeelNanda/SoLU_1L512W_Wiki_Finetune', 'NeelNanda/SoLU_4L512W_Wiki_Finetune']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhook_points\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     HookedRootModule,\n\u001b[1;32m      5\u001b[0m     HookPoint,\n\u001b[1;32m      6\u001b[0m )  \u001b[38;5;66;03m# Hooking utilities\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaufeeque/tiny-gpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codebook/lib/python3.8/site-packages/transformer_lens/HookedTransformer.py:672\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, move_state_dict_to_device, **model_kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Class method to load in a pretrained model weights to the HookedTransformer format and optionally to do some processing to make the model easier to interpret. Currently supports loading from most autoregressive HuggingFace models (GPT2, GPTNeo, GPTJ, OPT) and from a range of toy models and SoLU models trained by me (Neel Nanda).\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03mAlso supports loading from a checkpoint for checkpointed models (currently, models trained by me (NeelNanda) and the stanford-crfm models). These can either be determined by the checkpoint index (the index of the checkpoint in the checkpoint list) or by the checkpoint value (the value of the checkpoint, eg 1000 for a checkpoint taken at step 1000 or after 1000 tokens. Each model has checkpoints labelled with exactly one of labels and steps). If neither is specified the final model is loaded. If both are specified, the checkpoint index is used.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m        HookedTransformer initialization.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Get the model name used in HuggingFace, rather than the alias.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m \u001b[43mloading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_official_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# Load the config into an HookedTransformerConfig object. If loading from a\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# checkpoint, the config object will contain the information about the\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# checkpoint\u001b[39;00m\n\u001b[1;32m    677\u001b[0m cfg \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_pretrained_model_config(\n\u001b[1;32m    678\u001b[0m     official_model_name,\n\u001b[1;32m    679\u001b[0m     checkpoint_index\u001b[38;5;241m=\u001b[39mcheckpoint_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    683\u001b[0m )\n",
      "File \u001b[0;32m~/codebook/lib/python3.8/site-packages/transformer_lens/loading_from_pretrained.py:326\u001b[0m, in \u001b[0;36mget_official_model_name\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    324\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m model_alias_map\u001b[38;5;241m.\u001b[39mget(model_name\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m official_model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Valid official model names (excl aliases): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOFFICIAL_MODEL_NAMES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m official_model_name\n",
      "\u001b[0;31mValueError\u001b[0m: taufeeque/tiny-gpt2 not found. Valid official model names (excl aliases): ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', 'stanford-crfm/alias-gpt2-small-x21', 'stanford-crfm/battlestar-gpt2-small-x49', 'stanford-crfm/caprica-gpt2-small-x81', 'stanford-crfm/darkmatter-gpt2-small-x343', 'stanford-crfm/expanse-gpt2-small-x777', 'stanford-crfm/arwen-gpt2-medium-x21', 'stanford-crfm/beren-gpt2-medium-x49', 'stanford-crfm/celebrimbor-gpt2-medium-x81', 'stanford-crfm/durin-gpt2-medium-x343', 'stanford-crfm/eowyn-gpt2-medium-x777', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b', 'EleutherAI/pythia-12b', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-1b-deduped', 'EleutherAI/pythia-1.4b-deduped', 'EleutherAI/pythia-2.8b-deduped', 'EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-12b-deduped', 'NeelNanda/SoLU_1L_v9_old', 'NeelNanda/SoLU_2L_v10_old', 'NeelNanda/SoLU_4L_v11_old', 'NeelNanda/SoLU_6L_v13_old', 'NeelNanda/SoLU_8L_v21_old', 'NeelNanda/SoLU_10L_v22_old', 'NeelNanda/SoLU_12L_v23_old', 'NeelNanda/SoLU_1L512W_C4_Code', 'NeelNanda/SoLU_2L512W_C4_Code', 'NeelNanda/SoLU_3L512W_C4_Code', 'NeelNanda/SoLU_4L512W_C4_Code', 'NeelNanda/SoLU_6L768W_C4_Code', 'NeelNanda/SoLU_8L1024W_C4_Code', 'NeelNanda/SoLU_10L1280W_C4_Code', 'NeelNanda/SoLU_12L1536W_C4_Code', 'NeelNanda/GELU_1L512W_C4_Code', 'NeelNanda/GELU_2L512W_C4_Code', 'NeelNanda/GELU_3L512W_C4_Code', 'NeelNanda/GELU_4L512W_C4_Code', 'NeelNanda/Attn_Only_1L512W_C4_Code', 'NeelNanda/Attn_Only_2L512W_C4_Code', 'NeelNanda/Attn_Only_3L512W_C4_Code', 'NeelNanda/Attn_Only_4L512W_C4_Code', 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr', 'NeelNanda/SoLU_1L512W_Wiki_Finetune', 'NeelNanda/SoLU_4L512W_Wiki_Finetune']"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"taufeeque/tiny-gpt2\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46db344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
